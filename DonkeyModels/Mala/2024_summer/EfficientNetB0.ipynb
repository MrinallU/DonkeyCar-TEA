{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc56131d-94ec-4711-a887-ca4717d06156",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count: 10645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 04:23:13.916347: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-21 04:23:16.108213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78902 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:47:00.0, compute capability: 8.0\n",
      "2024-07-21 04:23:16.110976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78902 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:4e:00.0, compute capability: 8.0\n",
      "2024-07-21 04:23:16.115671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 78902 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:b7:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16711680/16705208 [==============================] - 0s 0us/step\n",
      "16719872/16705208 [==============================] - 0s 0us/step\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 144, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rescaling (Rescaling)          (None, 144, 224, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " normalization (Normalization)  (None, 144, 224, 3)  7           ['rescaling[0][0]']              \n",
      "                                                                                                  \n",
      " stem_conv_pad (ZeroPadding2D)  (None, 145, 225, 3)  0           ['normalization[0][0]']          \n",
      "                                                                                                  \n",
      " stem_conv (Conv2D)             (None, 72, 112, 32)  864         ['stem_conv_pad[0][0]']          \n",
      "                                                                                                  \n",
      " stem_bn (BatchNormalization)   (None, 72, 112, 32)  128         ['stem_conv[0][0]']              \n",
      "                                                                                                  \n",
      " stem_activation (Activation)   (None, 72, 112, 32)  0           ['stem_bn[0][0]']                \n",
      "                                                                                                  \n",
      " block1a_dwconv (DepthwiseConv2  (None, 72, 112, 32)  288        ['stem_activation[0][0]']        \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block1a_bn (BatchNormalization  (None, 72, 112, 32)  128        ['block1a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block1a_activation (Activation  (None, 72, 112, 32)  0          ['block1a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block1a_se_squeeze (GlobalAver  (None, 32)          0           ['block1a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block1a_se_reshape (Reshape)   (None, 1, 1, 32)     0           ['block1a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_reduce (Conv2D)     (None, 1, 1, 8)      264         ['block1a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_expand (Conv2D)     (None, 1, 1, 32)     288         ['block1a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_se_excite (Multiply)   (None, 72, 112, 32)  0           ['block1a_activation[0][0]',     \n",
      "                                                                  'block1a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_project_conv (Conv2D)  (None, 72, 112, 16)  512         ['block1a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_project_bn (BatchNorma  (None, 72, 112, 16)  64         ['block1a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2a_expand_conv (Conv2D)   (None, 72, 112, 96)  1536        ['block1a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_expand_bn (BatchNormal  (None, 72, 112, 96)  384        ['block2a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2a_expand_activation (Act  (None, 72, 112, 96)  0          ['block2a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2a_dwconv_pad (ZeroPaddin  (None, 73, 113, 96)  0          ['block2a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block2a_dwconv (DepthwiseConv2  (None, 36, 56, 96)  864         ['block2a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block2a_bn (BatchNormalization  (None, 36, 56, 96)  384         ['block2a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_activation (Activation  (None, 36, 56, 96)  0           ['block2a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_se_squeeze (GlobalAver  (None, 96)          0           ['block2a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2a_se_reshape (Reshape)   (None, 1, 1, 96)     0           ['block2a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_reduce (Conv2D)     (None, 1, 1, 4)      388         ['block2a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_expand (Conv2D)     (None, 1, 1, 96)     480         ['block2a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_se_excite (Multiply)   (None, 36, 56, 96)   0           ['block2a_activation[0][0]',     \n",
      "                                                                  'block2a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_conv (Conv2D)  (None, 36, 56, 24)   2304        ['block2a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_bn (BatchNorma  (None, 36, 56, 24)  96          ['block2a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_expand_conv (Conv2D)   (None, 36, 56, 144)  3456        ['block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_expand_bn (BatchNormal  (None, 36, 56, 144)  576        ['block2b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_expand_activation (Act  (None, 36, 56, 144)  0          ['block2b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_dwconv (DepthwiseConv2  (None, 36, 56, 144)  1296       ['block2b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block2b_bn (BatchNormalization  (None, 36, 56, 144)  576        ['block2b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_activation (Activation  (None, 36, 56, 144)  0          ['block2b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_se_squeeze (GlobalAver  (None, 144)         0           ['block2b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2b_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block2b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block2b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block2b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_se_excite (Multiply)   (None, 36, 56, 144)  0           ['block2b_activation[0][0]',     \n",
      "                                                                  'block2b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_conv (Conv2D)  (None, 36, 56, 24)   3456        ['block2b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_bn (BatchNorma  (None, 36, 56, 24)  96          ['block2b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_drop (Dropout)         (None, 36, 56, 24)   0           ['block2b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_add (Add)              (None, 36, 56, 24)   0           ['block2b_drop[0][0]',           \n",
      "                                                                  'block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_expand_conv (Conv2D)   (None, 36, 56, 144)  3456        ['block2b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block3a_expand_bn (BatchNormal  (None, 36, 56, 144)  576        ['block3a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_expand_activation (Act  (None, 36, 56, 144)  0          ['block3a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_dwconv_pad (ZeroPaddin  (None, 39, 59, 144)  0          ['block3a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block3a_dwconv (DepthwiseConv2  (None, 18, 28, 144)  3600       ['block3a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block3a_bn (BatchNormalization  (None, 18, 28, 144)  576        ['block3a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_activation (Activation  (None, 18, 28, 144)  0          ['block3a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_se_squeeze (GlobalAver  (None, 144)         0           ['block3a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3a_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block3a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block3a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block3a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_se_excite (Multiply)   (None, 18, 28, 144)  0           ['block3a_activation[0][0]',     \n",
      "                                                                  'block3a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_conv (Conv2D)  (None, 18, 28, 40)   5760        ['block3a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_bn (BatchNorma  (None, 18, 28, 40)  160         ['block3a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_expand_conv (Conv2D)   (None, 18, 28, 240)  9600        ['block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_expand_bn (BatchNormal  (None, 18, 28, 240)  960        ['block3b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_expand_activation (Act  (None, 18, 28, 240)  0          ['block3b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_dwconv (DepthwiseConv2  (None, 18, 28, 240)  6000       ['block3b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block3b_bn (BatchNormalization  (None, 18, 28, 240)  960        ['block3b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_activation (Activation  (None, 18, 28, 240)  0          ['block3b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_se_squeeze (GlobalAver  (None, 240)         0           ['block3b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3b_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block3b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block3b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block3b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_se_excite (Multiply)   (None, 18, 28, 240)  0           ['block3b_activation[0][0]',     \n",
      "                                                                  'block3b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_conv (Conv2D)  (None, 18, 28, 40)   9600        ['block3b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_bn (BatchNorma  (None, 18, 28, 40)  160         ['block3b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_drop (Dropout)         (None, 18, 28, 40)   0           ['block3b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_add (Add)              (None, 18, 28, 40)   0           ['block3b_drop[0][0]',           \n",
      "                                                                  'block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_expand_conv (Conv2D)   (None, 18, 28, 240)  9600        ['block3b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4a_expand_bn (BatchNormal  (None, 18, 28, 240)  960        ['block4a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_expand_activation (Act  (None, 18, 28, 240)  0          ['block4a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_dwconv_pad (ZeroPaddin  (None, 19, 29, 240)  0          ['block4a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block4a_dwconv (DepthwiseConv2  (None, 9, 14, 240)  2160        ['block4a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block4a_bn (BatchNormalization  (None, 9, 14, 240)  960         ['block4a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_activation (Activation  (None, 9, 14, 240)  0           ['block4a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_se_squeeze (GlobalAver  (None, 240)         0           ['block4a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4a_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block4a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block4a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block4a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_se_excite (Multiply)   (None, 9, 14, 240)   0           ['block4a_activation[0][0]',     \n",
      "                                                                  'block4a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_conv (Conv2D)  (None, 9, 14, 80)    19200       ['block4a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_bn (BatchNorma  (None, 9, 14, 80)   320         ['block4a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_expand_conv (Conv2D)   (None, 9, 14, 480)   38400       ['block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_expand_bn (BatchNormal  (None, 9, 14, 480)  1920        ['block4b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_expand_activation (Act  (None, 9, 14, 480)  0           ['block4b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_dwconv (DepthwiseConv2  (None, 9, 14, 480)  4320        ['block4b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4b_bn (BatchNormalization  (None, 9, 14, 480)  1920        ['block4b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_activation (Activation  (None, 9, 14, 480)  0           ['block4b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_se_squeeze (GlobalAver  (None, 480)         0           ['block4b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4b_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_se_excite (Multiply)   (None, 9, 14, 480)   0           ['block4b_activation[0][0]',     \n",
      "                                                                  'block4b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_conv (Conv2D)  (None, 9, 14, 80)    38400       ['block4b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_bn (BatchNorma  (None, 9, 14, 80)   320         ['block4b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_drop (Dropout)         (None, 9, 14, 80)    0           ['block4b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_add (Add)              (None, 9, 14, 80)    0           ['block4b_drop[0][0]',           \n",
      "                                                                  'block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_expand_conv (Conv2D)   (None, 9, 14, 480)   38400       ['block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4c_expand_bn (BatchNormal  (None, 9, 14, 480)  1920        ['block4c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_expand_activation (Act  (None, 9, 14, 480)  0           ['block4c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_dwconv (DepthwiseConv2  (None, 9, 14, 480)  4320        ['block4c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4c_bn (BatchNormalization  (None, 9, 14, 480)  1920        ['block4c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_activation (Activation  (None, 9, 14, 480)  0           ['block4c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_se_squeeze (GlobalAver  (None, 480)         0           ['block4c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4c_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_se_excite (Multiply)   (None, 9, 14, 480)   0           ['block4c_activation[0][0]',     \n",
      "                                                                  'block4c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_conv (Conv2D)  (None, 9, 14, 80)    38400       ['block4c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_bn (BatchNorma  (None, 9, 14, 80)   320         ['block4c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4c_drop (Dropout)         (None, 9, 14, 80)    0           ['block4c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_add (Add)              (None, 9, 14, 80)    0           ['block4c_drop[0][0]',           \n",
      "                                                                  'block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_conv (Conv2D)   (None, 9, 14, 480)   38400       ['block4c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_bn (BatchNormal  (None, 9, 14, 480)  1920        ['block5a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_expand_activation (Act  (None, 9, 14, 480)  0           ['block5a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_dwconv (DepthwiseConv2  (None, 9, 14, 480)  12000       ['block5a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5a_bn (BatchNormalization  (None, 9, 14, 480)  1920        ['block5a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_activation (Activation  (None, 9, 14, 480)  0           ['block5a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_se_squeeze (GlobalAver  (None, 480)         0           ['block5a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5a_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block5a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block5a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block5a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_se_excite (Multiply)   (None, 9, 14, 480)   0           ['block5a_activation[0][0]',     \n",
      "                                                                  'block5a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_conv (Conv2D)  (None, 9, 14, 112)   53760       ['block5a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_bn (BatchNorma  (None, 9, 14, 112)  448         ['block5a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_expand_conv (Conv2D)   (None, 9, 14, 672)   75264       ['block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_expand_bn (BatchNormal  (None, 9, 14, 672)  2688        ['block5b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_expand_activation (Act  (None, 9, 14, 672)  0           ['block5b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_dwconv (DepthwiseConv2  (None, 9, 14, 672)  16800       ['block5b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5b_bn (BatchNormalization  (None, 9, 14, 672)  2688        ['block5b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_activation (Activation  (None, 9, 14, 672)  0           ['block5b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_se_squeeze (GlobalAver  (None, 672)         0           ['block5b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5b_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_se_excite (Multiply)   (None, 9, 14, 672)   0           ['block5b_activation[0][0]',     \n",
      "                                                                  'block5b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_conv (Conv2D)  (None, 9, 14, 112)   75264       ['block5b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_bn (BatchNorma  (None, 9, 14, 112)  448         ['block5b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_drop (Dropout)         (None, 9, 14, 112)   0           ['block5b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_add (Add)              (None, 9, 14, 112)   0           ['block5b_drop[0][0]',           \n",
      "                                                                  'block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_expand_conv (Conv2D)   (None, 9, 14, 672)   75264       ['block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5c_expand_bn (BatchNormal  (None, 9, 14, 672)  2688        ['block5c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_expand_activation (Act  (None, 9, 14, 672)  0           ['block5c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_dwconv (DepthwiseConv2  (None, 9, 14, 672)  16800       ['block5c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5c_bn (BatchNormalization  (None, 9, 14, 672)  2688        ['block5c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_activation (Activation  (None, 9, 14, 672)  0           ['block5c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_se_squeeze (GlobalAver  (None, 672)         0           ['block5c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5c_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_se_excite (Multiply)   (None, 9, 14, 672)   0           ['block5c_activation[0][0]',     \n",
      "                                                                  'block5c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_conv (Conv2D)  (None, 9, 14, 112)   75264       ['block5c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_bn (BatchNorma  (None, 9, 14, 112)  448         ['block5c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5c_drop (Dropout)         (None, 9, 14, 112)   0           ['block5c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_add (Add)              (None, 9, 14, 112)   0           ['block5c_drop[0][0]',           \n",
      "                                                                  'block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_conv (Conv2D)   (None, 9, 14, 672)   75264       ['block5c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_bn (BatchNormal  (None, 9, 14, 672)  2688        ['block6a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_expand_activation (Act  (None, 9, 14, 672)  0           ['block6a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_dwconv_pad (ZeroPaddin  (None, 13, 17, 672)  0          ['block6a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block6a_dwconv (DepthwiseConv2  (None, 5, 7, 672)   16800       ['block6a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block6a_bn (BatchNormalization  (None, 5, 7, 672)   2688        ['block6a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_activation (Activation  (None, 5, 7, 672)   0           ['block6a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_se_squeeze (GlobalAver  (None, 672)         0           ['block6a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6a_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block6a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block6a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block6a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_se_excite (Multiply)   (None, 5, 7, 672)    0           ['block6a_activation[0][0]',     \n",
      "                                                                  'block6a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_conv (Conv2D)  (None, 5, 7, 192)    129024      ['block6a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_bn (BatchNorma  (None, 5, 7, 192)   768         ['block6a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_expand_conv (Conv2D)   (None, 5, 7, 1152)   221184      ['block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_expand_bn (BatchNormal  (None, 5, 7, 1152)  4608        ['block6b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_expand_activation (Act  (None, 5, 7, 1152)  0           ['block6b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_dwconv (DepthwiseConv2  (None, 5, 7, 1152)  28800       ['block6b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6b_bn (BatchNormalization  (None, 5, 7, 1152)  4608        ['block6b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6b_activation (Activation  (None, 5, 7, 1152)  0           ['block6b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6b_se_squeeze (GlobalAver  (None, 1152)        0           ['block6b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6b_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_se_excite (Multiply)   (None, 5, 7, 1152)   0           ['block6b_activation[0][0]',     \n",
      "                                                                  'block6b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_conv (Conv2D)  (None, 5, 7, 192)    221184      ['block6b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_bn (BatchNorma  (None, 5, 7, 192)   768         ['block6b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_drop (Dropout)         (None, 5, 7, 192)    0           ['block6b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_add (Add)              (None, 5, 7, 192)    0           ['block6b_drop[0][0]',           \n",
      "                                                                  'block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_expand_conv (Conv2D)   (None, 5, 7, 1152)   221184      ['block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6c_expand_bn (BatchNormal  (None, 5, 7, 1152)  4608        ['block6c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_expand_activation (Act  (None, 5, 7, 1152)  0           ['block6c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_dwconv (DepthwiseConv2  (None, 5, 7, 1152)  28800       ['block6c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6c_bn (BatchNormalization  (None, 5, 7, 1152)  4608        ['block6c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6c_activation (Activation  (None, 5, 7, 1152)  0           ['block6c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6c_se_squeeze (GlobalAver  (None, 1152)        0           ['block6c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6c_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_se_excite (Multiply)   (None, 5, 7, 1152)   0           ['block6c_activation[0][0]',     \n",
      "                                                                  'block6c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_conv (Conv2D)  (None, 5, 7, 192)    221184      ['block6c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_bn (BatchNorma  (None, 5, 7, 192)   768         ['block6c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6c_drop (Dropout)         (None, 5, 7, 192)    0           ['block6c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_add (Add)              (None, 5, 7, 192)    0           ['block6c_drop[0][0]',           \n",
      "                                                                  'block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_conv (Conv2D)   (None, 5, 7, 1152)   221184      ['block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_bn (BatchNormal  (None, 5, 7, 1152)  4608        ['block6d_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_expand_activation (Act  (None, 5, 7, 1152)  0           ['block6d_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_dwconv (DepthwiseConv2  (None, 5, 7, 1152)  28800       ['block6d_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6d_bn (BatchNormalization  (None, 5, 7, 1152)  4608        ['block6d_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6d_activation (Activation  (None, 5, 7, 1152)  0           ['block6d_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6d_se_squeeze (GlobalAver  (None, 1152)        0           ['block6d_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6d_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6d_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6d_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6d_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_se_excite (Multiply)   (None, 5, 7, 1152)   0           ['block6d_activation[0][0]',     \n",
      "                                                                  'block6d_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_conv (Conv2D)  (None, 5, 7, 192)    221184      ['block6d_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_bn (BatchNorma  (None, 5, 7, 192)   768         ['block6d_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6d_drop (Dropout)         (None, 5, 7, 192)    0           ['block6d_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_add (Add)              (None, 5, 7, 192)    0           ['block6d_drop[0][0]',           \n",
      "                                                                  'block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_conv (Conv2D)   (None, 5, 7, 1152)   221184      ['block6d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_bn (BatchNormal  (None, 5, 7, 1152)  4608        ['block7a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block7a_expand_activation (Act  (None, 5, 7, 1152)  0           ['block7a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block7a_dwconv (DepthwiseConv2  (None, 5, 7, 1152)  10368       ['block7a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block7a_bn (BatchNormalization  (None, 5, 7, 1152)  4608        ['block7a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block7a_activation (Activation  (None, 5, 7, 1152)  0           ['block7a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block7a_se_squeeze (GlobalAver  (None, 1152)        0           ['block7a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block7a_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block7a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block7a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block7a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_se_excite (Multiply)   (None, 5, 7, 1152)   0           ['block7a_activation[0][0]',     \n",
      "                                                                  'block7a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_conv (Conv2D)  (None, 5, 7, 320)    368640      ['block7a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_bn (BatchNorma  (None, 5, 7, 320)   1280        ['block7a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " top_conv (Conv2D)              (None, 5, 7, 1280)   409600      ['block7a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " top_bn (BatchNormalization)    (None, 5, 7, 1280)   5120        ['top_conv[0][0]']               \n",
      "                                                                                                  \n",
      " top_activation (Activation)    (None, 5, 7, 1280)   0           ['top_bn[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1280)        0           ['top_activation[0][0]']         \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         1311744     ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            1025        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,362,340\n",
      "Trainable params: 1,312,769\n",
      "Non-trainable params: 4,049,571\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 04:23:22.387826: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20/566 [>.............................] - ETA: 4s - loss: 0.4798 - mae: 0.5592 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 04:23:24.134030: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566/566 [==============================] - ETA: 0s - loss: 0.2581 - mae: 0.3978"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/tensorflow/2.7.0/lib/python3.9/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566/566 [==============================] - 12s 12ms/step - loss: 0.2581 - mae: 0.3978 - val_loss: 0.1732 - val_mae: 0.3190\n",
      "Epoch 2/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1859 - mae: 0.3342 - val_loss: 0.1517 - val_mae: 0.2993\n",
      "Epoch 3/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1763 - mae: 0.3249 - val_loss: 0.1762 - val_mae: 0.3258\n",
      "Epoch 4/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1681 - mae: 0.3161 - val_loss: 0.1475 - val_mae: 0.2747\n",
      "Epoch 5/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1632 - mae: 0.3096 - val_loss: 0.1453 - val_mae: 0.2797\n",
      "Epoch 6/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1606 - mae: 0.3071 - val_loss: 0.1480 - val_mae: 0.2932\n",
      "Epoch 7/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1573 - mae: 0.3034 - val_loss: 0.1442 - val_mae: 0.2825\n",
      "Epoch 8/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1563 - mae: 0.3024 - val_loss: 0.1450 - val_mae: 0.2754\n",
      "Epoch 9/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1532 - mae: 0.2985 - val_loss: 0.1439 - val_mae: 0.2799\n",
      "Epoch 10/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1510 - mae: 0.2957 - val_loss: 0.1367 - val_mae: 0.2723\n",
      "Epoch 11/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1495 - mae: 0.2942 - val_loss: 0.1402 - val_mae: 0.2793\n",
      "Epoch 12/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1469 - mae: 0.2915 - val_loss: 0.1427 - val_mae: 0.2808\n",
      "Epoch 13/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1454 - mae: 0.2902 - val_loss: 0.1396 - val_mae: 0.2751\n",
      "Epoch 14/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1448 - mae: 0.2891 - val_loss: 0.1421 - val_mae: 0.2815\n",
      "Epoch 15/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1435 - mae: 0.2872 - val_loss: 0.1374 - val_mae: 0.2756\n",
      "Epoch 16/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1415 - mae: 0.2853 - val_loss: 0.1369 - val_mae: 0.2720\n",
      "Epoch 17/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1429 - mae: 0.2865 - val_loss: 0.1404 - val_mae: 0.2738\n",
      "Epoch 18/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1381 - mae: 0.2818 - val_loss: 0.1352 - val_mae: 0.2636\n",
      "Epoch 19/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1384 - mae: 0.2815 - val_loss: 0.1365 - val_mae: 0.2660\n",
      "Epoch 20/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1380 - mae: 0.2813 - val_loss: 0.1324 - val_mae: 0.2652\n",
      "Epoch 21/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1351 - mae: 0.2776 - val_loss: 0.1333 - val_mae: 0.2644\n",
      "Epoch 22/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1369 - mae: 0.2797 - val_loss: 0.1313 - val_mae: 0.2624\n",
      "Epoch 23/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1346 - mae: 0.2771 - val_loss: 0.1352 - val_mae: 0.2655\n",
      "Epoch 24/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1322 - mae: 0.2733 - val_loss: 0.1298 - val_mae: 0.2626\n",
      "Epoch 25/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1323 - mae: 0.2741 - val_loss: 0.1318 - val_mae: 0.2688\n",
      "Epoch 26/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1325 - mae: 0.2741 - val_loss: 0.1288 - val_mae: 0.2606\n",
      "Epoch 27/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1306 - mae: 0.2732 - val_loss: 0.1332 - val_mae: 0.2664\n",
      "Epoch 28/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1301 - mae: 0.2717 - val_loss: 0.1383 - val_mae: 0.2763\n",
      "Epoch 29/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1285 - mae: 0.2686 - val_loss: 0.1316 - val_mae: 0.2595\n",
      "Epoch 30/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1282 - mae: 0.2679 - val_loss: 0.1283 - val_mae: 0.2612\n",
      "Epoch 31/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1267 - mae: 0.2677 - val_loss: 0.1303 - val_mae: 0.2656\n",
      "Epoch 32/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1274 - mae: 0.2671 - val_loss: 0.1265 - val_mae: 0.2554\n",
      "Epoch 33/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1261 - mae: 0.2651 - val_loss: 0.1264 - val_mae: 0.2578\n",
      "Epoch 34/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1239 - mae: 0.2647 - val_loss: 0.1347 - val_mae: 0.2661\n",
      "Epoch 35/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1247 - mae: 0.2642 - val_loss: 0.1292 - val_mae: 0.2598\n",
      "Epoch 36/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1229 - mae: 0.2632 - val_loss: 0.1270 - val_mae: 0.2592\n",
      "Epoch 37/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1220 - mae: 0.2609 - val_loss: 0.1280 - val_mae: 0.2513\n",
      "Epoch 38/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1225 - mae: 0.2608 - val_loss: 0.1242 - val_mae: 0.2502\n",
      "Epoch 39/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1207 - mae: 0.2602 - val_loss: 0.1263 - val_mae: 0.2539\n",
      "Epoch 40/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1189 - mae: 0.2572 - val_loss: 0.1271 - val_mae: 0.2538\n",
      "Epoch 41/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1200 - mae: 0.2581 - val_loss: 0.1313 - val_mae: 0.2672\n",
      "Epoch 42/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1176 - mae: 0.2557 - val_loss: 0.1267 - val_mae: 0.2589\n",
      "Epoch 43/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1181 - mae: 0.2557 - val_loss: 0.1301 - val_mae: 0.2694\n",
      "Epoch 44/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1172 - mae: 0.2561 - val_loss: 0.1263 - val_mae: 0.2577\n",
      "Epoch 45/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1152 - mae: 0.2534 - val_loss: 0.1295 - val_mae: 0.2562\n",
      "Epoch 46/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1170 - mae: 0.2558 - val_loss: 0.1256 - val_mae: 0.2569\n",
      "Epoch 47/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1148 - mae: 0.2524 - val_loss: 0.1277 - val_mae: 0.2591\n",
      "Epoch 48/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1162 - mae: 0.2528 - val_loss: 0.1234 - val_mae: 0.2527\n",
      "Epoch 49/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1133 - mae: 0.2502 - val_loss: 0.1239 - val_mae: 0.2517\n",
      "Epoch 50/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1141 - mae: 0.2518 - val_loss: 0.1257 - val_mae: 0.2509\n",
      "Epoch 51/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1123 - mae: 0.2498 - val_loss: 0.1213 - val_mae: 0.2480\n",
      "Epoch 52/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1129 - mae: 0.2499 - val_loss: 0.1220 - val_mae: 0.2484\n",
      "Epoch 53/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1121 - mae: 0.2495 - val_loss: 0.1258 - val_mae: 0.2518\n",
      "Epoch 54/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1112 - mae: 0.2482 - val_loss: 0.1250 - val_mae: 0.2527\n",
      "Epoch 55/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1096 - mae: 0.2466 - val_loss: 0.1237 - val_mae: 0.2500\n",
      "Epoch 56/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1118 - mae: 0.2476 - val_loss: 0.1236 - val_mae: 0.2498\n",
      "Epoch 57/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1116 - mae: 0.2479 - val_loss: 0.1239 - val_mae: 0.2545\n",
      "Epoch 58/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1090 - mae: 0.2452 - val_loss: 0.1215 - val_mae: 0.2481\n",
      "Epoch 59/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1074 - mae: 0.2435 - val_loss: 0.1221 - val_mae: 0.2513\n",
      "Epoch 60/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1078 - mae: 0.2441 - val_loss: 0.1234 - val_mae: 0.2503\n",
      "Epoch 61/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1075 - mae: 0.2443 - val_loss: 0.1214 - val_mae: 0.2443\n",
      "Epoch 62/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1061 - mae: 0.2409 - val_loss: 0.1252 - val_mae: 0.2562\n",
      "Epoch 63/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1059 - mae: 0.2419 - val_loss: 0.1216 - val_mae: 0.2461\n",
      "Epoch 64/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1038 - mae: 0.2384 - val_loss: 0.1250 - val_mae: 0.2543\n",
      "Epoch 65/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1048 - mae: 0.2400 - val_loss: 0.1277 - val_mae: 0.2528\n",
      "Epoch 66/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1056 - mae: 0.2405 - val_loss: 0.1201 - val_mae: 0.2476\n",
      "Epoch 67/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1042 - mae: 0.2388 - val_loss: 0.1212 - val_mae: 0.2484\n",
      "Epoch 68/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1036 - mae: 0.2387 - val_loss: 0.1219 - val_mae: 0.2514\n",
      "Epoch 69/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1040 - mae: 0.2376 - val_loss: 0.1230 - val_mae: 0.2498\n",
      "Epoch 70/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1035 - mae: 0.2368 - val_loss: 0.1241 - val_mae: 0.2498\n",
      "Epoch 71/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1045 - mae: 0.2379 - val_loss: 0.1249 - val_mae: 0.2527\n",
      "Epoch 72/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1007 - mae: 0.2356 - val_loss: 0.1228 - val_mae: 0.2491\n",
      "Epoch 73/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1001 - mae: 0.2341 - val_loss: 0.1202 - val_mae: 0.2469\n",
      "Epoch 74/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1018 - mae: 0.2352 - val_loss: 0.1216 - val_mae: 0.2453\n",
      "Epoch 75/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.1017 - mae: 0.2352 - val_loss: 0.1174 - val_mae: 0.2446\n",
      "Epoch 76/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.1008 - mae: 0.2353 - val_loss: 0.1223 - val_mae: 0.2480\n",
      "Epoch 77/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0990 - mae: 0.2316 - val_loss: 0.1205 - val_mae: 0.2428\n",
      "Epoch 78/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0991 - mae: 0.2310 - val_loss: 0.1232 - val_mae: 0.2501\n",
      "Epoch 79/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0988 - mae: 0.2317 - val_loss: 0.1176 - val_mae: 0.2424\n",
      "Epoch 80/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0981 - mae: 0.2323 - val_loss: 0.1192 - val_mae: 0.2415\n",
      "Epoch 81/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0988 - mae: 0.2319 - val_loss: 0.1208 - val_mae: 0.2426\n",
      "Epoch 82/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0963 - mae: 0.2294 - val_loss: 0.1197 - val_mae: 0.2412\n",
      "Epoch 83/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0974 - mae: 0.2294 - val_loss: 0.1236 - val_mae: 0.2511\n",
      "Epoch 84/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0954 - mae: 0.2278 - val_loss: 0.1195 - val_mae: 0.2413\n",
      "Epoch 85/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0952 - mae: 0.2281 - val_loss: 0.1206 - val_mae: 0.2490\n",
      "Epoch 86/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0958 - mae: 0.2284 - val_loss: 0.1164 - val_mae: 0.2403\n",
      "Epoch 87/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0945 - mae: 0.2265 - val_loss: 0.1170 - val_mae: 0.2402\n",
      "Epoch 88/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0924 - mae: 0.2249 - val_loss: 0.1185 - val_mae: 0.2439\n",
      "Epoch 89/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0942 - mae: 0.2249 - val_loss: 0.1216 - val_mae: 0.2461\n",
      "Epoch 90/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0946 - mae: 0.2268 - val_loss: 0.1203 - val_mae: 0.2459\n",
      "Epoch 91/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0944 - mae: 0.2266 - val_loss: 0.1214 - val_mae: 0.2420\n",
      "Epoch 92/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0927 - mae: 0.2236 - val_loss: 0.1171 - val_mae: 0.2390\n",
      "Epoch 93/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0916 - mae: 0.2228 - val_loss: 0.1200 - val_mae: 0.2402\n",
      "Epoch 94/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0941 - mae: 0.2263 - val_loss: 0.1193 - val_mae: 0.2430\n",
      "Epoch 95/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0922 - mae: 0.2245 - val_loss: 0.1194 - val_mae: 0.2448\n",
      "Epoch 96/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0918 - mae: 0.2230 - val_loss: 0.1181 - val_mae: 0.2420\n",
      "Epoch 97/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0909 - mae: 0.2203 - val_loss: 0.1199 - val_mae: 0.2443\n",
      "Epoch 98/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0917 - mae: 0.2220 - val_loss: 0.1185 - val_mae: 0.2414\n",
      "Epoch 99/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0902 - mae: 0.2208 - val_loss: 0.1201 - val_mae: 0.2487\n",
      "Epoch 100/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0906 - mae: 0.2219 - val_loss: 0.1208 - val_mae: 0.2451\n",
      "Epoch 101/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0882 - mae: 0.2178 - val_loss: 0.1173 - val_mae: 0.2393\n",
      "Epoch 102/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0895 - mae: 0.2208 - val_loss: 0.1183 - val_mae: 0.2410\n",
      "Epoch 103/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0884 - mae: 0.2187 - val_loss: 0.1211 - val_mae: 0.2440\n",
      "Epoch 104/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0888 - mae: 0.2186 - val_loss: 0.1193 - val_mae: 0.2436\n",
      "Epoch 105/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0892 - mae: 0.2198 - val_loss: 0.1242 - val_mae: 0.2456\n",
      "Epoch 106/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0868 - mae: 0.2158 - val_loss: 0.1186 - val_mae: 0.2424\n",
      "Epoch 107/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0886 - mae: 0.2182 - val_loss: 0.1203 - val_mae: 0.2444\n",
      "Epoch 108/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0879 - mae: 0.2170 - val_loss: 0.1197 - val_mae: 0.2418\n",
      "Epoch 109/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0892 - mae: 0.2182 - val_loss: 0.1176 - val_mae: 0.2417\n",
      "Epoch 110/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0866 - mae: 0.2153 - val_loss: 0.1214 - val_mae: 0.2426\n",
      "Epoch 111/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0876 - mae: 0.2170 - val_loss: 0.1213 - val_mae: 0.2454\n",
      "Epoch 112/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0860 - mae: 0.2138 - val_loss: 0.1204 - val_mae: 0.2436\n",
      "Epoch 113/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0855 - mae: 0.2138 - val_loss: 0.1213 - val_mae: 0.2445\n",
      "Epoch 114/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0845 - mae: 0.2132 - val_loss: 0.1198 - val_mae: 0.2373\n",
      "Epoch 115/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0851 - mae: 0.2133 - val_loss: 0.1179 - val_mae: 0.2397\n",
      "Epoch 116/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0841 - mae: 0.2123 - val_loss: 0.1185 - val_mae: 0.2401\n",
      "Epoch 117/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0842 - mae: 0.2126 - val_loss: 0.1174 - val_mae: 0.2372\n",
      "Epoch 118/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0845 - mae: 0.2129 - val_loss: 0.1166 - val_mae: 0.2342\n",
      "Epoch 119/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0839 - mae: 0.2116 - val_loss: 0.1212 - val_mae: 0.2392\n",
      "Epoch 120/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0847 - mae: 0.2138 - val_loss: 0.1201 - val_mae: 0.2439\n",
      "Epoch 121/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0840 - mae: 0.2127 - val_loss: 0.1190 - val_mae: 0.2379\n",
      "Epoch 122/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0828 - mae: 0.2118 - val_loss: 0.1181 - val_mae: 0.2414\n",
      "Epoch 123/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0852 - mae: 0.2122 - val_loss: 0.1236 - val_mae: 0.2504\n",
      "Epoch 124/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0835 - mae: 0.2106 - val_loss: 0.1177 - val_mae: 0.2347\n",
      "Epoch 125/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0804 - mae: 0.2075 - val_loss: 0.1155 - val_mae: 0.2360\n",
      "Epoch 126/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0800 - mae: 0.2076 - val_loss: 0.1181 - val_mae: 0.2405\n",
      "Epoch 127/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0811 - mae: 0.2091 - val_loss: 0.1169 - val_mae: 0.2420\n",
      "Epoch 128/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0795 - mae: 0.2062 - val_loss: 0.1186 - val_mae: 0.2404\n",
      "Epoch 129/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0814 - mae: 0.2087 - val_loss: 0.1238 - val_mae: 0.2480\n",
      "Epoch 130/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0806 - mae: 0.2080 - val_loss: 0.1194 - val_mae: 0.2402\n",
      "Epoch 131/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.0808 - mae: 0.2074 - val_loss: 0.1146 - val_mae: 0.2335\n",
      "Epoch 132/2000\n",
      "566/566 [==============================] - 5s 8ms/step - loss: 0.0792 - mae: 0.2048 - val_loss: 0.1172 - val_mae: 0.2357\n",
      "Epoch 133/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0790 - mae: 0.2049 - val_loss: 0.1177 - val_mae: 0.2368\n",
      "Epoch 134/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0776 - mae: 0.2028 - val_loss: 0.1176 - val_mae: 0.2391\n",
      "Epoch 135/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0800 - mae: 0.2069 - val_loss: 0.1163 - val_mae: 0.2393\n",
      "Epoch 136/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0781 - mae: 0.2046 - val_loss: 0.1184 - val_mae: 0.2398\n",
      "Epoch 137/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0777 - mae: 0.2049 - val_loss: 0.1184 - val_mae: 0.2362\n",
      "Epoch 138/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0788 - mae: 0.2049 - val_loss: 0.1169 - val_mae: 0.2388\n",
      "Epoch 139/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0774 - mae: 0.2039 - val_loss: 0.1197 - val_mae: 0.2411\n",
      "Epoch 140/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0760 - mae: 0.2008 - val_loss: 0.1157 - val_mae: 0.2349\n",
      "Epoch 141/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0778 - mae: 0.2027 - val_loss: 0.1209 - val_mae: 0.2431\n",
      "Epoch 142/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0776 - mae: 0.2037 - val_loss: 0.1205 - val_mae: 0.2413\n",
      "Epoch 143/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0787 - mae: 0.2050 - val_loss: 0.1192 - val_mae: 0.2389\n",
      "Epoch 144/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0763 - mae: 0.2018 - val_loss: 0.1176 - val_mae: 0.2386\n",
      "Epoch 145/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0771 - mae: 0.2022 - val_loss: 0.1163 - val_mae: 0.2341\n",
      "Epoch 146/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0774 - mae: 0.2034 - val_loss: 0.1173 - val_mae: 0.2379\n",
      "Epoch 147/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0756 - mae: 0.2001 - val_loss: 0.1159 - val_mae: 0.2354\n",
      "Epoch 148/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0769 - mae: 0.2012 - val_loss: 0.1201 - val_mae: 0.2391\n",
      "Epoch 149/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0773 - mae: 0.2028 - val_loss: 0.1183 - val_mae: 0.2389\n",
      "Epoch 150/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0756 - mae: 0.1996 - val_loss: 0.1176 - val_mae: 0.2340\n",
      "Epoch 151/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0753 - mae: 0.1987 - val_loss: 0.1212 - val_mae: 0.2438\n",
      "Epoch 152/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0741 - mae: 0.1974 - val_loss: 0.1172 - val_mae: 0.2352\n",
      "Epoch 153/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0779 - mae: 0.2019 - val_loss: 0.1172 - val_mae: 0.2338\n",
      "Epoch 154/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0744 - mae: 0.1975 - val_loss: 0.1161 - val_mae: 0.2311\n",
      "Epoch 155/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0729 - mae: 0.1971 - val_loss: 0.1174 - val_mae: 0.2328\n",
      "Epoch 156/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0743 - mae: 0.1978 - val_loss: 0.1161 - val_mae: 0.2323\n",
      "Epoch 157/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0733 - mae: 0.1959 - val_loss: 0.1159 - val_mae: 0.2325\n",
      "Epoch 158/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0739 - mae: 0.1968 - val_loss: 0.1175 - val_mae: 0.2353\n",
      "Epoch 159/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0758 - mae: 0.2000 - val_loss: 0.1154 - val_mae: 0.2347\n",
      "Epoch 160/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0744 - mae: 0.1972 - val_loss: 0.1212 - val_mae: 0.2443\n",
      "Epoch 161/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0738 - mae: 0.1972 - val_loss: 0.1181 - val_mae: 0.2370\n",
      "Epoch 162/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0742 - mae: 0.1973 - val_loss: 0.1159 - val_mae: 0.2414\n",
      "Epoch 163/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0725 - mae: 0.1955 - val_loss: 0.1186 - val_mae: 0.2358\n",
      "Epoch 164/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0739 - mae: 0.1980 - val_loss: 0.1189 - val_mae: 0.2408\n",
      "Epoch 165/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0738 - mae: 0.1960 - val_loss: 0.1180 - val_mae: 0.2386\n",
      "Epoch 166/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0706 - mae: 0.1926 - val_loss: 0.1182 - val_mae: 0.2407\n",
      "Epoch 167/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0718 - mae: 0.1955 - val_loss: 0.1185 - val_mae: 0.2391\n",
      "Epoch 168/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0710 - mae: 0.1946 - val_loss: 0.1144 - val_mae: 0.2368\n",
      "Epoch 169/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0726 - mae: 0.1958 - val_loss: 0.1178 - val_mae: 0.2390\n",
      "Epoch 170/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0704 - mae: 0.1923 - val_loss: 0.1158 - val_mae: 0.2369\n",
      "Epoch 171/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0715 - mae: 0.1936 - val_loss: 0.1148 - val_mae: 0.2308\n",
      "Epoch 172/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0712 - mae: 0.1925 - val_loss: 0.1157 - val_mae: 0.2345\n",
      "Epoch 173/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0722 - mae: 0.1937 - val_loss: 0.1180 - val_mae: 0.2351\n",
      "Epoch 174/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0715 - mae: 0.1935 - val_loss: 0.1177 - val_mae: 0.2359\n",
      "Epoch 175/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0710 - mae: 0.1941 - val_loss: 0.1161 - val_mae: 0.2333\n",
      "Epoch 176/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0699 - mae: 0.1919 - val_loss: 0.1177 - val_mae: 0.2395\n",
      "Epoch 177/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0689 - mae: 0.1899 - val_loss: 0.1178 - val_mae: 0.2336\n",
      "Epoch 178/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0691 - mae: 0.1903 - val_loss: 0.1183 - val_mae: 0.2377\n",
      "Epoch 179/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0694 - mae: 0.1913 - val_loss: 0.1187 - val_mae: 0.2398\n",
      "Epoch 180/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0687 - mae: 0.1900 - val_loss: 0.1157 - val_mae: 0.2381\n",
      "Epoch 181/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0715 - mae: 0.1943 - val_loss: 0.1142 - val_mae: 0.2263\n",
      "Epoch 182/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0700 - mae: 0.1915 - val_loss: 0.1159 - val_mae: 0.2364\n",
      "Epoch 183/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0700 - mae: 0.1920 - val_loss: 0.1166 - val_mae: 0.2354\n",
      "Epoch 184/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0696 - mae: 0.1912 - val_loss: 0.1171 - val_mae: 0.2339\n",
      "Epoch 185/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0703 - mae: 0.1910 - val_loss: 0.1135 - val_mae: 0.2328\n",
      "Epoch 186/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0689 - mae: 0.1893 - val_loss: 0.1183 - val_mae: 0.2368\n",
      "Epoch 187/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0695 - mae: 0.1904 - val_loss: 0.1175 - val_mae: 0.2335\n",
      "Epoch 188/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0675 - mae: 0.1883 - val_loss: 0.1178 - val_mae: 0.2312\n",
      "Epoch 189/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0690 - mae: 0.1894 - val_loss: 0.1172 - val_mae: 0.2364\n",
      "Epoch 190/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0671 - mae: 0.1864 - val_loss: 0.1157 - val_mae: 0.2319\n",
      "Epoch 191/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0682 - mae: 0.1891 - val_loss: 0.1135 - val_mae: 0.2306\n",
      "Epoch 192/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0681 - mae: 0.1884 - val_loss: 0.1161 - val_mae: 0.2338\n",
      "Epoch 193/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0687 - mae: 0.1895 - val_loss: 0.1161 - val_mae: 0.2352\n",
      "Epoch 194/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0669 - mae: 0.1870 - val_loss: 0.1148 - val_mae: 0.2331\n",
      "Epoch 195/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0672 - mae: 0.1881 - val_loss: 0.1172 - val_mae: 0.2321\n",
      "Epoch 196/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0660 - mae: 0.1854 - val_loss: 0.1147 - val_mae: 0.2308\n",
      "Epoch 197/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0664 - mae: 0.1861 - val_loss: 0.1197 - val_mae: 0.2414\n",
      "Epoch 198/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0691 - mae: 0.1907 - val_loss: 0.1188 - val_mae: 0.2378\n",
      "Epoch 199/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0657 - mae: 0.1848 - val_loss: 0.1166 - val_mae: 0.2331\n",
      "Epoch 200/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0661 - mae: 0.1855 - val_loss: 0.1158 - val_mae: 0.2316\n",
      "Epoch 201/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0662 - mae: 0.1856 - val_loss: 0.1163 - val_mae: 0.2344\n",
      "Epoch 202/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0683 - mae: 0.1868 - val_loss: 0.1149 - val_mae: 0.2299\n",
      "Epoch 203/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0659 - mae: 0.1858 - val_loss: 0.1192 - val_mae: 0.2362\n",
      "Epoch 204/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0644 - mae: 0.1831 - val_loss: 0.1173 - val_mae: 0.2356\n",
      "Epoch 205/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0662 - mae: 0.1837 - val_loss: 0.1159 - val_mae: 0.2299\n",
      "Epoch 206/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0649 - mae: 0.1839 - val_loss: 0.1178 - val_mae: 0.2384\n",
      "Epoch 207/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0655 - mae: 0.1846 - val_loss: 0.1163 - val_mae: 0.2329\n",
      "Epoch 208/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0658 - mae: 0.1859 - val_loss: 0.1159 - val_mae: 0.2315\n",
      "Epoch 209/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0652 - mae: 0.1840 - val_loss: 0.1143 - val_mae: 0.2324\n",
      "Epoch 210/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0646 - mae: 0.1831 - val_loss: 0.1178 - val_mae: 0.2336\n",
      "Epoch 211/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0643 - mae: 0.1833 - val_loss: 0.1139 - val_mae: 0.2275\n",
      "Epoch 212/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0643 - mae: 0.1812 - val_loss: 0.1176 - val_mae: 0.2327\n",
      "Epoch 213/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0657 - mae: 0.1843 - val_loss: 0.1149 - val_mae: 0.2313\n",
      "Epoch 214/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0667 - mae: 0.1857 - val_loss: 0.1158 - val_mae: 0.2320\n",
      "Epoch 215/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0631 - mae: 0.1802 - val_loss: 0.1161 - val_mae: 0.2342\n",
      "Epoch 216/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0650 - mae: 0.1842 - val_loss: 0.1209 - val_mae: 0.2380\n",
      "Epoch 217/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0646 - mae: 0.1825 - val_loss: 0.1158 - val_mae: 0.2336\n",
      "Epoch 218/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0630 - mae: 0.1825 - val_loss: 0.1146 - val_mae: 0.2294\n",
      "Epoch 219/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0646 - mae: 0.1821 - val_loss: 0.1149 - val_mae: 0.2307\n",
      "Epoch 220/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0637 - mae: 0.1819 - val_loss: 0.1186 - val_mae: 0.2341\n",
      "Epoch 221/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0640 - mae: 0.1825 - val_loss: 0.1159 - val_mae: 0.2348\n",
      "Epoch 222/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0619 - mae: 0.1784 - val_loss: 0.1152 - val_mae: 0.2308\n",
      "Epoch 223/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0614 - mae: 0.1783 - val_loss: 0.1144 - val_mae: 0.2285\n",
      "Epoch 224/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0626 - mae: 0.1807 - val_loss: 0.1169 - val_mae: 0.2304\n",
      "Epoch 225/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0640 - mae: 0.1829 - val_loss: 0.1160 - val_mae: 0.2335\n",
      "Epoch 226/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0602 - mae: 0.1771 - val_loss: 0.1153 - val_mae: 0.2287\n",
      "Epoch 227/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0625 - mae: 0.1798 - val_loss: 0.1178 - val_mae: 0.2301\n",
      "Epoch 228/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0636 - mae: 0.1805 - val_loss: 0.1184 - val_mae: 0.2368\n",
      "Epoch 229/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0644 - mae: 0.1828 - val_loss: 0.1186 - val_mae: 0.2306\n",
      "Epoch 230/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0621 - mae: 0.1791 - val_loss: 0.1174 - val_mae: 0.2341\n",
      "Epoch 231/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0630 - mae: 0.1808 - val_loss: 0.1142 - val_mae: 0.2277\n",
      "Epoch 232/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0634 - mae: 0.1819 - val_loss: 0.1150 - val_mae: 0.2323\n",
      "Epoch 233/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0617 - mae: 0.1789 - val_loss: 0.1194 - val_mae: 0.2328\n",
      "Epoch 234/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0634 - mae: 0.1805 - val_loss: 0.1152 - val_mae: 0.2333\n",
      "Epoch 235/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0618 - mae: 0.1785 - val_loss: 0.1158 - val_mae: 0.2319\n",
      "Epoch 236/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0622 - mae: 0.1784 - val_loss: 0.1162 - val_mae: 0.2320\n",
      "Epoch 237/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0615 - mae: 0.1777 - val_loss: 0.1158 - val_mae: 0.2304\n",
      "Epoch 238/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0590 - mae: 0.1741 - val_loss: 0.1165 - val_mae: 0.2320\n",
      "Epoch 239/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0610 - mae: 0.1779 - val_loss: 0.1135 - val_mae: 0.2265\n",
      "Epoch 240/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0618 - mae: 0.1786 - val_loss: 0.1139 - val_mae: 0.2313\n",
      "Epoch 241/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0631 - mae: 0.1790 - val_loss: 0.1186 - val_mae: 0.2393\n",
      "Epoch 242/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0631 - mae: 0.1802 - val_loss: 0.1161 - val_mae: 0.2330\n",
      "Epoch 243/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0599 - mae: 0.1761 - val_loss: 0.1149 - val_mae: 0.2308\n",
      "Epoch 244/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0606 - mae: 0.1771 - val_loss: 0.1164 - val_mae: 0.2341\n",
      "Epoch 245/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0610 - mae: 0.1773 - val_loss: 0.1179 - val_mae: 0.2366\n",
      "Epoch 246/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0616 - mae: 0.1781 - val_loss: 0.1156 - val_mae: 0.2300\n",
      "Epoch 247/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0605 - mae: 0.1753 - val_loss: 0.1177 - val_mae: 0.2349\n",
      "Epoch 248/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0607 - mae: 0.1771 - val_loss: 0.1165 - val_mae: 0.2302\n",
      "Epoch 249/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0602 - mae: 0.1767 - val_loss: 0.1163 - val_mae: 0.2316\n",
      "Epoch 250/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0619 - mae: 0.1787 - val_loss: 0.1189 - val_mae: 0.2366\n",
      "Epoch 251/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0602 - mae: 0.1765 - val_loss: 0.1171 - val_mae: 0.2324\n",
      "Epoch 252/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0612 - mae: 0.1780 - val_loss: 0.1148 - val_mae: 0.2297\n",
      "Epoch 253/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0606 - mae: 0.1751 - val_loss: 0.1167 - val_mae: 0.2328\n",
      "Epoch 254/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0601 - mae: 0.1759 - val_loss: 0.1142 - val_mae: 0.2296\n",
      "Epoch 255/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0595 - mae: 0.1758 - val_loss: 0.1163 - val_mae: 0.2358\n",
      "Epoch 256/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0584 - mae: 0.1744 - val_loss: 0.1145 - val_mae: 0.2295\n",
      "Epoch 257/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0594 - mae: 0.1755 - val_loss: 0.1150 - val_mae: 0.2296\n",
      "Epoch 258/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0610 - mae: 0.1768 - val_loss: 0.1166 - val_mae: 0.2286\n",
      "Epoch 259/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0606 - mae: 0.1768 - val_loss: 0.1154 - val_mae: 0.2292\n",
      "Epoch 260/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0598 - mae: 0.1755 - val_loss: 0.1156 - val_mae: 0.2293\n",
      "Epoch 261/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0582 - mae: 0.1736 - val_loss: 0.1145 - val_mae: 0.2294\n",
      "Epoch 262/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0585 - mae: 0.1734 - val_loss: 0.1160 - val_mae: 0.2288\n",
      "Epoch 263/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0596 - mae: 0.1753 - val_loss: 0.1181 - val_mae: 0.2353\n",
      "Epoch 264/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.0583 - mae: 0.1733 - val_loss: 0.1133 - val_mae: 0.2303\n",
      "Epoch 265/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0607 - mae: 0.1764 - val_loss: 0.1119 - val_mae: 0.2245\n",
      "Epoch 266/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0604 - mae: 0.1740 - val_loss: 0.1128 - val_mae: 0.2274\n",
      "Epoch 267/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0589 - mae: 0.1735 - val_loss: 0.1170 - val_mae: 0.2321\n",
      "Epoch 268/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0605 - mae: 0.1749 - val_loss: 0.1137 - val_mae: 0.2271\n",
      "Epoch 269/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0605 - mae: 0.1758 - val_loss: 0.1140 - val_mae: 0.2257\n",
      "Epoch 270/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0582 - mae: 0.1731 - val_loss: 0.1123 - val_mae: 0.2268\n",
      "Epoch 271/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0578 - mae: 0.1720 - val_loss: 0.1153 - val_mae: 0.2282\n",
      "Epoch 272/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0582 - mae: 0.1719 - val_loss: 0.1193 - val_mae: 0.2350\n",
      "Epoch 273/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0588 - mae: 0.1732 - val_loss: 0.1156 - val_mae: 0.2286\n",
      "Epoch 274/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0582 - mae: 0.1715 - val_loss: 0.1140 - val_mae: 0.2262\n",
      "Epoch 275/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0574 - mae: 0.1713 - val_loss: 0.1145 - val_mae: 0.2308\n",
      "Epoch 276/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0567 - mae: 0.1700 - val_loss: 0.1147 - val_mae: 0.2274\n",
      "Epoch 277/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0580 - mae: 0.1730 - val_loss: 0.1161 - val_mae: 0.2302\n",
      "Epoch 278/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0577 - mae: 0.1712 - val_loss: 0.1149 - val_mae: 0.2298\n",
      "Epoch 279/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0568 - mae: 0.1702 - val_loss: 0.1152 - val_mae: 0.2269\n",
      "Epoch 280/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0586 - mae: 0.1726 - val_loss: 0.1170 - val_mae: 0.2318\n",
      "Epoch 281/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0581 - mae: 0.1722 - val_loss: 0.1183 - val_mae: 0.2314\n",
      "Epoch 282/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0574 - mae: 0.1710 - val_loss: 0.1166 - val_mae: 0.2288\n",
      "Epoch 283/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0562 - mae: 0.1702 - val_loss: 0.1184 - val_mae: 0.2316\n",
      "Epoch 284/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0568 - mae: 0.1704 - val_loss: 0.1152 - val_mae: 0.2308\n",
      "Epoch 285/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0565 - mae: 0.1704 - val_loss: 0.1158 - val_mae: 0.2312\n",
      "Epoch 286/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0585 - mae: 0.1722 - val_loss: 0.1165 - val_mae: 0.2298\n",
      "Epoch 287/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0564 - mae: 0.1688 - val_loss: 0.1153 - val_mae: 0.2270\n",
      "Epoch 288/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0571 - mae: 0.1702 - val_loss: 0.1149 - val_mae: 0.2266\n",
      "Epoch 289/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0571 - mae: 0.1706 - val_loss: 0.1164 - val_mae: 0.2318\n",
      "Epoch 290/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0561 - mae: 0.1692 - val_loss: 0.1161 - val_mae: 0.2281\n",
      "Epoch 291/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0573 - mae: 0.1708 - val_loss: 0.1155 - val_mae: 0.2323\n",
      "Epoch 292/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0557 - mae: 0.1690 - val_loss: 0.1157 - val_mae: 0.2282\n",
      "Epoch 293/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0564 - mae: 0.1699 - val_loss: 0.1183 - val_mae: 0.2315\n",
      "Epoch 294/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0561 - mae: 0.1689 - val_loss: 0.1146 - val_mae: 0.2282\n",
      "Epoch 295/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0564 - mae: 0.1681 - val_loss: 0.1165 - val_mae: 0.2297\n",
      "Epoch 296/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0551 - mae: 0.1673 - val_loss: 0.1200 - val_mae: 0.2360\n",
      "Epoch 297/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0554 - mae: 0.1670 - val_loss: 0.1134 - val_mae: 0.2251\n",
      "Epoch 298/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0559 - mae: 0.1684 - val_loss: 0.1175 - val_mae: 0.2290\n",
      "Epoch 299/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0562 - mae: 0.1684 - val_loss: 0.1161 - val_mae: 0.2290\n",
      "Epoch 300/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0542 - mae: 0.1658 - val_loss: 0.1172 - val_mae: 0.2307\n",
      "Epoch 301/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0552 - mae: 0.1680 - val_loss: 0.1158 - val_mae: 0.2310\n",
      "Epoch 302/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0552 - mae: 0.1686 - val_loss: 0.1153 - val_mae: 0.2286\n",
      "Epoch 303/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0565 - mae: 0.1702 - val_loss: 0.1161 - val_mae: 0.2290\n",
      "Epoch 304/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0566 - mae: 0.1694 - val_loss: 0.1149 - val_mae: 0.2281\n",
      "Epoch 305/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0532 - mae: 0.1650 - val_loss: 0.1142 - val_mae: 0.2246\n",
      "Epoch 306/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0550 - mae: 0.1652 - val_loss: 0.1184 - val_mae: 0.2331\n",
      "Epoch 307/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0553 - mae: 0.1673 - val_loss: 0.1115 - val_mae: 0.2224\n",
      "Epoch 308/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0535 - mae: 0.1654 - val_loss: 0.1130 - val_mae: 0.2251\n",
      "Epoch 309/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0554 - mae: 0.1678 - val_loss: 0.1136 - val_mae: 0.2257\n",
      "Epoch 310/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0551 - mae: 0.1667 - val_loss: 0.1148 - val_mae: 0.2249\n",
      "Epoch 311/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0540 - mae: 0.1662 - val_loss: 0.1148 - val_mae: 0.2278\n",
      "Epoch 312/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0549 - mae: 0.1672 - val_loss: 0.1150 - val_mae: 0.2251\n",
      "Epoch 313/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0546 - mae: 0.1641 - val_loss: 0.1139 - val_mae: 0.2270\n",
      "Epoch 314/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0530 - mae: 0.1634 - val_loss: 0.1110 - val_mae: 0.2209\n",
      "Epoch 315/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0522 - mae: 0.1632 - val_loss: 0.1158 - val_mae: 0.2274\n",
      "Epoch 316/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0549 - mae: 0.1663 - val_loss: 0.1165 - val_mae: 0.2232\n",
      "Epoch 317/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0549 - mae: 0.1665 - val_loss: 0.1146 - val_mae: 0.2268\n",
      "Epoch 318/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0539 - mae: 0.1648 - val_loss: 0.1122 - val_mae: 0.2217\n",
      "Epoch 319/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0547 - mae: 0.1671 - val_loss: 0.1144 - val_mae: 0.2267\n",
      "Epoch 320/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0549 - mae: 0.1671 - val_loss: 0.1118 - val_mae: 0.2211\n",
      "Epoch 321/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0537 - mae: 0.1648 - val_loss: 0.1148 - val_mae: 0.2277\n",
      "Epoch 322/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0531 - mae: 0.1650 - val_loss: 0.1148 - val_mae: 0.2248\n",
      "Epoch 323/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0541 - mae: 0.1657 - val_loss: 0.1180 - val_mae: 0.2310\n",
      "Epoch 324/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0545 - mae: 0.1666 - val_loss: 0.1120 - val_mae: 0.2240\n",
      "Epoch 325/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0540 - mae: 0.1651 - val_loss: 0.1134 - val_mae: 0.2266\n",
      "Epoch 326/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0547 - mae: 0.1670 - val_loss: 0.1096 - val_mae: 0.2183\n",
      "Epoch 327/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0538 - mae: 0.1657 - val_loss: 0.1121 - val_mae: 0.2230\n",
      "Epoch 328/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0520 - mae: 0.1618 - val_loss: 0.1142 - val_mae: 0.2268\n",
      "Epoch 329/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0541 - mae: 0.1654 - val_loss: 0.1147 - val_mae: 0.2249\n",
      "Epoch 330/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0547 - mae: 0.1660 - val_loss: 0.1110 - val_mae: 0.2224\n",
      "Epoch 331/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0536 - mae: 0.1652 - val_loss: 0.1120 - val_mae: 0.2220\n",
      "Epoch 332/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0543 - mae: 0.1663 - val_loss: 0.1155 - val_mae: 0.2291\n",
      "Epoch 333/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0527 - mae: 0.1625 - val_loss: 0.1150 - val_mae: 0.2269\n",
      "Epoch 334/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0538 - mae: 0.1655 - val_loss: 0.1138 - val_mae: 0.2227\n",
      "Epoch 335/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0531 - mae: 0.1647 - val_loss: 0.1133 - val_mae: 0.2225\n",
      "Epoch 336/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0537 - mae: 0.1649 - val_loss: 0.1165 - val_mae: 0.2279\n",
      "Epoch 337/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0546 - mae: 0.1658 - val_loss: 0.1137 - val_mae: 0.2235\n",
      "Epoch 338/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0537 - mae: 0.1652 - val_loss: 0.1098 - val_mae: 0.2221\n",
      "Epoch 339/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0527 - mae: 0.1630 - val_loss: 0.1128 - val_mae: 0.2245\n",
      "Epoch 340/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0532 - mae: 0.1641 - val_loss: 0.1134 - val_mae: 0.2258\n",
      "Epoch 341/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0527 - mae: 0.1632 - val_loss: 0.1150 - val_mae: 0.2260\n",
      "Epoch 342/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0534 - mae: 0.1636 - val_loss: 0.1127 - val_mae: 0.2212\n",
      "Epoch 343/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0534 - mae: 0.1641 - val_loss: 0.1146 - val_mae: 0.2259\n",
      "Epoch 344/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0511 - mae: 0.1610 - val_loss: 0.1141 - val_mae: 0.2255\n",
      "Epoch 345/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0524 - mae: 0.1614 - val_loss: 0.1153 - val_mae: 0.2255\n",
      "Epoch 346/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0518 - mae: 0.1610 - val_loss: 0.1146 - val_mae: 0.2253\n",
      "Epoch 347/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0548 - mae: 0.1661 - val_loss: 0.1122 - val_mae: 0.2266\n",
      "Epoch 348/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0509 - mae: 0.1610 - val_loss: 0.1136 - val_mae: 0.2245\n",
      "Epoch 349/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0512 - mae: 0.1617 - val_loss: 0.1109 - val_mae: 0.2217\n",
      "Epoch 350/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0516 - mae: 0.1608 - val_loss: 0.1117 - val_mae: 0.2253\n",
      "Epoch 351/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0510 - mae: 0.1610 - val_loss: 0.1131 - val_mae: 0.2252\n",
      "Epoch 352/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0530 - mae: 0.1647 - val_loss: 0.1122 - val_mae: 0.2237\n",
      "Epoch 353/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0513 - mae: 0.1603 - val_loss: 0.1143 - val_mae: 0.2256\n",
      "Epoch 354/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0506 - mae: 0.1598 - val_loss: 0.1144 - val_mae: 0.2281\n",
      "Epoch 355/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0517 - mae: 0.1616 - val_loss: 0.1117 - val_mae: 0.2221\n",
      "Epoch 356/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0524 - mae: 0.1607 - val_loss: 0.1133 - val_mae: 0.2191\n",
      "Epoch 357/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0536 - mae: 0.1638 - val_loss: 0.1153 - val_mae: 0.2261\n",
      "Epoch 358/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0507 - mae: 0.1593 - val_loss: 0.1127 - val_mae: 0.2247\n",
      "Epoch 359/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0514 - mae: 0.1615 - val_loss: 0.1148 - val_mae: 0.2268\n",
      "Epoch 360/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0512 - mae: 0.1601 - val_loss: 0.1133 - val_mae: 0.2245\n",
      "Epoch 361/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0510 - mae: 0.1594 - val_loss: 0.1141 - val_mae: 0.2243\n",
      "Epoch 362/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0514 - mae: 0.1613 - val_loss: 0.1140 - val_mae: 0.2249\n",
      "Epoch 363/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0539 - mae: 0.1627 - val_loss: 0.1124 - val_mae: 0.2204\n",
      "Epoch 364/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0514 - mae: 0.1609 - val_loss: 0.1112 - val_mae: 0.2211\n",
      "Epoch 365/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0504 - mae: 0.1591 - val_loss: 0.1124 - val_mae: 0.2229\n",
      "Epoch 366/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0504 - mae: 0.1590 - val_loss: 0.1114 - val_mae: 0.2201\n",
      "Epoch 367/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0491 - mae: 0.1566 - val_loss: 0.1160 - val_mae: 0.2276\n",
      "Epoch 368/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0520 - mae: 0.1614 - val_loss: 0.1114 - val_mae: 0.2241\n",
      "Epoch 369/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0525 - mae: 0.1612 - val_loss: 0.1112 - val_mae: 0.2238\n",
      "Epoch 370/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0501 - mae: 0.1585 - val_loss: 0.1133 - val_mae: 0.2221\n",
      "Epoch 371/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0511 - mae: 0.1607 - val_loss: 0.1103 - val_mae: 0.2229\n",
      "Epoch 372/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0514 - mae: 0.1610 - val_loss: 0.1106 - val_mae: 0.2196\n",
      "Epoch 373/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0526 - mae: 0.1617 - val_loss: 0.1119 - val_mae: 0.2235\n",
      "Epoch 374/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0495 - mae: 0.1584 - val_loss: 0.1122 - val_mae: 0.2230\n",
      "Epoch 375/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0521 - mae: 0.1620 - val_loss: 0.1136 - val_mae: 0.2271\n",
      "Epoch 376/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0498 - mae: 0.1589 - val_loss: 0.1133 - val_mae: 0.2234\n",
      "Epoch 377/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0495 - mae: 0.1573 - val_loss: 0.1146 - val_mae: 0.2251\n",
      "Epoch 378/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0504 - mae: 0.1589 - val_loss: 0.1136 - val_mae: 0.2244\n",
      "Epoch 379/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0510 - mae: 0.1602 - val_loss: 0.1140 - val_mae: 0.2268\n",
      "Epoch 380/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0512 - mae: 0.1598 - val_loss: 0.1114 - val_mae: 0.2195\n",
      "Epoch 381/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0494 - mae: 0.1567 - val_loss: 0.1116 - val_mae: 0.2227\n",
      "Epoch 382/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0515 - mae: 0.1607 - val_loss: 0.1121 - val_mae: 0.2202\n",
      "Epoch 383/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0521 - mae: 0.1613 - val_loss: 0.1144 - val_mae: 0.2256\n",
      "Epoch 384/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0506 - mae: 0.1596 - val_loss: 0.1148 - val_mae: 0.2275\n",
      "Epoch 385/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0491 - mae: 0.1568 - val_loss: 0.1134 - val_mae: 0.2227\n",
      "Epoch 386/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0484 - mae: 0.1564 - val_loss: 0.1155 - val_mae: 0.2283\n",
      "Epoch 387/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0515 - mae: 0.1597 - val_loss: 0.1150 - val_mae: 0.2263\n",
      "Epoch 388/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0507 - mae: 0.1593 - val_loss: 0.1115 - val_mae: 0.2213\n",
      "Epoch 389/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0497 - mae: 0.1575 - val_loss: 0.1120 - val_mae: 0.2216\n",
      "Epoch 390/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0496 - mae: 0.1582 - val_loss: 0.1148 - val_mae: 0.2251\n",
      "Epoch 391/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0480 - mae: 0.1557 - val_loss: 0.1146 - val_mae: 0.2229\n",
      "Epoch 392/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0500 - mae: 0.1577 - val_loss: 0.1169 - val_mae: 0.2299\n",
      "Epoch 393/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0484 - mae: 0.1559 - val_loss: 0.1118 - val_mae: 0.2218\n",
      "Epoch 394/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0503 - mae: 0.1584 - val_loss: 0.1112 - val_mae: 0.2198\n",
      "Epoch 395/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0477 - mae: 0.1544 - val_loss: 0.1161 - val_mae: 0.2283\n",
      "Epoch 396/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0487 - mae: 0.1566 - val_loss: 0.1175 - val_mae: 0.2306\n",
      "Epoch 397/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0482 - mae: 0.1560 - val_loss: 0.1135 - val_mae: 0.2234\n",
      "Epoch 398/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0487 - mae: 0.1551 - val_loss: 0.1108 - val_mae: 0.2198\n",
      "Epoch 399/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0506 - mae: 0.1595 - val_loss: 0.1139 - val_mae: 0.2238\n",
      "Epoch 400/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0490 - mae: 0.1563 - val_loss: 0.1140 - val_mae: 0.2222\n",
      "Epoch 401/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0493 - mae: 0.1566 - val_loss: 0.1122 - val_mae: 0.2234\n",
      "Epoch 402/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0497 - mae: 0.1588 - val_loss: 0.1134 - val_mae: 0.2217\n",
      "Epoch 403/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0487 - mae: 0.1568 - val_loss: 0.1103 - val_mae: 0.2178\n",
      "Epoch 404/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0483 - mae: 0.1551 - val_loss: 0.1129 - val_mae: 0.2215\n",
      "Epoch 405/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0489 - mae: 0.1564 - val_loss: 0.1128 - val_mae: 0.2263\n",
      "Epoch 406/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0507 - mae: 0.1597 - val_loss: 0.1113 - val_mae: 0.2225\n",
      "Epoch 407/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0485 - mae: 0.1563 - val_loss: 0.1174 - val_mae: 0.2302\n",
      "Epoch 408/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0487 - mae: 0.1559 - val_loss: 0.1113 - val_mae: 0.2206\n",
      "Epoch 409/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0493 - mae: 0.1561 - val_loss: 0.1118 - val_mae: 0.2244\n",
      "Epoch 410/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0494 - mae: 0.1574 - val_loss: 0.1140 - val_mae: 0.2245\n",
      "Epoch 411/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0490 - mae: 0.1555 - val_loss: 0.1116 - val_mae: 0.2226\n",
      "Epoch 412/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0490 - mae: 0.1572 - val_loss: 0.1144 - val_mae: 0.2284\n",
      "Epoch 413/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0489 - mae: 0.1565 - val_loss: 0.1110 - val_mae: 0.2228\n",
      "Epoch 414/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0483 - mae: 0.1563 - val_loss: 0.1108 - val_mae: 0.2201\n",
      "Epoch 415/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0497 - mae: 0.1571 - val_loss: 0.1122 - val_mae: 0.2210\n",
      "Epoch 416/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0487 - mae: 0.1554 - val_loss: 0.1107 - val_mae: 0.2202\n",
      "Epoch 417/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0494 - mae: 0.1569 - val_loss: 0.1127 - val_mae: 0.2231\n",
      "Epoch 418/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0496 - mae: 0.1583 - val_loss: 0.1129 - val_mae: 0.2226\n",
      "Epoch 419/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0484 - mae: 0.1558 - val_loss: 0.1098 - val_mae: 0.2198\n",
      "Epoch 420/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0482 - mae: 0.1548 - val_loss: 0.1141 - val_mae: 0.2221\n",
      "Epoch 421/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0478 - mae: 0.1546 - val_loss: 0.1131 - val_mae: 0.2224\n",
      "Epoch 422/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0494 - mae: 0.1555 - val_loss: 0.1121 - val_mae: 0.2211\n",
      "Epoch 423/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0490 - mae: 0.1557 - val_loss: 0.1117 - val_mae: 0.2211\n",
      "Epoch 424/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0496 - mae: 0.1567 - val_loss: 0.1119 - val_mae: 0.2218\n",
      "Epoch 425/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0484 - mae: 0.1551 - val_loss: 0.1116 - val_mae: 0.2221\n",
      "Epoch 426/2000\n",
      "566/566 [==============================] - 5s 9ms/step - loss: 0.0472 - mae: 0.1522 - val_loss: 0.1117 - val_mae: 0.2213\n",
      "Models saved at: model/Sun Jul 21 04:22:56 2024\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "def load_catalogs(folder: str):\n",
    "    _img_name = []\n",
    "    _angle = []\n",
    "    _throttle = []\n",
    "\n",
    "    for _file in sorted(glob.glob(f\"{folder}/*.catalog\"),\n",
    "                        key=lambda x: [\n",
    "                            int(c) if c.isdigit()\n",
    "                            else c for c in re.split(r'(\\d+)', x)]):\n",
    "        with open(_file) as f:\n",
    "            for _line in f:\n",
    "                _img_name.append(_line.split()[7][1:-2])\n",
    "                _angle.append(float(_line.split()[9][0:-1]))\n",
    "                _throttle.append(float(_line.split()[13][0:-1]))\n",
    "\n",
    "    print(f'Image count: {len(_img_name)}')\n",
    "    return _img_name, _angle, _throttle\n",
    "\n",
    "\n",
    "def load_images(_img_name: list, folder: str):\n",
    "    _image = []\n",
    "    for i in range(len(_img_name)):\n",
    "        _img = cv2.imread(os.path.join(f\"{folder}/images\", _img_name[i]))\n",
    "        assert _img.shape == (224, 224, 3),\\\n",
    "            \"img %s has shape %r\" % (_img_name[i], _img.shape)\n",
    "        _image.append(_img)\n",
    "    return _image\n",
    "\n",
    "\n",
    "def data_preprocessing(_throttle, _angle, _image):\n",
    "    _throttle = np.array(_throttle)\n",
    "    _steering = np.array(_angle)\n",
    "    _train_img = np.array(_image)\n",
    "    _label = _steering\n",
    "    _cut_height = 80\n",
    "    _train_img_cut_orig = _train_img[:, _cut_height:224, :]\n",
    "    _train_img_cut_gray = _train_img_cut_orig\n",
    "    return _train_img_cut_orig, _train_img_cut_gray, _label\n",
    "\n",
    "\n",
    "def train_split(_train_img_cut_orig, _train_img_cut_gray, _label):\n",
    "    _X_train, _X_val, _y_train, _y_val = train_test_split(\n",
    "        _train_img_cut_gray, _label,\n",
    "        test_size=0.15, random_state=42)\n",
    "    return _X_train, _X_val, _y_train, _y_val\n",
    "\n",
    "\n",
    "def build_fine_tuned_efficientnet_model(input_shape):\n",
    "    base_model = tf.keras.applications.EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    _x = base_model.output\n",
    "    _x = GlobalAveragePooling2D()(_x)\n",
    "    _x = Dense(1024, activation='relu')(_x)\n",
    "    _x = Dropout(0.5)(_x)\n",
    "    _outputs = Dense(1, activation='linear')(_x)\n",
    "\n",
    "    _model = Model(inputs=base_model.input, outputs=_outputs)\n",
    "    return _model\n",
    "\n",
    "\n",
    "def train_start(_model, _X_train, _X_val, _y_train, _y_val, \n",
    "                epochs: int=100, batch_size: int=16, patience: int=100, save_folder: str=''):\n",
    "    _optimizer = tf.optimizers.Adam(learning_rate=0.0001,\n",
    "                                    beta_1=0.9, beta_2=0.999)\n",
    "    _model.compile(optimizer=_optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
    "    _model.summary()\n",
    "    \n",
    "    # Add EarlyStopping callback\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                  patience=patience, \n",
    "                                                  restore_best_weights=True)\n",
    "    \n",
    "    # Add ModelCheckpoint callback to save the best model\n",
    "    best_model_path = os.path.join(save_folder, \"best_model.h5\")\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_path, \n",
    "                                                          monitor='val_loss', \n",
    "                                                          save_best_only=True)\n",
    "    \n",
    "    _trained_model = _model.fit(_X_train, _y_train,\n",
    "                                epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(_X_val, _y_val),\n",
    "                                callbacks=[early_stop, model_checkpoint])\n",
    "    return _trained_model\n",
    "\n",
    "\n",
    "def plot_trained_model(_trained_model, \n",
    "                       show: bool=False,\n",
    "                       save: bool=True,\n",
    "                       save_folder: str=''):\n",
    "    \n",
    "    history = _trained_model.history\n",
    "\n",
    "    plt.plot(history['loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(save_folder, f'Loss.png'),\n",
    "                    bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    plt.plot(history['mae'], label='Train MAE')\n",
    "    plt.plot(history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(save_folder, f'MAE.png'),\n",
    "                    bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"../data/data_0202\"\n",
    "    save_folder = f\"model/{time.ctime(time.time())}\"\n",
    "    # create save path\n",
    "    if not os.path.isdir(save_folder):\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    img_name, angle, throttle = load_catalogs(data_folder)\n",
    "    image = load_images(img_name, data_folder)\n",
    "    image = np.array(image)\n",
    "    train_img_cut_orig, train_img_cut_gray, label = data_preprocessing(\n",
    "        throttle, angle, image)\n",
    "    X_train, X_val, y_train, y_val = train_split(\n",
    "        train_img_cut_orig, train_img_cut_gray, label)\n",
    "\n",
    "    # Update input shape for EfficientNet\n",
    "    model = build_fine_tuned_efficientnet_model(input_shape=(144, 224, 3))\n",
    "    trained_model = train_start(model, X_train, X_val, y_train, y_val, \n",
    "                               epochs=2000, save_folder=save_folder)\n",
    "    plot_trained_model(trained_model, show=False, save=True, save_folder=save_folder)\n",
    "    \n",
    "    # Save the last model\n",
    "    model.save(os.path.join(save_folder, \"last_model.h5\"))\n",
    "    \n",
    "    print(f\"Models saved at: {save_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb129d26-2597-44b5-9c75-8b5869e1740d",
   "metadata": {},
   "source": [
    "### Models saved at: model/Sun Jul 21 04:22:56 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6840e8-3746-4486-8b03-79e0370dfaef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.7.0",
   "language": "python",
   "name": "tensorflow-2.7.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
