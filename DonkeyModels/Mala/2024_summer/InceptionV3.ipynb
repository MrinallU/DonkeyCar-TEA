{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43dcd366-5961-4b74-9ad0-45c67b8554ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count: 10645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 22:23:59.714646: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-21 22:24:01.821316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78902 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:47:00.0, compute capability: 8.0\n",
      "2024-07-21 22:24:01.824295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78902 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:4e:00.0, compute capability: 8.0\n",
      "2024-07-21 22:24:01.825912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 78902 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:b7:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 1s 0us/step\n",
      "87924736/87910968 [==============================] - 1s 0us/step\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 144, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 71, 111, 32)  864         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 71, 111, 32)  96         ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 71, 111, 32)  0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 69, 109, 32)  9216        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 69, 109, 32)  96         ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 69, 109, 32)  0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 69, 109, 64)  18432       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 69, 109, 64)  192        ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 69, 109, 64)  0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 34, 54, 64)   0           ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 34, 54, 80)   5120        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 34, 54, 80)  240         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 34, 54, 80)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 52, 192)  138240      ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 52, 192)  576        ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 52, 192)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 15, 25, 192)  0          ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 15, 25, 64)   12288       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 15, 25, 64)  192         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 15, 25, 64)   0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 15, 25, 48)   9216        ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 15, 25, 96)   55296       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 15, 25, 48)  144         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 15, 25, 96)  288         ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 15, 25, 48)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 15, 25, 96)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 15, 25, 192)  0          ['max_pooling2d_1[0][0]']        \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 15, 25, 64)   12288       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 15, 25, 64)   76800       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 15, 25, 96)   82944       ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 15, 25, 32)   6144        ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 15, 25, 64)  192         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 15, 25, 64)  192         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 15, 25, 96)  288         ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 15, 25, 32)  96          ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 15, 25, 64)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 15, 25, 64)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 15, 25, 96)   0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 15, 25, 32)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " mixed0 (Concatenate)           (None, 15, 25, 256)  0           ['activation_5[0][0]',           \n",
      "                                                                  'activation_7[0][0]',           \n",
      "                                                                  'activation_10[0][0]',          \n",
      "                                                                  'activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 15, 25, 64)   16384       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 15, 25, 64)  192         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 15, 25, 64)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 15, 25, 48)   12288       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 15, 25, 96)   55296       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 15, 25, 48)  144         ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 15, 25, 96)  288         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 15, 25, 48)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 15, 25, 96)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 15, 25, 256)  0          ['mixed0[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 15, 25, 64)   16384       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 15, 25, 64)   76800       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 15, 25, 96)   82944       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 15, 25, 64)   16384       ['average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 15, 25, 64)  192         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 15, 25, 64)  192         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 15, 25, 96)  288         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 15, 25, 64)  192         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 15, 25, 64)   0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 15, 25, 64)   0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 15, 25, 96)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 15, 25, 64)   0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " mixed1 (Concatenate)           (None, 15, 25, 288)  0           ['activation_12[0][0]',          \n",
      "                                                                  'activation_14[0][0]',          \n",
      "                                                                  'activation_17[0][0]',          \n",
      "                                                                  'activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 15, 25, 64)   18432       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 15, 25, 64)  192         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 15, 25, 64)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 15, 25, 48)   13824       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 15, 25, 96)   55296       ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 15, 25, 48)  144         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 15, 25, 96)  288         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 15, 25, 48)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 15, 25, 96)   0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 15, 25, 288)  0          ['mixed1[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 15, 25, 64)   18432       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 15, 25, 64)   76800       ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 15, 25, 96)   82944       ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 15, 25, 64)   18432       ['average_pooling2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 15, 25, 64)  192         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 15, 25, 64)  192         ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 15, 25, 96)  288         ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 15, 25, 64)  192         ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 15, 25, 64)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 15, 25, 64)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 15, 25, 96)   0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 15, 25, 64)   0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " mixed2 (Concatenate)           (None, 15, 25, 288)  0           ['activation_19[0][0]',          \n",
      "                                                                  'activation_21[0][0]',          \n",
      "                                                                  'activation_24[0][0]',          \n",
      "                                                                  'activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 15, 25, 64)   18432       ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 15, 25, 64)  192         ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 15, 25, 64)   0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 15, 25, 96)   55296       ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 15, 25, 96)  288         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 15, 25, 96)   0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 7, 12, 384)   995328      ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 7, 12, 96)    82944       ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 7, 12, 384)  1152        ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 7, 12, 96)   288         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 7, 12, 384)   0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 7, 12, 96)    0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 7, 12, 288)  0           ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " mixed3 (Concatenate)           (None, 7, 12, 768)   0           ['activation_26[0][0]',          \n",
      "                                                                  'activation_29[0][0]',          \n",
      "                                                                  'max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 7, 12, 128)   98304       ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 7, 12, 128)  384         ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 7, 12, 128)   0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 7, 12, 128)   114688      ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 7, 12, 128)  384         ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 7, 12, 128)   0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 7, 12, 128)   98304       ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 7, 12, 128)   114688      ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 7, 12, 128)  384         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 7, 12, 128)  384         ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 7, 12, 128)   0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 7, 12, 128)   0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 7, 12, 128)   114688      ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 7, 12, 128)   114688      ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 7, 12, 128)  384         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 7, 12, 128)  384         ['conv2d_37[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 7, 12, 128)   0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 7, 12, 128)   0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (AveragePo  (None, 7, 12, 768)  0           ['mixed3[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 7, 12, 192)   147456      ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 7, 12, 192)   172032      ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 7, 12, 192)   172032      ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 7, 12, 192)   147456      ['average_pooling2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 7, 12, 192)  576         ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 7, 12, 192)  576         ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 7, 12, 192)  576         ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 7, 12, 192)  576         ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " mixed4 (Concatenate)           (None, 7, 12, 768)   0           ['activation_30[0][0]',          \n",
      "                                                                  'activation_33[0][0]',          \n",
      "                                                                  'activation_38[0][0]',          \n",
      "                                                                  'activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 7, 12, 160)   122880      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 7, 12, 160)  480         ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 7, 12, 160)   0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 7, 12, 160)   179200      ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 7, 12, 160)  480         ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 7, 12, 160)   0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 7, 12, 160)   122880      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 7, 12, 160)   179200      ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 7, 12, 160)  480         ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 7, 12, 160)  480         ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 7, 12, 160)   0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 7, 12, 160)   0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 7, 12, 160)   179200      ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 7, 12, 160)   179200      ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 7, 12, 160)  480         ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 7, 12, 160)  480         ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 7, 12, 160)   0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 7, 12, 160)   0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_4 (AveragePo  (None, 7, 12, 768)  0           ['mixed4[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 7, 12, 192)   147456      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 7, 12, 192)   215040      ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 7, 12, 192)   215040      ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 7, 12, 192)   147456      ['average_pooling2d_4[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 7, 12, 192)  576         ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 7, 12, 192)  576         ['conv2d_43[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 7, 12, 192)  576         ['conv2d_48[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 7, 12, 192)  576         ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " mixed5 (Concatenate)           (None, 7, 12, 768)   0           ['activation_40[0][0]',          \n",
      "                                                                  'activation_43[0][0]',          \n",
      "                                                                  'activation_48[0][0]',          \n",
      "                                                                  'activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 7, 12, 160)   122880      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 7, 12, 160)  480         ['conv2d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 7, 12, 160)   0           ['batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 7, 12, 160)   179200      ['activation_54[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 7, 12, 160)  480         ['conv2d_55[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 7, 12, 160)   0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 7, 12, 160)   122880      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 7, 12, 160)   179200      ['activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 7, 12, 160)  480         ['conv2d_51[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 7, 12, 160)  480         ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 7, 12, 160)   0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 7, 12, 160)   0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 7, 12, 160)   179200      ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 7, 12, 160)   179200      ['activation_56[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 7, 12, 160)  480         ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 7, 12, 160)  480         ['conv2d_57[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 7, 12, 160)   0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 7, 12, 160)   0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_5 (AveragePo  (None, 7, 12, 768)  0           ['mixed5[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 7, 12, 192)   147456      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 7, 12, 192)   215040      ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 7, 12, 192)   215040      ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 7, 12, 192)   147456      ['average_pooling2d_5[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 7, 12, 192)  576         ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 7, 12, 192)  576         ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 7, 12, 192)  576         ['conv2d_58[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 7, 12, 192)  576         ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " mixed6 (Concatenate)           (None, 7, 12, 768)   0           ['activation_50[0][0]',          \n",
      "                                                                  'activation_53[0][0]',          \n",
      "                                                                  'activation_58[0][0]',          \n",
      "                                                                  'activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 7, 12, 192)   147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 7, 12, 192)  576         ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 7, 12, 192)   258048      ['activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 7, 12, 192)  576         ['conv2d_65[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 7, 12, 192)   147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 7, 12, 192)   258048      ['activation_65[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 7, 12, 192)  576         ['conv2d_61[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 7, 12, 192)  576         ['conv2d_66[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " activation_66 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 7, 12, 192)   258048      ['activation_61[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 7, 12, 192)   258048      ['activation_66[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 7, 12, 192)  576         ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 7, 12, 192)  576         ['conv2d_67[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " activation_67 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_6 (AveragePo  (None, 7, 12, 768)  0           ['mixed6[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 7, 12, 192)   147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 7, 12, 192)   258048      ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 7, 12, 192)   258048      ['activation_67[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 7, 12, 192)   147456      ['average_pooling2d_6[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 7, 12, 192)  576         ['conv2d_60[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 7, 12, 192)  576         ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 7, 12, 192)  576         ['conv2d_68[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 7, 12, 192)  576         ['conv2d_69[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " activation_68 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " activation_69 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " mixed7 (Concatenate)           (None, 7, 12, 768)   0           ['activation_60[0][0]',          \n",
      "                                                                  'activation_63[0][0]',          \n",
      "                                                                  'activation_68[0][0]',          \n",
      "                                                                  'activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 7, 12, 192)   147456      ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 7, 12, 192)  576         ['conv2d_72[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_72 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 7, 12, 192)   258048      ['activation_72[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_73 (BatchN  (None, 7, 12, 192)  576         ['conv2d_73[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_73 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_73[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 7, 12, 192)   147456      ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 7, 12, 192)   258048      ['activation_73[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 7, 12, 192)  576         ['conv2d_70[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_74 (BatchN  (None, 7, 12, 192)  576         ['conv2d_74[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_70 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " activation_74 (Activation)     (None, 7, 12, 192)   0           ['batch_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 3, 5, 320)    552960      ['activation_70[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 3, 5, 192)    331776      ['activation_74[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 3, 5, 320)   960         ['conv2d_71[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 3, 5, 192)   576         ['conv2d_75[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_71 (Activation)     (None, 3, 5, 320)    0           ['batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " activation_75 (Activation)     (None, 3, 5, 192)    0           ['batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 3, 5, 768)   0           ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " mixed8 (Concatenate)           (None, 3, 5, 1280)   0           ['activation_71[0][0]',          \n",
      "                                                                  'activation_75[0][0]',          \n",
      "                                                                  'max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 3, 5, 448)    573440      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 3, 5, 448)   1344        ['conv2d_80[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_80 (Activation)     (None, 3, 5, 448)    0           ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 3, 5, 384)    491520      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 3, 5, 384)    1548288     ['activation_80[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 3, 5, 384)   1152        ['conv2d_77[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 3, 5, 384)   1152        ['conv2d_81[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_77 (Activation)     (None, 3, 5, 384)    0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " activation_81 (Activation)     (None, 3, 5, 384)    0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 3, 5, 384)    442368      ['activation_77[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 3, 5, 384)    442368      ['activation_77[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 3, 5, 384)    442368      ['activation_81[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 3, 5, 384)    442368      ['activation_81[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_7 (AveragePo  (None, 3, 5, 1280)  0           ['mixed8[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 3, 5, 320)    409600      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_78 (BatchN  (None, 3, 5, 384)   1152        ['conv2d_78[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_79 (BatchN  (None, 3, 5, 384)   1152        ['conv2d_79[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 3, 5, 384)   1152        ['conv2d_82[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_83 (BatchN  (None, 3, 5, 384)   1152        ['conv2d_83[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)             (None, 3, 5, 192)    245760      ['average_pooling2d_7[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 3, 5, 320)   960         ['conv2d_76[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_78 (Activation)     (None, 3, 5, 384)    0           ['batch_normalization_78[0][0]'] \n",
      "                                                                                                  \n",
      " activation_79 (Activation)     (None, 3, 5, 384)    0           ['batch_normalization_79[0][0]'] \n",
      "                                                                                                  \n",
      " activation_82 (Activation)     (None, 3, 5, 384)    0           ['batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " activation_83 (Activation)     (None, 3, 5, 384)    0           ['batch_normalization_83[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_84 (BatchN  (None, 3, 5, 192)   576         ['conv2d_84[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_76 (Activation)     (None, 3, 5, 320)    0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " mixed9_0 (Concatenate)         (None, 3, 5, 768)    0           ['activation_78[0][0]',          \n",
      "                                                                  'activation_79[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3, 5, 768)    0           ['activation_82[0][0]',          \n",
      "                                                                  'activation_83[0][0]']          \n",
      "                                                                                                  \n",
      " activation_84 (Activation)     (None, 3, 5, 192)    0           ['batch_normalization_84[0][0]'] \n",
      "                                                                                                  \n",
      " mixed9 (Concatenate)           (None, 3, 5, 2048)   0           ['activation_76[0][0]',          \n",
      "                                                                  'mixed9_0[0][0]',               \n",
      "                                                                  'concatenate[0][0]',            \n",
      "                                                                  'activation_84[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)             (None, 3, 5, 448)    917504      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_89 (BatchN  (None, 3, 5, 448)   1344        ['conv2d_89[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_89 (Activation)     (None, 3, 5, 448)    0           ['batch_normalization_89[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)             (None, 3, 5, 384)    786432      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)             (None, 3, 5, 384)    1548288     ['activation_89[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 3, 5, 384)   1152        ['conv2d_86[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 3, 5, 384)   1152        ['conv2d_90[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_86 (Activation)     (None, 3, 5, 384)    0           ['batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " activation_90 (Activation)     (None, 3, 5, 384)    0           ['batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)             (None, 3, 5, 384)    442368      ['activation_86[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)             (None, 3, 5, 384)    442368      ['activation_86[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)             (None, 3, 5, 384)    442368      ['activation_90[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)             (None, 3, 5, 384)    442368      ['activation_90[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_8 (AveragePo  (None, 3, 5, 2048)  0           ['mixed9[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)             (None, 3, 5, 320)    655360      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 3, 5, 384)   1152        ['conv2d_87[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_88 (BatchN  (None, 3, 5, 384)   1152        ['conv2d_88[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 3, 5, 384)   1152        ['conv2d_91[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 3, 5, 384)   1152        ['conv2d_92[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)             (None, 3, 5, 192)    393216      ['average_pooling2d_8[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 3, 5, 320)   960         ['conv2d_85[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_87 (Activation)     (None, 3, 5, 384)    0           ['batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " activation_88 (Activation)     (None, 3, 5, 384)    0           ['batch_normalization_88[0][0]'] \n",
      "                                                                                                  \n",
      " activation_91 (Activation)     (None, 3, 5, 384)    0           ['batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " activation_92 (Activation)     (None, 3, 5, 384)    0           ['batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_93 (BatchN  (None, 3, 5, 192)   576         ['conv2d_93[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_85 (Activation)     (None, 3, 5, 320)    0           ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " mixed9_1 (Concatenate)         (None, 3, 5, 768)    0           ['activation_87[0][0]',          \n",
      "                                                                  'activation_88[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 3, 5, 768)    0           ['activation_91[0][0]',          \n",
      "                                                                  'activation_92[0][0]']          \n",
      "                                                                                                  \n",
      " activation_93 (Activation)     (None, 3, 5, 192)    0           ['batch_normalization_93[0][0]'] \n",
      "                                                                                                  \n",
      " mixed10 (Concatenate)          (None, 3, 5, 2048)   0           ['activation_85[0][0]',          \n",
      "                                                                  'mixed9_1[0][0]',               \n",
      "                                                                  'concatenate_1[0][0]',          \n",
      "                                                                  'activation_93[0][0]']          \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 2048)        0           ['mixed10[0][0]']                \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         2098176     ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            1025        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,901,985\n",
      "Trainable params: 2,099,201\n",
      "Non-trainable params: 21,802,784\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 22:24:08.534190: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19/566 [>.............................] - ETA: 4s - loss: 916.6901 - mae: 23.5028  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 22:24:10.479251: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566/566 [==============================] - 12s 14ms/step - loss: 111.9113 - mae: 5.5521 - val_loss: 0.5598 - val_mae: 0.5960\n",
      "Epoch 2/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 1.2406 - mae: 0.6843 - val_loss: 0.3309 - val_mae: 0.3892\n",
      "Epoch 3/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.5268 - mae: 0.4611 - val_loss: 0.2094 - val_mae: 0.3507\n",
      "Epoch 4/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.3240 - mae: 0.4036 - val_loss: 0.2471 - val_mae: 0.3549\n",
      "Epoch 5/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.2565 - mae: 0.3800 - val_loss: 0.1847 - val_mae: 0.3308\n",
      "Epoch 6/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.2357 - mae: 0.3672 - val_loss: 0.1760 - val_mae: 0.3231\n",
      "Epoch 7/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.2139 - mae: 0.3562 - val_loss: 0.1797 - val_mae: 0.3203\n",
      "Epoch 8/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.2072 - mae: 0.3499 - val_loss: 0.1879 - val_mae: 0.3277\n",
      "Epoch 9/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.2073 - mae: 0.3510 - val_loss: 0.1882 - val_mae: 0.3342\n",
      "Epoch 10/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.2036 - mae: 0.3483 - val_loss: 0.1799 - val_mae: 0.3315\n",
      "Epoch 11/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.2022 - mae: 0.3466 - val_loss: 0.1749 - val_mae: 0.3186\n",
      "Epoch 12/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.2008 - mae: 0.3459 - val_loss: 0.1803 - val_mae: 0.3329\n",
      "Epoch 13/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.2011 - mae: 0.3453 - val_loss: 0.1798 - val_mae: 0.3224\n",
      "Epoch 14/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1997 - mae: 0.3443 - val_loss: 0.1755 - val_mae: 0.3082\n",
      "Epoch 15/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.2004 - mae: 0.3424 - val_loss: 0.1713 - val_mae: 0.3273\n",
      "Epoch 16/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.2019 - mae: 0.3465 - val_loss: 0.1742 - val_mae: 0.3120\n",
      "Epoch 17/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1990 - mae: 0.3420 - val_loss: 0.1781 - val_mae: 0.3087\n",
      "Epoch 18/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1981 - mae: 0.3397 - val_loss: 0.1842 - val_mae: 0.3259\n",
      "Epoch 19/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1970 - mae: 0.3391 - val_loss: 0.1731 - val_mae: 0.3093\n",
      "Epoch 20/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1978 - mae: 0.3418 - val_loss: 0.1772 - val_mae: 0.3112\n",
      "Epoch 21/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1941 - mae: 0.3388 - val_loss: 0.1785 - val_mae: 0.3092\n",
      "Epoch 22/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1976 - mae: 0.3422 - val_loss: 0.1752 - val_mae: 0.3134\n",
      "Epoch 23/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1937 - mae: 0.3365 - val_loss: 0.1725 - val_mae: 0.3160\n",
      "Epoch 24/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1930 - mae: 0.3364 - val_loss: 0.1773 - val_mae: 0.3266\n",
      "Epoch 25/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1905 - mae: 0.3342 - val_loss: 0.1684 - val_mae: 0.3057\n",
      "Epoch 26/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1916 - mae: 0.3363 - val_loss: 0.1716 - val_mae: 0.3219\n",
      "Epoch 27/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1931 - mae: 0.3361 - val_loss: 0.1678 - val_mae: 0.3113\n",
      "Epoch 28/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1934 - mae: 0.3372 - val_loss: 0.1657 - val_mae: 0.3155\n",
      "Epoch 29/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1898 - mae: 0.3328 - val_loss: 0.1659 - val_mae: 0.3065\n",
      "Epoch 30/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1905 - mae: 0.3331 - val_loss: 0.1663 - val_mae: 0.3054\n",
      "Epoch 31/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1896 - mae: 0.3329 - val_loss: 0.1676 - val_mae: 0.3125\n",
      "Epoch 32/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1932 - mae: 0.3348 - val_loss: 0.1740 - val_mae: 0.3140\n",
      "Epoch 33/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1907 - mae: 0.3335 - val_loss: 0.1767 - val_mae: 0.3165\n",
      "Epoch 34/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1905 - mae: 0.3325 - val_loss: 0.1915 - val_mae: 0.3297\n",
      "Epoch 35/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1882 - mae: 0.3316 - val_loss: 0.1622 - val_mae: 0.2986\n",
      "Epoch 36/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1864 - mae: 0.3298 - val_loss: 0.1668 - val_mae: 0.3155\n",
      "Epoch 37/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1881 - mae: 0.3317 - val_loss: 0.1718 - val_mae: 0.3125\n",
      "Epoch 38/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1910 - mae: 0.3318 - val_loss: 0.1657 - val_mae: 0.3062\n",
      "Epoch 39/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1884 - mae: 0.3305 - val_loss: 0.1640 - val_mae: 0.2949\n",
      "Epoch 40/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1884 - mae: 0.3315 - val_loss: 0.1611 - val_mae: 0.3065\n",
      "Epoch 41/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1874 - mae: 0.3298 - val_loss: 0.1701 - val_mae: 0.3153\n",
      "Epoch 42/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1871 - mae: 0.3310 - val_loss: 0.1609 - val_mae: 0.3095\n",
      "Epoch 43/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1849 - mae: 0.3297 - val_loss: 0.1658 - val_mae: 0.3167\n",
      "Epoch 44/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1862 - mae: 0.3304 - val_loss: 0.1765 - val_mae: 0.3196\n",
      "Epoch 45/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1844 - mae: 0.3279 - val_loss: 0.1607 - val_mae: 0.2995\n",
      "Epoch 46/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1875 - mae: 0.3316 - val_loss: 0.1711 - val_mae: 0.3107\n",
      "Epoch 47/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1869 - mae: 0.3311 - val_loss: 0.1785 - val_mae: 0.3360\n",
      "Epoch 48/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1845 - mae: 0.3292 - val_loss: 0.1611 - val_mae: 0.3036\n",
      "Epoch 49/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1857 - mae: 0.3295 - val_loss: 0.1774 - val_mae: 0.3259\n",
      "Epoch 50/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1857 - mae: 0.3296 - val_loss: 0.1604 - val_mae: 0.2899\n",
      "Epoch 51/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1853 - mae: 0.3294 - val_loss: 0.1664 - val_mae: 0.3088\n",
      "Epoch 52/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1872 - mae: 0.3310 - val_loss: 0.1659 - val_mae: 0.3057\n",
      "Epoch 53/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1841 - mae: 0.3285 - val_loss: 0.1573 - val_mae: 0.2924\n",
      "Epoch 54/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1853 - mae: 0.3289 - val_loss: 0.1740 - val_mae: 0.3126\n",
      "Epoch 55/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1828 - mae: 0.3283 - val_loss: 0.1597 - val_mae: 0.2963\n",
      "Epoch 56/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1853 - mae: 0.3292 - val_loss: 0.1544 - val_mae: 0.2947\n",
      "Epoch 57/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1831 - mae: 0.3262 - val_loss: 0.1569 - val_mae: 0.2938\n",
      "Epoch 58/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1824 - mae: 0.3261 - val_loss: 0.1544 - val_mae: 0.2946\n",
      "Epoch 59/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1828 - mae: 0.3275 - val_loss: 0.1621 - val_mae: 0.3107\n",
      "Epoch 60/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1833 - mae: 0.3277 - val_loss: 0.1580 - val_mae: 0.3092\n",
      "Epoch 61/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1816 - mae: 0.3256 - val_loss: 0.1617 - val_mae: 0.3020\n",
      "Epoch 62/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1865 - mae: 0.3299 - val_loss: 0.1622 - val_mae: 0.3041\n",
      "Epoch 63/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1849 - mae: 0.3289 - val_loss: 0.1598 - val_mae: 0.2979\n",
      "Epoch 64/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1846 - mae: 0.3290 - val_loss: 0.1616 - val_mae: 0.3082\n",
      "Epoch 65/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1847 - mae: 0.3283 - val_loss: 0.1569 - val_mae: 0.2948\n",
      "Epoch 66/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1831 - mae: 0.3274 - val_loss: 0.1602 - val_mae: 0.3034\n",
      "Epoch 67/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1814 - mae: 0.3257 - val_loss: 0.1683 - val_mae: 0.3151\n",
      "Epoch 68/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1817 - mae: 0.3252 - val_loss: 0.1620 - val_mae: 0.3067\n",
      "Epoch 69/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1827 - mae: 0.3286 - val_loss: 0.1681 - val_mae: 0.3161\n",
      "Epoch 70/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1831 - mae: 0.3275 - val_loss: 0.1554 - val_mae: 0.2928\n",
      "Epoch 71/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1825 - mae: 0.3266 - val_loss: 0.1556 - val_mae: 0.3029\n",
      "Epoch 72/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1789 - mae: 0.3242 - val_loss: 0.1530 - val_mae: 0.2922\n",
      "Epoch 73/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1816 - mae: 0.3270 - val_loss: 0.1528 - val_mae: 0.2910\n",
      "Epoch 74/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1823 - mae: 0.3266 - val_loss: 0.1586 - val_mae: 0.3053\n",
      "Epoch 75/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1819 - mae: 0.3275 - val_loss: 0.1693 - val_mae: 0.3205\n",
      "Epoch 76/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1819 - mae: 0.3272 - val_loss: 0.1625 - val_mae: 0.3074\n",
      "Epoch 77/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1820 - mae: 0.3288 - val_loss: 0.1584 - val_mae: 0.3023\n",
      "Epoch 78/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1808 - mae: 0.3268 - val_loss: 0.1596 - val_mae: 0.3125\n",
      "Epoch 79/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1805 - mae: 0.3257 - val_loss: 0.1586 - val_mae: 0.3038\n",
      "Epoch 80/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1823 - mae: 0.3280 - val_loss: 0.1612 - val_mae: 0.2969\n",
      "Epoch 81/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1805 - mae: 0.3259 - val_loss: 0.1615 - val_mae: 0.3050\n",
      "Epoch 82/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1817 - mae: 0.3282 - val_loss: 0.1542 - val_mae: 0.3015\n",
      "Epoch 83/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1814 - mae: 0.3278 - val_loss: 0.1541 - val_mae: 0.2882\n",
      "Epoch 84/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1800 - mae: 0.3259 - val_loss: 0.1541 - val_mae: 0.3043\n",
      "Epoch 85/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1831 - mae: 0.3278 - val_loss: 0.1642 - val_mae: 0.3087\n",
      "Epoch 86/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1805 - mae: 0.3256 - val_loss: 0.1537 - val_mae: 0.3026\n",
      "Epoch 87/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1816 - mae: 0.3261 - val_loss: 0.1588 - val_mae: 0.2972\n",
      "Epoch 88/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1811 - mae: 0.3271 - val_loss: 0.1584 - val_mae: 0.2969\n",
      "Epoch 89/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1794 - mae: 0.3250 - val_loss: 0.1520 - val_mae: 0.2938\n",
      "Epoch 90/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1782 - mae: 0.3244 - val_loss: 0.1543 - val_mae: 0.2945\n",
      "Epoch 91/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1819 - mae: 0.3295 - val_loss: 0.1640 - val_mae: 0.3064\n",
      "Epoch 92/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1799 - mae: 0.3269 - val_loss: 0.1543 - val_mae: 0.2977\n",
      "Epoch 93/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1786 - mae: 0.3257 - val_loss: 0.1626 - val_mae: 0.2993\n",
      "Epoch 94/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1789 - mae: 0.3247 - val_loss: 0.1590 - val_mae: 0.2919\n",
      "Epoch 95/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1780 - mae: 0.3251 - val_loss: 0.1599 - val_mae: 0.2995\n",
      "Epoch 96/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1804 - mae: 0.3265 - val_loss: 0.1759 - val_mae: 0.3267\n",
      "Epoch 97/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1802 - mae: 0.3276 - val_loss: 0.1587 - val_mae: 0.3041\n",
      "Epoch 98/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1769 - mae: 0.3229 - val_loss: 0.1542 - val_mae: 0.2987\n",
      "Epoch 99/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1797 - mae: 0.3275 - val_loss: 0.1549 - val_mae: 0.2914\n",
      "Epoch 100/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1797 - mae: 0.3267 - val_loss: 0.1514 - val_mae: 0.2857\n",
      "Epoch 101/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1753 - mae: 0.3217 - val_loss: 0.1548 - val_mae: 0.3083\n",
      "Epoch 102/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1773 - mae: 0.3251 - val_loss: 0.1672 - val_mae: 0.3181\n",
      "Epoch 103/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1802 - mae: 0.3258 - val_loss: 0.1747 - val_mae: 0.3216\n",
      "Epoch 104/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1765 - mae: 0.3246 - val_loss: 0.1503 - val_mae: 0.2892\n",
      "Epoch 105/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1783 - mae: 0.3250 - val_loss: 0.1518 - val_mae: 0.2989\n",
      "Epoch 106/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1772 - mae: 0.3244 - val_loss: 0.1498 - val_mae: 0.2877\n",
      "Epoch 107/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1787 - mae: 0.3264 - val_loss: 0.1500 - val_mae: 0.2905\n",
      "Epoch 108/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1806 - mae: 0.3256 - val_loss: 0.1538 - val_mae: 0.3031\n",
      "Epoch 109/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1817 - mae: 0.3295 - val_loss: 0.1636 - val_mae: 0.3129\n",
      "Epoch 110/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1786 - mae: 0.3263 - val_loss: 0.1530 - val_mae: 0.2993\n",
      "Epoch 111/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1759 - mae: 0.3227 - val_loss: 0.1540 - val_mae: 0.3086\n",
      "Epoch 112/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1781 - mae: 0.3243 - val_loss: 0.1573 - val_mae: 0.2979\n",
      "Epoch 113/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1778 - mae: 0.3251 - val_loss: 0.1542 - val_mae: 0.2991\n",
      "Epoch 114/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1778 - mae: 0.3245 - val_loss: 0.1507 - val_mae: 0.2944\n",
      "Epoch 115/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1765 - mae: 0.3229 - val_loss: 0.1519 - val_mae: 0.2944\n",
      "Epoch 116/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1786 - mae: 0.3246 - val_loss: 0.1512 - val_mae: 0.2874\n",
      "Epoch 117/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1771 - mae: 0.3234 - val_loss: 0.1585 - val_mae: 0.3013\n",
      "Epoch 118/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1794 - mae: 0.3262 - val_loss: 0.1616 - val_mae: 0.3067\n",
      "Epoch 119/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1773 - mae: 0.3231 - val_loss: 0.1641 - val_mae: 0.3134\n",
      "Epoch 120/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1788 - mae: 0.3239 - val_loss: 0.1562 - val_mae: 0.2964\n",
      "Epoch 121/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1782 - mae: 0.3241 - val_loss: 0.1506 - val_mae: 0.2920\n",
      "Epoch 122/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1758 - mae: 0.3230 - val_loss: 0.1597 - val_mae: 0.3019\n",
      "Epoch 123/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1779 - mae: 0.3241 - val_loss: 0.1508 - val_mae: 0.2949\n",
      "Epoch 124/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1750 - mae: 0.3224 - val_loss: 0.1487 - val_mae: 0.2963\n",
      "Epoch 125/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1777 - mae: 0.3239 - val_loss: 0.1618 - val_mae: 0.3037\n",
      "Epoch 126/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1740 - mae: 0.3206 - val_loss: 0.1517 - val_mae: 0.2970\n",
      "Epoch 127/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1776 - mae: 0.3236 - val_loss: 0.1537 - val_mae: 0.2914\n",
      "Epoch 128/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1747 - mae: 0.3212 - val_loss: 0.1527 - val_mae: 0.3029\n",
      "Epoch 129/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1749 - mae: 0.3223 - val_loss: 0.1688 - val_mae: 0.3294\n",
      "Epoch 130/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1773 - mae: 0.3245 - val_loss: 0.1579 - val_mae: 0.2985\n",
      "Epoch 131/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1767 - mae: 0.3249 - val_loss: 0.1474 - val_mae: 0.2878\n",
      "Epoch 132/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1762 - mae: 0.3223 - val_loss: 0.1492 - val_mae: 0.2988\n",
      "Epoch 133/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1764 - mae: 0.3237 - val_loss: 0.1534 - val_mae: 0.3030\n",
      "Epoch 134/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1765 - mae: 0.3228 - val_loss: 0.1510 - val_mae: 0.2910\n",
      "Epoch 135/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1762 - mae: 0.3225 - val_loss: 0.1519 - val_mae: 0.2858\n",
      "Epoch 136/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1745 - mae: 0.3206 - val_loss: 0.1515 - val_mae: 0.3007\n",
      "Epoch 137/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1744 - mae: 0.3219 - val_loss: 0.1563 - val_mae: 0.2967\n",
      "Epoch 138/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1763 - mae: 0.3220 - val_loss: 0.1614 - val_mae: 0.3107\n",
      "Epoch 139/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1750 - mae: 0.3205 - val_loss: 0.1538 - val_mae: 0.2921\n",
      "Epoch 140/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1752 - mae: 0.3213 - val_loss: 0.1501 - val_mae: 0.2974\n",
      "Epoch 141/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1772 - mae: 0.3233 - val_loss: 0.1534 - val_mae: 0.2950\n",
      "Epoch 142/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1741 - mae: 0.3207 - val_loss: 0.1511 - val_mae: 0.2916\n",
      "Epoch 143/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1783 - mae: 0.3241 - val_loss: 0.1593 - val_mae: 0.3094\n",
      "Epoch 144/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1753 - mae: 0.3224 - val_loss: 0.1565 - val_mae: 0.3078\n",
      "Epoch 145/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1773 - mae: 0.3248 - val_loss: 0.1595 - val_mae: 0.3036\n",
      "Epoch 146/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1774 - mae: 0.3256 - val_loss: 0.1621 - val_mae: 0.3042\n",
      "Epoch 147/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1750 - mae: 0.3226 - val_loss: 0.1493 - val_mae: 0.2899\n",
      "Epoch 148/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1750 - mae: 0.3230 - val_loss: 0.1634 - val_mae: 0.3085\n",
      "Epoch 149/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1739 - mae: 0.3211 - val_loss: 0.1497 - val_mae: 0.2872\n",
      "Epoch 150/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1767 - mae: 0.3241 - val_loss: 0.1542 - val_mae: 0.2977\n",
      "Epoch 151/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1781 - mae: 0.3247 - val_loss: 0.1551 - val_mae: 0.2902\n",
      "Epoch 152/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1751 - mae: 0.3223 - val_loss: 0.1495 - val_mae: 0.2907\n",
      "Epoch 153/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1764 - mae: 0.3236 - val_loss: 0.1515 - val_mae: 0.2901\n",
      "Epoch 154/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1757 - mae: 0.3234 - val_loss: 0.1496 - val_mae: 0.2885\n",
      "Epoch 155/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1764 - mae: 0.3238 - val_loss: 0.1484 - val_mae: 0.2909\n",
      "Epoch 156/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1786 - mae: 0.3260 - val_loss: 0.1467 - val_mae: 0.2905\n",
      "Epoch 157/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1781 - mae: 0.3268 - val_loss: 0.1534 - val_mae: 0.2888\n",
      "Epoch 158/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1729 - mae: 0.3198 - val_loss: 0.1504 - val_mae: 0.3064\n",
      "Epoch 159/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1743 - mae: 0.3221 - val_loss: 0.1506 - val_mae: 0.2949\n",
      "Epoch 160/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1753 - mae: 0.3226 - val_loss: 0.1601 - val_mae: 0.3036\n",
      "Epoch 161/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1729 - mae: 0.3207 - val_loss: 0.1568 - val_mae: 0.3131\n",
      "Epoch 162/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1764 - mae: 0.3239 - val_loss: 0.1508 - val_mae: 0.2951\n",
      "Epoch 163/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1725 - mae: 0.3213 - val_loss: 0.1516 - val_mae: 0.2900\n",
      "Epoch 164/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1730 - mae: 0.3208 - val_loss: 0.1525 - val_mae: 0.2909\n",
      "Epoch 165/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1762 - mae: 0.3223 - val_loss: 0.1499 - val_mae: 0.2907\n",
      "Epoch 166/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1751 - mae: 0.3214 - val_loss: 0.1557 - val_mae: 0.3056\n",
      "Epoch 167/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1742 - mae: 0.3214 - val_loss: 0.1558 - val_mae: 0.2998\n",
      "Epoch 168/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1734 - mae: 0.3223 - val_loss: 0.1564 - val_mae: 0.3052\n",
      "Epoch 169/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1725 - mae: 0.3200 - val_loss: 0.1490 - val_mae: 0.2853\n",
      "Epoch 170/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1746 - mae: 0.3222 - val_loss: 0.1502 - val_mae: 0.3016\n",
      "Epoch 171/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1766 - mae: 0.3245 - val_loss: 0.1574 - val_mae: 0.2984\n",
      "Epoch 172/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1757 - mae: 0.3232 - val_loss: 0.1538 - val_mae: 0.2940\n",
      "Epoch 173/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1761 - mae: 0.3236 - val_loss: 0.1544 - val_mae: 0.3068\n",
      "Epoch 174/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1742 - mae: 0.3218 - val_loss: 0.1547 - val_mae: 0.2910\n",
      "Epoch 175/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1750 - mae: 0.3231 - val_loss: 0.1470 - val_mae: 0.2893\n",
      "Epoch 176/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1762 - mae: 0.3229 - val_loss: 0.1590 - val_mae: 0.3010\n",
      "Epoch 177/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1754 - mae: 0.3216 - val_loss: 0.1519 - val_mae: 0.2863\n",
      "Epoch 178/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1746 - mae: 0.3210 - val_loss: 0.1479 - val_mae: 0.2886\n",
      "Epoch 179/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1731 - mae: 0.3215 - val_loss: 0.1519 - val_mae: 0.2968\n",
      "Epoch 180/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1776 - mae: 0.3241 - val_loss: 0.1538 - val_mae: 0.2940\n",
      "Epoch 181/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1740 - mae: 0.3213 - val_loss: 0.1507 - val_mae: 0.2903\n",
      "Epoch 182/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1735 - mae: 0.3199 - val_loss: 0.1477 - val_mae: 0.2853\n",
      "Epoch 183/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1748 - mae: 0.3226 - val_loss: 0.1571 - val_mae: 0.2876\n",
      "Epoch 184/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1751 - mae: 0.3210 - val_loss: 0.1532 - val_mae: 0.3040\n",
      "Epoch 185/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1727 - mae: 0.3179 - val_loss: 0.1461 - val_mae: 0.2843\n",
      "Epoch 186/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1746 - mae: 0.3229 - val_loss: 0.1639 - val_mae: 0.3080\n",
      "Epoch 187/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1752 - mae: 0.3217 - val_loss: 0.1583 - val_mae: 0.3072\n",
      "Epoch 188/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1761 - mae: 0.3226 - val_loss: 0.1551 - val_mae: 0.3080\n",
      "Epoch 189/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1752 - mae: 0.3221 - val_loss: 0.1541 - val_mae: 0.3074\n",
      "Epoch 190/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1722 - mae: 0.3197 - val_loss: 0.1531 - val_mae: 0.2950\n",
      "Epoch 191/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1739 - mae: 0.3217 - val_loss: 0.1453 - val_mae: 0.2841\n",
      "Epoch 192/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1750 - mae: 0.3214 - val_loss: 0.1489 - val_mae: 0.2925\n",
      "Epoch 193/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1725 - mae: 0.3184 - val_loss: 0.1468 - val_mae: 0.2823\n",
      "Epoch 194/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1745 - mae: 0.3222 - val_loss: 0.1505 - val_mae: 0.2886\n",
      "Epoch 195/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1745 - mae: 0.3211 - val_loss: 0.1480 - val_mae: 0.2943\n",
      "Epoch 196/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1753 - mae: 0.3231 - val_loss: 0.1539 - val_mae: 0.2971\n",
      "Epoch 197/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1729 - mae: 0.3204 - val_loss: 0.1457 - val_mae: 0.2849\n",
      "Epoch 198/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1747 - mae: 0.3225 - val_loss: 0.1487 - val_mae: 0.2875\n",
      "Epoch 199/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1747 - mae: 0.3233 - val_loss: 0.1677 - val_mae: 0.3187\n",
      "Epoch 200/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1734 - mae: 0.3195 - val_loss: 0.1598 - val_mae: 0.2919\n",
      "Epoch 201/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1763 - mae: 0.3222 - val_loss: 0.1447 - val_mae: 0.2831\n",
      "Epoch 202/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1740 - mae: 0.3212 - val_loss: 0.1511 - val_mae: 0.2963\n",
      "Epoch 203/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1728 - mae: 0.3206 - val_loss: 0.1516 - val_mae: 0.2965\n",
      "Epoch 204/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1730 - mae: 0.3197 - val_loss: 0.1481 - val_mae: 0.2887\n",
      "Epoch 205/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1717 - mae: 0.3189 - val_loss: 0.1447 - val_mae: 0.2848\n",
      "Epoch 206/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1720 - mae: 0.3190 - val_loss: 0.1597 - val_mae: 0.2996\n",
      "Epoch 207/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1740 - mae: 0.3215 - val_loss: 0.1509 - val_mae: 0.2911\n",
      "Epoch 208/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1735 - mae: 0.3217 - val_loss: 0.1524 - val_mae: 0.2994\n",
      "Epoch 209/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1712 - mae: 0.3200 - val_loss: 0.1436 - val_mae: 0.2803\n",
      "Epoch 210/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1752 - mae: 0.3219 - val_loss: 0.1488 - val_mae: 0.2811\n",
      "Epoch 211/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1725 - mae: 0.3199 - val_loss: 0.1537 - val_mae: 0.2970\n",
      "Epoch 212/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1708 - mae: 0.3191 - val_loss: 0.1586 - val_mae: 0.3077\n",
      "Epoch 213/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1736 - mae: 0.3209 - val_loss: 0.1579 - val_mae: 0.2998\n",
      "Epoch 214/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1748 - mae: 0.3223 - val_loss: 0.1472 - val_mae: 0.2846\n",
      "Epoch 215/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1709 - mae: 0.3200 - val_loss: 0.1576 - val_mae: 0.3145\n",
      "Epoch 216/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1735 - mae: 0.3231 - val_loss: 0.1448 - val_mae: 0.2853\n",
      "Epoch 217/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1735 - mae: 0.3220 - val_loss: 0.1665 - val_mae: 0.3178\n",
      "Epoch 218/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1725 - mae: 0.3210 - val_loss: 0.1449 - val_mae: 0.2902\n",
      "Epoch 219/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1732 - mae: 0.3218 - val_loss: 0.1519 - val_mae: 0.2966\n",
      "Epoch 220/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1751 - mae: 0.3229 - val_loss: 0.1522 - val_mae: 0.2939\n",
      "Epoch 221/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1719 - mae: 0.3206 - val_loss: 0.1610 - val_mae: 0.3174\n",
      "Epoch 222/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1749 - mae: 0.3236 - val_loss: 0.1504 - val_mae: 0.2906\n",
      "Epoch 223/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1732 - mae: 0.3213 - val_loss: 0.1466 - val_mae: 0.2826\n",
      "Epoch 224/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1727 - mae: 0.3205 - val_loss: 0.1449 - val_mae: 0.2808\n",
      "Epoch 225/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1732 - mae: 0.3212 - val_loss: 0.1455 - val_mae: 0.2817\n",
      "Epoch 226/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1735 - mae: 0.3214 - val_loss: 0.1495 - val_mae: 0.2890\n",
      "Epoch 227/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1720 - mae: 0.3216 - val_loss: 0.1664 - val_mae: 0.3129\n",
      "Epoch 228/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1741 - mae: 0.3232 - val_loss: 0.1601 - val_mae: 0.3156\n",
      "Epoch 229/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1741 - mae: 0.3225 - val_loss: 0.1488 - val_mae: 0.2861\n",
      "Epoch 230/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1742 - mae: 0.3218 - val_loss: 0.1463 - val_mae: 0.2865\n",
      "Epoch 231/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1714 - mae: 0.3189 - val_loss: 0.1558 - val_mae: 0.3078\n",
      "Epoch 232/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1735 - mae: 0.3218 - val_loss: 0.1611 - val_mae: 0.3143\n",
      "Epoch 233/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1712 - mae: 0.3200 - val_loss: 0.1593 - val_mae: 0.3003\n",
      "Epoch 234/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1713 - mae: 0.3190 - val_loss: 0.1685 - val_mae: 0.3195\n",
      "Epoch 235/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1731 - mae: 0.3203 - val_loss: 0.1437 - val_mae: 0.2837\n",
      "Epoch 236/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1736 - mae: 0.3228 - val_loss: 0.1508 - val_mae: 0.2931\n",
      "Epoch 237/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1723 - mae: 0.3203 - val_loss: 0.1463 - val_mae: 0.2838\n",
      "Epoch 238/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1735 - mae: 0.3207 - val_loss: 0.1510 - val_mae: 0.2930\n",
      "Epoch 239/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1729 - mae: 0.3226 - val_loss: 0.1489 - val_mae: 0.2991\n",
      "Epoch 240/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1719 - mae: 0.3193 - val_loss: 0.1503 - val_mae: 0.2955\n",
      "Epoch 241/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1705 - mae: 0.3178 - val_loss: 0.1489 - val_mae: 0.2959\n",
      "Epoch 242/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1714 - mae: 0.3202 - val_loss: 0.1512 - val_mae: 0.2954\n",
      "Epoch 243/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1712 - mae: 0.3194 - val_loss: 0.1475 - val_mae: 0.2956\n",
      "Epoch 244/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1728 - mae: 0.3203 - val_loss: 0.1505 - val_mae: 0.3008\n",
      "Epoch 245/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1704 - mae: 0.3174 - val_loss: 0.1568 - val_mae: 0.3024\n",
      "Epoch 246/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1717 - mae: 0.3215 - val_loss: 0.1601 - val_mae: 0.3026\n",
      "Epoch 247/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1733 - mae: 0.3206 - val_loss: 0.1539 - val_mae: 0.3054\n",
      "Epoch 248/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1718 - mae: 0.3184 - val_loss: 0.1451 - val_mae: 0.2904\n",
      "Epoch 249/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1724 - mae: 0.3198 - val_loss: 0.1464 - val_mae: 0.2827\n",
      "Epoch 250/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1720 - mae: 0.3198 - val_loss: 0.1505 - val_mae: 0.2993\n",
      "Epoch 251/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1735 - mae: 0.3221 - val_loss: 0.1483 - val_mae: 0.2927\n",
      "Epoch 252/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1702 - mae: 0.3167 - val_loss: 0.1460 - val_mae: 0.2895\n",
      "Epoch 253/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1732 - mae: 0.3206 - val_loss: 0.1492 - val_mae: 0.3027\n",
      "Epoch 254/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1706 - mae: 0.3185 - val_loss: 0.1539 - val_mae: 0.3021\n",
      "Epoch 255/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1711 - mae: 0.3185 - val_loss: 0.1504 - val_mae: 0.2947\n",
      "Epoch 256/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1709 - mae: 0.3183 - val_loss: 0.1471 - val_mae: 0.2870\n",
      "Epoch 257/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1744 - mae: 0.3229 - val_loss: 0.1569 - val_mae: 0.2936\n",
      "Epoch 258/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1738 - mae: 0.3218 - val_loss: 0.1466 - val_mae: 0.2897\n",
      "Epoch 259/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1711 - mae: 0.3187 - val_loss: 0.1470 - val_mae: 0.2944\n",
      "Epoch 260/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1706 - mae: 0.3171 - val_loss: 0.1522 - val_mae: 0.2903\n",
      "Epoch 261/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1714 - mae: 0.3188 - val_loss: 0.1466 - val_mae: 0.2851\n",
      "Epoch 262/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1702 - mae: 0.3182 - val_loss: 0.1480 - val_mae: 0.3009\n",
      "Epoch 263/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1694 - mae: 0.3160 - val_loss: 0.1690 - val_mae: 0.3161\n",
      "Epoch 264/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1728 - mae: 0.3209 - val_loss: 0.1491 - val_mae: 0.3044\n",
      "Epoch 265/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1695 - mae: 0.3176 - val_loss: 0.1523 - val_mae: 0.2985\n",
      "Epoch 266/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1697 - mae: 0.3170 - val_loss: 0.1497 - val_mae: 0.2951\n",
      "Epoch 267/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1740 - mae: 0.3229 - val_loss: 0.1463 - val_mae: 0.2841\n",
      "Epoch 268/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1700 - mae: 0.3191 - val_loss: 0.1447 - val_mae: 0.2908\n",
      "Epoch 269/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1697 - mae: 0.3182 - val_loss: 0.1488 - val_mae: 0.2944\n",
      "Epoch 270/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1751 - mae: 0.3241 - val_loss: 0.1591 - val_mae: 0.3042\n",
      "Epoch 271/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1719 - mae: 0.3187 - val_loss: 0.1444 - val_mae: 0.2819\n",
      "Epoch 272/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1727 - mae: 0.3187 - val_loss: 0.1594 - val_mae: 0.3162\n",
      "Epoch 273/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1693 - mae: 0.3169 - val_loss: 0.1614 - val_mae: 0.3074\n",
      "Epoch 274/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1709 - mae: 0.3180 - val_loss: 0.1473 - val_mae: 0.2833\n",
      "Epoch 275/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1733 - mae: 0.3202 - val_loss: 0.1444 - val_mae: 0.2814\n",
      "Epoch 276/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1716 - mae: 0.3195 - val_loss: 0.1491 - val_mae: 0.2927\n",
      "Epoch 277/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1713 - mae: 0.3180 - val_loss: 0.1493 - val_mae: 0.2942\n",
      "Epoch 278/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1711 - mae: 0.3187 - val_loss: 0.1546 - val_mae: 0.2935\n",
      "Epoch 279/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1703 - mae: 0.3170 - val_loss: 0.1507 - val_mae: 0.2915\n",
      "Epoch 280/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1706 - mae: 0.3175 - val_loss: 0.1534 - val_mae: 0.2884\n",
      "Epoch 281/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1727 - mae: 0.3204 - val_loss: 0.1504 - val_mae: 0.2896\n",
      "Epoch 282/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1748 - mae: 0.3231 - val_loss: 0.1804 - val_mae: 0.3351\n",
      "Epoch 283/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1735 - mae: 0.3205 - val_loss: 0.1481 - val_mae: 0.2968\n",
      "Epoch 284/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1691 - mae: 0.3176 - val_loss: 0.1494 - val_mae: 0.3002\n",
      "Epoch 285/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1700 - mae: 0.3183 - val_loss: 0.1533 - val_mae: 0.3024\n",
      "Epoch 286/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1721 - mae: 0.3217 - val_loss: 0.1487 - val_mae: 0.2921\n",
      "Epoch 287/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1729 - mae: 0.3208 - val_loss: 0.1449 - val_mae: 0.2906\n",
      "Epoch 288/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1721 - mae: 0.3207 - val_loss: 0.1532 - val_mae: 0.2938\n",
      "Epoch 289/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1682 - mae: 0.3156 - val_loss: 0.1430 - val_mae: 0.2810\n",
      "Epoch 290/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1690 - mae: 0.3163 - val_loss: 0.1475 - val_mae: 0.2879\n",
      "Epoch 291/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1688 - mae: 0.3163 - val_loss: 0.1553 - val_mae: 0.3030\n",
      "Epoch 292/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1682 - mae: 0.3168 - val_loss: 0.1430 - val_mae: 0.2840\n",
      "Epoch 293/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1743 - mae: 0.3221 - val_loss: 0.1482 - val_mae: 0.2831\n",
      "Epoch 294/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1678 - mae: 0.3158 - val_loss: 0.1510 - val_mae: 0.2964\n",
      "Epoch 295/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1696 - mae: 0.3179 - val_loss: 0.1535 - val_mae: 0.2952\n",
      "Epoch 296/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1696 - mae: 0.3170 - val_loss: 0.1523 - val_mae: 0.2994\n",
      "Epoch 297/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1702 - mae: 0.3192 - val_loss: 0.1578 - val_mae: 0.3125\n",
      "Epoch 298/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1707 - mae: 0.3189 - val_loss: 0.1493 - val_mae: 0.2862\n",
      "Epoch 299/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1717 - mae: 0.3187 - val_loss: 0.1583 - val_mae: 0.3030\n",
      "Epoch 300/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1710 - mae: 0.3190 - val_loss: 0.1440 - val_mae: 0.2851\n",
      "Epoch 301/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1708 - mae: 0.3173 - val_loss: 0.1457 - val_mae: 0.2928\n",
      "Epoch 302/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1681 - mae: 0.3160 - val_loss: 0.1530 - val_mae: 0.2965\n",
      "Epoch 303/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1682 - mae: 0.3157 - val_loss: 0.1560 - val_mae: 0.3029\n",
      "Epoch 304/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1694 - mae: 0.3164 - val_loss: 0.1431 - val_mae: 0.2833\n",
      "Epoch 305/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1682 - mae: 0.3157 - val_loss: 0.1470 - val_mae: 0.2861\n",
      "Epoch 306/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1706 - mae: 0.3179 - val_loss: 0.1557 - val_mae: 0.2977\n",
      "Epoch 307/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1730 - mae: 0.3202 - val_loss: 0.1459 - val_mae: 0.2826\n",
      "Epoch 308/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1690 - mae: 0.3163 - val_loss: 0.1429 - val_mae: 0.2854\n",
      "Epoch 309/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1700 - mae: 0.3168 - val_loss: 0.1461 - val_mae: 0.2907\n",
      "Epoch 310/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1699 - mae: 0.3182 - val_loss: 0.1455 - val_mae: 0.2847\n",
      "Epoch 311/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1722 - mae: 0.3201 - val_loss: 0.1550 - val_mae: 0.2961\n",
      "Epoch 312/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1677 - mae: 0.3154 - val_loss: 0.1524 - val_mae: 0.3005\n",
      "Epoch 313/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1701 - mae: 0.3180 - val_loss: 0.1456 - val_mae: 0.2895\n",
      "Epoch 314/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1698 - mae: 0.3165 - val_loss: 0.1461 - val_mae: 0.2955\n",
      "Epoch 315/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1701 - mae: 0.3173 - val_loss: 0.1481 - val_mae: 0.2883\n",
      "Epoch 316/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1704 - mae: 0.3177 - val_loss: 0.1521 - val_mae: 0.2951\n",
      "Epoch 317/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1704 - mae: 0.3179 - val_loss: 0.1423 - val_mae: 0.2781\n",
      "Epoch 318/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1710 - mae: 0.3185 - val_loss: 0.1426 - val_mae: 0.2764\n",
      "Epoch 319/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1687 - mae: 0.3162 - val_loss: 0.1476 - val_mae: 0.2862\n",
      "Epoch 320/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1697 - mae: 0.3180 - val_loss: 0.1426 - val_mae: 0.2780\n",
      "Epoch 321/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1672 - mae: 0.3148 - val_loss: 0.1426 - val_mae: 0.2870\n",
      "Epoch 322/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1679 - mae: 0.3145 - val_loss: 0.1528 - val_mae: 0.2998\n",
      "Epoch 323/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1693 - mae: 0.3168 - val_loss: 0.1543 - val_mae: 0.3023\n",
      "Epoch 324/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1724 - mae: 0.3189 - val_loss: 0.1447 - val_mae: 0.2906\n",
      "Epoch 325/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1685 - mae: 0.3160 - val_loss: 0.1479 - val_mae: 0.2903\n",
      "Epoch 326/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1675 - mae: 0.3161 - val_loss: 0.1450 - val_mae: 0.2847\n",
      "Epoch 327/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1672 - mae: 0.3135 - val_loss: 0.1566 - val_mae: 0.3030\n",
      "Epoch 328/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1673 - mae: 0.3127 - val_loss: 0.1552 - val_mae: 0.3053\n",
      "Epoch 329/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1697 - mae: 0.3160 - val_loss: 0.1505 - val_mae: 0.2926\n",
      "Epoch 330/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1681 - mae: 0.3130 - val_loss: 0.1495 - val_mae: 0.2904\n",
      "Epoch 331/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1674 - mae: 0.3132 - val_loss: 0.1476 - val_mae: 0.2858\n",
      "Epoch 332/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1708 - mae: 0.3192 - val_loss: 0.1436 - val_mae: 0.2822\n",
      "Epoch 333/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1688 - mae: 0.3155 - val_loss: 0.1516 - val_mae: 0.2891\n",
      "Epoch 334/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1691 - mae: 0.3164 - val_loss: 0.1475 - val_mae: 0.2933\n",
      "Epoch 335/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1705 - mae: 0.3173 - val_loss: 0.1428 - val_mae: 0.2879\n",
      "Epoch 336/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1701 - mae: 0.3166 - val_loss: 0.1482 - val_mae: 0.2887\n",
      "Epoch 337/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1687 - mae: 0.3159 - val_loss: 0.1513 - val_mae: 0.2924\n",
      "Epoch 338/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1697 - mae: 0.3181 - val_loss: 0.1446 - val_mae: 0.2812\n",
      "Epoch 339/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1669 - mae: 0.3116 - val_loss: 0.1452 - val_mae: 0.2978\n",
      "Epoch 340/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1676 - mae: 0.3141 - val_loss: 0.1496 - val_mae: 0.2919\n",
      "Epoch 341/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1693 - mae: 0.3175 - val_loss: 0.1603 - val_mae: 0.3029\n",
      "Epoch 342/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1692 - mae: 0.3179 - val_loss: 0.1453 - val_mae: 0.2880\n",
      "Epoch 343/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1679 - mae: 0.3150 - val_loss: 0.1539 - val_mae: 0.2977\n",
      "Epoch 344/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1687 - mae: 0.3144 - val_loss: 0.1464 - val_mae: 0.2958\n",
      "Epoch 345/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1661 - mae: 0.3136 - val_loss: 0.1468 - val_mae: 0.2908\n",
      "Epoch 346/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1676 - mae: 0.3137 - val_loss: 0.1450 - val_mae: 0.2880\n",
      "Epoch 347/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1698 - mae: 0.3168 - val_loss: 0.1609 - val_mae: 0.3060\n",
      "Epoch 348/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1729 - mae: 0.3201 - val_loss: 0.1425 - val_mae: 0.2812\n",
      "Epoch 349/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1688 - mae: 0.3164 - val_loss: 0.1469 - val_mae: 0.2842\n",
      "Epoch 350/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1684 - mae: 0.3147 - val_loss: 0.1465 - val_mae: 0.2879\n",
      "Epoch 351/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1706 - mae: 0.3183 - val_loss: 0.1458 - val_mae: 0.2867\n",
      "Epoch 352/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1695 - mae: 0.3172 - val_loss: 0.1455 - val_mae: 0.2891\n",
      "Epoch 353/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1670 - mae: 0.3149 - val_loss: 0.1418 - val_mae: 0.2797\n",
      "Epoch 354/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1682 - mae: 0.3147 - val_loss: 0.1411 - val_mae: 0.2794\n",
      "Epoch 355/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1665 - mae: 0.3136 - val_loss: 0.1522 - val_mae: 0.2932\n",
      "Epoch 356/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1695 - mae: 0.3162 - val_loss: 0.1487 - val_mae: 0.2950\n",
      "Epoch 357/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1695 - mae: 0.3164 - val_loss: 0.1439 - val_mae: 0.2828\n",
      "Epoch 358/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1698 - mae: 0.3168 - val_loss: 0.1443 - val_mae: 0.2839\n",
      "Epoch 359/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1700 - mae: 0.3167 - val_loss: 0.1581 - val_mae: 0.2938\n",
      "Epoch 360/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1669 - mae: 0.3148 - val_loss: 0.1463 - val_mae: 0.2845\n",
      "Epoch 361/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1667 - mae: 0.3141 - val_loss: 0.1505 - val_mae: 0.2978\n",
      "Epoch 362/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1691 - mae: 0.3149 - val_loss: 0.1430 - val_mae: 0.2859\n",
      "Epoch 363/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1685 - mae: 0.3154 - val_loss: 0.1431 - val_mae: 0.2884\n",
      "Epoch 364/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1700 - mae: 0.3162 - val_loss: 0.1432 - val_mae: 0.2767\n",
      "Epoch 365/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1708 - mae: 0.3177 - val_loss: 0.1454 - val_mae: 0.2849\n",
      "Epoch 366/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1714 - mae: 0.3172 - val_loss: 0.1519 - val_mae: 0.2928\n",
      "Epoch 367/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1677 - mae: 0.3146 - val_loss: 0.1538 - val_mae: 0.2962\n",
      "Epoch 368/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1687 - mae: 0.3167 - val_loss: 0.1411 - val_mae: 0.2833\n",
      "Epoch 369/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1693 - mae: 0.3160 - val_loss: 0.1467 - val_mae: 0.2821\n",
      "Epoch 370/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1694 - mae: 0.3177 - val_loss: 0.1537 - val_mae: 0.3008\n",
      "Epoch 371/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1704 - mae: 0.3161 - val_loss: 0.1532 - val_mae: 0.2955\n",
      "Epoch 372/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1714 - mae: 0.3184 - val_loss: 0.1474 - val_mae: 0.2794\n",
      "Epoch 373/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1707 - mae: 0.3183 - val_loss: 0.1502 - val_mae: 0.2986\n",
      "Epoch 374/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1678 - mae: 0.3165 - val_loss: 0.1454 - val_mae: 0.2895\n",
      "Epoch 375/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1728 - mae: 0.3198 - val_loss: 0.1533 - val_mae: 0.2969\n",
      "Epoch 376/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1698 - mae: 0.3178 - val_loss: 0.1518 - val_mae: 0.2966\n",
      "Epoch 377/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1704 - mae: 0.3173 - val_loss: 0.1575 - val_mae: 0.3083\n",
      "Epoch 378/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1689 - mae: 0.3153 - val_loss: 0.1439 - val_mae: 0.2885\n",
      "Epoch 379/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1705 - mae: 0.3186 - val_loss: 0.1480 - val_mae: 0.2874\n",
      "Epoch 380/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1682 - mae: 0.3163 - val_loss: 0.1469 - val_mae: 0.2881\n",
      "Epoch 381/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1695 - mae: 0.3171 - val_loss: 0.1469 - val_mae: 0.2840\n",
      "Epoch 382/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1699 - mae: 0.3182 - val_loss: 0.1523 - val_mae: 0.2995\n",
      "Epoch 383/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1691 - mae: 0.3174 - val_loss: 0.1490 - val_mae: 0.2868\n",
      "Epoch 384/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1699 - mae: 0.3181 - val_loss: 0.1538 - val_mae: 0.3004\n",
      "Epoch 385/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1689 - mae: 0.3154 - val_loss: 0.1462 - val_mae: 0.2900\n",
      "Epoch 386/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1667 - mae: 0.3139 - val_loss: 0.1568 - val_mae: 0.3068\n",
      "Epoch 387/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.1710 - mae: 0.3182 - val_loss: 0.1464 - val_mae: 0.2812\n",
      "Epoch 388/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1681 - mae: 0.3157 - val_loss: 0.1470 - val_mae: 0.2888\n",
      "Epoch 389/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1661 - mae: 0.3138 - val_loss: 0.1422 - val_mae: 0.2851\n",
      "Epoch 390/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1675 - mae: 0.3152 - val_loss: 0.1487 - val_mae: 0.2950\n",
      "Epoch 391/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1689 - mae: 0.3169 - val_loss: 0.1607 - val_mae: 0.3070\n",
      "Epoch 392/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1697 - mae: 0.3177 - val_loss: 0.1468 - val_mae: 0.2822\n",
      "Epoch 393/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1674 - mae: 0.3156 - val_loss: 0.1417 - val_mae: 0.2818\n",
      "Epoch 394/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1657 - mae: 0.3139 - val_loss: 0.1426 - val_mae: 0.2874\n",
      "Epoch 395/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1678 - mae: 0.3155 - val_loss: 0.1463 - val_mae: 0.2828\n",
      "Epoch 396/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1695 - mae: 0.3158 - val_loss: 0.1415 - val_mae: 0.2814\n",
      "Epoch 397/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1679 - mae: 0.3156 - val_loss: 0.1619 - val_mae: 0.3156\n",
      "Epoch 398/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1663 - mae: 0.3136 - val_loss: 0.1454 - val_mae: 0.2898\n",
      "Epoch 399/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1677 - mae: 0.3162 - val_loss: 0.1420 - val_mae: 0.2846\n",
      "Epoch 400/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1691 - mae: 0.3174 - val_loss: 0.1437 - val_mae: 0.2785\n",
      "Epoch 401/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1689 - mae: 0.3163 - val_loss: 0.1469 - val_mae: 0.2924\n",
      "Epoch 402/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1683 - mae: 0.3164 - val_loss: 0.1466 - val_mae: 0.2900\n",
      "Epoch 403/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1686 - mae: 0.3159 - val_loss: 0.1540 - val_mae: 0.2997\n",
      "Epoch 404/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1672 - mae: 0.3147 - val_loss: 0.1487 - val_mae: 0.2946\n",
      "Epoch 405/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1669 - mae: 0.3148 - val_loss: 0.1407 - val_mae: 0.2846\n",
      "Epoch 406/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1692 - mae: 0.3166 - val_loss: 0.1486 - val_mae: 0.2875\n",
      "Epoch 407/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1675 - mae: 0.3165 - val_loss: 0.1501 - val_mae: 0.2914\n",
      "Epoch 408/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1701 - mae: 0.3194 - val_loss: 0.1508 - val_mae: 0.2897\n",
      "Epoch 409/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1660 - mae: 0.3145 - val_loss: 0.1511 - val_mae: 0.3009\n",
      "Epoch 410/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1715 - mae: 0.3205 - val_loss: 0.1470 - val_mae: 0.2915\n",
      "Epoch 411/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1686 - mae: 0.3177 - val_loss: 0.1546 - val_mae: 0.2931\n",
      "Epoch 412/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1685 - mae: 0.3171 - val_loss: 0.1429 - val_mae: 0.2842\n",
      "Epoch 413/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1670 - mae: 0.3146 - val_loss: 0.1427 - val_mae: 0.2817\n",
      "Epoch 414/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1639 - mae: 0.3123 - val_loss: 0.1568 - val_mae: 0.2958\n",
      "Epoch 415/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1660 - mae: 0.3147 - val_loss: 0.1437 - val_mae: 0.2832\n",
      "Epoch 416/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1690 - mae: 0.3164 - val_loss: 0.1471 - val_mae: 0.2807\n",
      "Epoch 417/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1687 - mae: 0.3170 - val_loss: 0.1564 - val_mae: 0.2955\n",
      "Epoch 418/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1685 - mae: 0.3179 - val_loss: 0.1431 - val_mae: 0.2855\n",
      "Epoch 419/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1691 - mae: 0.3173 - val_loss: 0.1517 - val_mae: 0.2920\n",
      "Epoch 420/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1676 - mae: 0.3161 - val_loss: 0.1481 - val_mae: 0.2811\n",
      "Epoch 421/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1668 - mae: 0.3155 - val_loss: 0.1470 - val_mae: 0.2833\n",
      "Epoch 422/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1679 - mae: 0.3171 - val_loss: 0.1602 - val_mae: 0.3131\n",
      "Epoch 423/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1689 - mae: 0.3186 - val_loss: 0.1562 - val_mae: 0.3061\n",
      "Epoch 424/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1669 - mae: 0.3148 - val_loss: 0.1588 - val_mae: 0.3177\n",
      "Epoch 425/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1700 - mae: 0.3177 - val_loss: 0.1588 - val_mae: 0.3038\n",
      "Epoch 426/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1677 - mae: 0.3159 - val_loss: 0.1542 - val_mae: 0.3103\n",
      "Epoch 427/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1645 - mae: 0.3121 - val_loss: 0.1461 - val_mae: 0.2906\n",
      "Epoch 428/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1692 - mae: 0.3163 - val_loss: 0.1418 - val_mae: 0.2782\n",
      "Epoch 429/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1679 - mae: 0.3160 - val_loss: 0.1493 - val_mae: 0.2957\n",
      "Epoch 430/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1663 - mae: 0.3151 - val_loss: 0.1679 - val_mae: 0.3175\n",
      "Epoch 431/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1675 - mae: 0.3159 - val_loss: 0.1503 - val_mae: 0.2878\n",
      "Epoch 432/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1686 - mae: 0.3174 - val_loss: 0.1648 - val_mae: 0.3188\n",
      "Epoch 433/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1664 - mae: 0.3147 - val_loss: 0.1410 - val_mae: 0.2783\n",
      "Epoch 434/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1651 - mae: 0.3122 - val_loss: 0.1499 - val_mae: 0.2901\n",
      "Epoch 435/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1661 - mae: 0.3145 - val_loss: 0.1461 - val_mae: 0.2973\n",
      "Epoch 436/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1685 - mae: 0.3170 - val_loss: 0.1429 - val_mae: 0.2788\n",
      "Epoch 437/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1672 - mae: 0.3151 - val_loss: 0.1438 - val_mae: 0.2869\n",
      "Epoch 438/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1690 - mae: 0.3163 - val_loss: 0.1564 - val_mae: 0.2980\n",
      "Epoch 439/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1684 - mae: 0.3183 - val_loss: 0.1552 - val_mae: 0.2961\n",
      "Epoch 440/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1679 - mae: 0.3157 - val_loss: 0.1469 - val_mae: 0.2903\n",
      "Epoch 441/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1673 - mae: 0.3139 - val_loss: 0.1516 - val_mae: 0.2972\n",
      "Epoch 442/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1673 - mae: 0.3147 - val_loss: 0.1510 - val_mae: 0.2992\n",
      "Epoch 443/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1670 - mae: 0.3158 - val_loss: 0.1528 - val_mae: 0.3049\n",
      "Epoch 444/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1650 - mae: 0.3136 - val_loss: 0.1419 - val_mae: 0.2807\n",
      "Epoch 445/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1675 - mae: 0.3161 - val_loss: 0.1399 - val_mae: 0.2709\n",
      "Epoch 446/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1664 - mae: 0.3154 - val_loss: 0.1407 - val_mae: 0.2825\n",
      "Epoch 447/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1690 - mae: 0.3154 - val_loss: 0.1483 - val_mae: 0.2907\n",
      "Epoch 448/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1652 - mae: 0.3120 - val_loss: 0.1484 - val_mae: 0.2982\n",
      "Epoch 449/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1657 - mae: 0.3145 - val_loss: 0.1544 - val_mae: 0.3040\n",
      "Epoch 450/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1691 - mae: 0.3170 - val_loss: 0.1560 - val_mae: 0.3068\n",
      "Epoch 451/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1660 - mae: 0.3143 - val_loss: 0.1697 - val_mae: 0.3228\n",
      "Epoch 452/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1698 - mae: 0.3190 - val_loss: 0.1458 - val_mae: 0.2944\n",
      "Epoch 453/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1659 - mae: 0.3138 - val_loss: 0.1443 - val_mae: 0.2907\n",
      "Epoch 454/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1701 - mae: 0.3186 - val_loss: 0.1588 - val_mae: 0.3069\n",
      "Epoch 455/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1671 - mae: 0.3147 - val_loss: 0.1416 - val_mae: 0.2776\n",
      "Epoch 456/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1650 - mae: 0.3120 - val_loss: 0.1453 - val_mae: 0.2878\n",
      "Epoch 457/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1691 - mae: 0.3167 - val_loss: 0.1472 - val_mae: 0.2957\n",
      "Epoch 458/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1673 - mae: 0.3138 - val_loss: 0.1424 - val_mae: 0.2813\n",
      "Epoch 459/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1673 - mae: 0.3146 - val_loss: 0.1422 - val_mae: 0.2815\n",
      "Epoch 460/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1674 - mae: 0.3158 - val_loss: 0.1435 - val_mae: 0.2855\n",
      "Epoch 461/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1681 - mae: 0.3159 - val_loss: 0.1619 - val_mae: 0.3126\n",
      "Epoch 462/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1716 - mae: 0.3199 - val_loss: 0.1405 - val_mae: 0.2746\n",
      "Epoch 463/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1681 - mae: 0.3154 - val_loss: 0.1505 - val_mae: 0.2954\n",
      "Epoch 464/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1647 - mae: 0.3113 - val_loss: 0.1547 - val_mae: 0.3081\n",
      "Epoch 465/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1685 - mae: 0.3174 - val_loss: 0.1474 - val_mae: 0.2898\n",
      "Epoch 466/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1674 - mae: 0.3164 - val_loss: 0.1486 - val_mae: 0.2962\n",
      "Epoch 467/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1680 - mae: 0.3145 - val_loss: 0.1418 - val_mae: 0.2774\n",
      "Epoch 468/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1699 - mae: 0.3183 - val_loss: 0.1540 - val_mae: 0.2919\n",
      "Epoch 469/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1681 - mae: 0.3155 - val_loss: 0.1497 - val_mae: 0.2885\n",
      "Epoch 470/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1688 - mae: 0.3162 - val_loss: 0.1503 - val_mae: 0.2900\n",
      "Epoch 471/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1679 - mae: 0.3156 - val_loss: 0.1459 - val_mae: 0.2800\n",
      "Epoch 472/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1686 - mae: 0.3170 - val_loss: 0.1434 - val_mae: 0.2865\n",
      "Epoch 473/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1696 - mae: 0.3166 - val_loss: 0.1494 - val_mae: 0.3016\n",
      "Epoch 474/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1684 - mae: 0.3157 - val_loss: 0.1544 - val_mae: 0.3027\n",
      "Epoch 475/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1674 - mae: 0.3160 - val_loss: 0.1445 - val_mae: 0.2912\n",
      "Epoch 476/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1696 - mae: 0.3181 - val_loss: 0.1426 - val_mae: 0.2821\n",
      "Epoch 477/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1674 - mae: 0.3163 - val_loss: 0.1411 - val_mae: 0.2850\n",
      "Epoch 478/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1675 - mae: 0.3147 - val_loss: 0.1426 - val_mae: 0.2805\n",
      "Epoch 479/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1662 - mae: 0.3136 - val_loss: 0.1414 - val_mae: 0.2842\n",
      "Epoch 480/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1643 - mae: 0.3123 - val_loss: 0.1427 - val_mae: 0.2860\n",
      "Epoch 481/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1676 - mae: 0.3151 - val_loss: 0.1520 - val_mae: 0.2954\n",
      "Epoch 482/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1700 - mae: 0.3174 - val_loss: 0.1410 - val_mae: 0.2814\n",
      "Epoch 483/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1672 - mae: 0.3162 - val_loss: 0.1421 - val_mae: 0.2855\n",
      "Epoch 484/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1676 - mae: 0.3161 - val_loss: 0.1435 - val_mae: 0.2914\n",
      "Epoch 485/2000\n",
      "566/566 [==============================] - 7s 12ms/step - loss: 0.1682 - mae: 0.3149 - val_loss: 0.1383 - val_mae: 0.2734\n",
      "Epoch 486/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1696 - mae: 0.3182 - val_loss: 0.1446 - val_mae: 0.2835\n",
      "Epoch 487/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1678 - mae: 0.3151 - val_loss: 0.1445 - val_mae: 0.2905\n",
      "Epoch 488/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1689 - mae: 0.3164 - val_loss: 0.1434 - val_mae: 0.2832\n",
      "Epoch 489/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1674 - mae: 0.3165 - val_loss: 0.1433 - val_mae: 0.2823\n",
      "Epoch 490/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1656 - mae: 0.3135 - val_loss: 0.1504 - val_mae: 0.2947\n",
      "Epoch 491/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1651 - mae: 0.3131 - val_loss: 0.1512 - val_mae: 0.2998\n",
      "Epoch 492/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1688 - mae: 0.3163 - val_loss: 0.1401 - val_mae: 0.2807\n",
      "Epoch 493/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1644 - mae: 0.3116 - val_loss: 0.1462 - val_mae: 0.2927\n",
      "Epoch 494/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1664 - mae: 0.3127 - val_loss: 0.1428 - val_mae: 0.2817\n",
      "Epoch 495/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1667 - mae: 0.3141 - val_loss: 0.1478 - val_mae: 0.2960\n",
      "Epoch 496/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1675 - mae: 0.3162 - val_loss: 0.1475 - val_mae: 0.2945\n",
      "Epoch 497/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1668 - mae: 0.3158 - val_loss: 0.1469 - val_mae: 0.2948\n",
      "Epoch 498/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1683 - mae: 0.3168 - val_loss: 0.1397 - val_mae: 0.2773\n",
      "Epoch 499/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1644 - mae: 0.3112 - val_loss: 0.1499 - val_mae: 0.2971\n",
      "Epoch 500/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1679 - mae: 0.3158 - val_loss: 0.1412 - val_mae: 0.2756\n",
      "Epoch 501/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1664 - mae: 0.3136 - val_loss: 0.1406 - val_mae: 0.2816\n",
      "Epoch 502/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1699 - mae: 0.3194 - val_loss: 0.1466 - val_mae: 0.2948\n",
      "Epoch 503/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1658 - mae: 0.3154 - val_loss: 0.1476 - val_mae: 0.2815\n",
      "Epoch 504/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1699 - mae: 0.3190 - val_loss: 0.1459 - val_mae: 0.2880\n",
      "Epoch 505/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1686 - mae: 0.3171 - val_loss: 0.1414 - val_mae: 0.2757\n",
      "Epoch 506/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1659 - mae: 0.3147 - val_loss: 0.1474 - val_mae: 0.2920\n",
      "Epoch 507/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1657 - mae: 0.3139 - val_loss: 0.1466 - val_mae: 0.2828\n",
      "Epoch 508/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1673 - mae: 0.3158 - val_loss: 0.1436 - val_mae: 0.2840\n",
      "Epoch 509/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1678 - mae: 0.3183 - val_loss: 0.1539 - val_mae: 0.2928\n",
      "Epoch 510/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1663 - mae: 0.3130 - val_loss: 0.1481 - val_mae: 0.2904\n",
      "Epoch 511/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1666 - mae: 0.3149 - val_loss: 0.1541 - val_mae: 0.3049\n",
      "Epoch 512/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1657 - mae: 0.3148 - val_loss: 0.1521 - val_mae: 0.2954\n",
      "Epoch 513/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1670 - mae: 0.3162 - val_loss: 0.1537 - val_mae: 0.3058\n",
      "Epoch 514/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1656 - mae: 0.3152 - val_loss: 0.1457 - val_mae: 0.2904\n",
      "Epoch 515/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1704 - mae: 0.3185 - val_loss: 0.1393 - val_mae: 0.2779\n",
      "Epoch 516/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1687 - mae: 0.3187 - val_loss: 0.1396 - val_mae: 0.2785\n",
      "Epoch 517/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1690 - mae: 0.3201 - val_loss: 0.1468 - val_mae: 0.2946\n",
      "Epoch 518/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1675 - mae: 0.3155 - val_loss: 0.1495 - val_mae: 0.2967\n",
      "Epoch 519/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1682 - mae: 0.3182 - val_loss: 0.1473 - val_mae: 0.2871\n",
      "Epoch 520/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1671 - mae: 0.3162 - val_loss: 0.1422 - val_mae: 0.2864\n",
      "Epoch 521/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1663 - mae: 0.3161 - val_loss: 0.1425 - val_mae: 0.2873\n",
      "Epoch 522/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1663 - mae: 0.3142 - val_loss: 0.1507 - val_mae: 0.3004\n",
      "Epoch 523/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1638 - mae: 0.3126 - val_loss: 0.1461 - val_mae: 0.2925\n",
      "Epoch 524/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1643 - mae: 0.3130 - val_loss: 0.1517 - val_mae: 0.3003\n",
      "Epoch 525/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1674 - mae: 0.3163 - val_loss: 0.1449 - val_mae: 0.2912\n",
      "Epoch 526/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1679 - mae: 0.3169 - val_loss: 0.1548 - val_mae: 0.3140\n",
      "Epoch 527/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1663 - mae: 0.3154 - val_loss: 0.1445 - val_mae: 0.2796\n",
      "Epoch 528/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1681 - mae: 0.3176 - val_loss: 0.1482 - val_mae: 0.2964\n",
      "Epoch 529/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1669 - mae: 0.3162 - val_loss: 0.1487 - val_mae: 0.2939\n",
      "Epoch 530/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1648 - mae: 0.3137 - val_loss: 0.1400 - val_mae: 0.2772\n",
      "Epoch 531/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1632 - mae: 0.3136 - val_loss: 0.1503 - val_mae: 0.2982\n",
      "Epoch 532/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1675 - mae: 0.3163 - val_loss: 0.1548 - val_mae: 0.3058\n",
      "Epoch 533/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1678 - mae: 0.3166 - val_loss: 0.1443 - val_mae: 0.2857\n",
      "Epoch 534/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1704 - mae: 0.3197 - val_loss: 0.1472 - val_mae: 0.2938\n",
      "Epoch 535/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1665 - mae: 0.3166 - val_loss: 0.1518 - val_mae: 0.2977\n",
      "Epoch 536/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1658 - mae: 0.3149 - val_loss: 0.1526 - val_mae: 0.2970\n",
      "Epoch 537/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1674 - mae: 0.3187 - val_loss: 0.1462 - val_mae: 0.2926\n",
      "Epoch 538/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1661 - mae: 0.3145 - val_loss: 0.1393 - val_mae: 0.2754\n",
      "Epoch 539/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1657 - mae: 0.3125 - val_loss: 0.1476 - val_mae: 0.2964\n",
      "Epoch 540/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1676 - mae: 0.3170 - val_loss: 0.1405 - val_mae: 0.2773\n",
      "Epoch 541/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1629 - mae: 0.3113 - val_loss: 0.1426 - val_mae: 0.2785\n",
      "Epoch 542/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1649 - mae: 0.3138 - val_loss: 0.1496 - val_mae: 0.2983\n",
      "Epoch 543/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1681 - mae: 0.3168 - val_loss: 0.1513 - val_mae: 0.2911\n",
      "Epoch 544/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1689 - mae: 0.3178 - val_loss: 0.1530 - val_mae: 0.2975\n",
      "Epoch 545/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1661 - mae: 0.3142 - val_loss: 0.1549 - val_mae: 0.3102\n",
      "Epoch 546/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1662 - mae: 0.3157 - val_loss: 0.1418 - val_mae: 0.2793\n",
      "Epoch 547/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1663 - mae: 0.3154 - val_loss: 0.1542 - val_mae: 0.3035\n",
      "Epoch 548/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1666 - mae: 0.3154 - val_loss: 0.1479 - val_mae: 0.2937\n",
      "Epoch 549/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1656 - mae: 0.3137 - val_loss: 0.1423 - val_mae: 0.2894\n",
      "Epoch 550/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1681 - mae: 0.3169 - val_loss: 0.1419 - val_mae: 0.2869\n",
      "Epoch 551/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1657 - mae: 0.3137 - val_loss: 0.1553 - val_mae: 0.2993\n",
      "Epoch 552/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1677 - mae: 0.3154 - val_loss: 0.1430 - val_mae: 0.2865\n",
      "Epoch 553/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1645 - mae: 0.3126 - val_loss: 0.1416 - val_mae: 0.2857\n",
      "Epoch 554/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1703 - mae: 0.3186 - val_loss: 0.1428 - val_mae: 0.2817\n",
      "Epoch 555/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1658 - mae: 0.3150 - val_loss: 0.1435 - val_mae: 0.2806\n",
      "Epoch 556/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1692 - mae: 0.3167 - val_loss: 0.1449 - val_mae: 0.2860\n",
      "Epoch 557/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1685 - mae: 0.3167 - val_loss: 0.1497 - val_mae: 0.2956\n",
      "Epoch 558/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1658 - mae: 0.3142 - val_loss: 0.1585 - val_mae: 0.3025\n",
      "Epoch 559/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1689 - mae: 0.3160 - val_loss: 0.1445 - val_mae: 0.2886\n",
      "Epoch 560/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1645 - mae: 0.3123 - val_loss: 0.1461 - val_mae: 0.2968\n",
      "Epoch 561/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1676 - mae: 0.3143 - val_loss: 0.1487 - val_mae: 0.2927\n",
      "Epoch 562/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1680 - mae: 0.3164 - val_loss: 0.1408 - val_mae: 0.2861\n",
      "Epoch 563/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1670 - mae: 0.3166 - val_loss: 0.1567 - val_mae: 0.2970\n",
      "Epoch 564/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1700 - mae: 0.3181 - val_loss: 0.1436 - val_mae: 0.2886\n",
      "Epoch 565/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1688 - mae: 0.3175 - val_loss: 0.1476 - val_mae: 0.2929\n",
      "Epoch 566/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1689 - mae: 0.3188 - val_loss: 0.1478 - val_mae: 0.2931\n",
      "Epoch 567/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1674 - mae: 0.3166 - val_loss: 0.1605 - val_mae: 0.3102\n",
      "Epoch 568/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1652 - mae: 0.3148 - val_loss: 0.1393 - val_mae: 0.2833\n",
      "Epoch 569/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1678 - mae: 0.3160 - val_loss: 0.1406 - val_mae: 0.2766\n",
      "Epoch 570/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1659 - mae: 0.3161 - val_loss: 0.1470 - val_mae: 0.2869\n",
      "Epoch 571/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1642 - mae: 0.3133 - val_loss: 0.1461 - val_mae: 0.2855\n",
      "Epoch 572/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1662 - mae: 0.3156 - val_loss: 0.1402 - val_mae: 0.2803\n",
      "Epoch 573/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1634 - mae: 0.3123 - val_loss: 0.1388 - val_mae: 0.2731\n",
      "Epoch 574/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1652 - mae: 0.3141 - val_loss: 0.1395 - val_mae: 0.2804\n",
      "Epoch 575/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1660 - mae: 0.3146 - val_loss: 0.1424 - val_mae: 0.2761\n",
      "Epoch 576/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1676 - mae: 0.3178 - val_loss: 0.1432 - val_mae: 0.2835\n",
      "Epoch 577/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1627 - mae: 0.3107 - val_loss: 0.1488 - val_mae: 0.2921\n",
      "Epoch 578/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1649 - mae: 0.3131 - val_loss: 0.1477 - val_mae: 0.2879\n",
      "Epoch 579/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1654 - mae: 0.3134 - val_loss: 0.1412 - val_mae: 0.2869\n",
      "Epoch 580/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1658 - mae: 0.3152 - val_loss: 0.1418 - val_mae: 0.2801\n",
      "Epoch 581/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1654 - mae: 0.3149 - val_loss: 0.1452 - val_mae: 0.2934\n",
      "Epoch 582/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1648 - mae: 0.3143 - val_loss: 0.1578 - val_mae: 0.3102\n",
      "Epoch 583/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1654 - mae: 0.3133 - val_loss: 0.1472 - val_mae: 0.2850\n",
      "Epoch 584/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1655 - mae: 0.3149 - val_loss: 0.1387 - val_mae: 0.2726\n",
      "Epoch 585/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1654 - mae: 0.3154 - val_loss: 0.1447 - val_mae: 0.2835\n",
      "Models saved at: model/Sun Jul 21 22:20:22 2024\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "def load_catalogs(folder: str):\n",
    "    _img_name = []\n",
    "    _angle = []\n",
    "    _throttle = []\n",
    "\n",
    "    for _file in sorted(glob.glob(f\"{folder}/*.catalog\"),\n",
    "                        key=lambda x: [\n",
    "                            int(c) if c.isdigit()\n",
    "                            else c for c in re.split(r'(\\d+)', x)]):\n",
    "        with open(_file) as f:\n",
    "            for _line in f:\n",
    "                _img_name.append(_line.split()[7][1:-2])\n",
    "                _angle.append(float(_line.split()[9][0:-1]))\n",
    "                _throttle.append(float(_line.split()[13][0:-1]))\n",
    "\n",
    "    print(f'Image count: {len(_img_name)}')\n",
    "    return _img_name, _angle, _throttle\n",
    "\n",
    "\n",
    "def load_images(_img_name: list, folder: str):\n",
    "    _image = []\n",
    "    for i in range(len(_img_name)):\n",
    "        _img = cv2.imread(os.path.join(f\"{folder}/images\", _img_name[i]))\n",
    "        assert _img.shape == (224, 224, 3),\\\n",
    "            \"img %s has shape %r\" % (_img_name[i], _img.shape)\n",
    "        _image.append(_img)\n",
    "    return _image\n",
    "\n",
    "\n",
    "def data_preprocessing(_throttle, _angle, _image):\n",
    "    _throttle = np.array(_throttle)\n",
    "    _steering = np.array(_angle)\n",
    "    _train_img = np.array(_image)\n",
    "    _label = _steering\n",
    "    _cut_height = 80\n",
    "    _train_img_cut_orig = _train_img[:, _cut_height:224, :]\n",
    "    # _train_img_cut_gray = np.dot(_train_img_cut_orig[..., :3],\n",
    "    #                              [0.299, 0.587, 0.114])\n",
    "    _train_img_cut_gray = _train_img_cut_orig\n",
    "    return _train_img_cut_orig, _train_img_cut_gray, _label\n",
    "\n",
    "\n",
    "def train_split(_train_img_cut_orig, _train_img_cut_gray, _label):\n",
    "    _X_train, _X_val, _y_train, _y_val = train_test_split(\n",
    "        _train_img_cut_gray, _label,\n",
    "        test_size=0.15, random_state=42)\n",
    "    return _X_train, _X_val, _y_train, _y_val\n",
    "\n",
    "\n",
    "def build_fine_tuned_mobilenetv2_model(input_shape):\n",
    "    base_model = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    _x = base_model.output\n",
    "    _x = GlobalAveragePooling2D()(_x)\n",
    "    _x = Dense(1024, activation='relu')(_x)\n",
    "    _x = Dropout(0.5)(_x)\n",
    "    _outputs = Dense(1, activation='linear')(_x)\n",
    "\n",
    "    _model = Model(inputs=base_model.input, outputs=_outputs)\n",
    "    return _model\n",
    "\n",
    "\n",
    "def train_start(_model, _X_train, _X_val, _y_train, _y_val, \n",
    "                epochs: int=100, batch_size: int=16, patience: int=100, save_folder: str=''):\n",
    "    _optimizer = tf.optimizers.Adam(learning_rate=0.0001,\n",
    "                                    beta_1=0.9, beta_2=0.999)\n",
    "    _model.compile(optimizer=_optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
    "    _model.summary()\n",
    "    \n",
    "    # Add EarlyStopping callback\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                  patience=patience, \n",
    "                                                  restore_best_weights=True)\n",
    "    \n",
    "    # Add ModelCheckpoint callback to save the best model\n",
    "    best_model_path = os.path.join(save_folder, \"best_model.h5\")\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_path, \n",
    "                                                          monitor='val_loss', \n",
    "                                                          save_best_only=True)\n",
    "    \n",
    "    _trained_model = _model.fit(_X_train, _y_train,\n",
    "                                epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(_X_val, _y_val),\n",
    "                                callbacks=[early_stop, model_checkpoint])\n",
    "    return _trained_model\n",
    "\n",
    "\n",
    "def plot_trained_model(_trained_model, \n",
    "                       show: bool=False,\n",
    "                       save: bool=True,\n",
    "                       save_folder: str=''):\n",
    "    \n",
    "    history = _trained_model.history\n",
    "\n",
    "    plt.plot(history['loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(save_folder, f'Loss.png'),\n",
    "                    bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    plt.plot(history['mae'], label='Train MAE')\n",
    "    plt.plot(history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(save_folder, f'MAE.png'),\n",
    "                    bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"../data/data_0202\"\n",
    "    save_folder = f\"model/{time.ctime(time.time())}\"\n",
    "    # create save path\n",
    "    if not os.path.isdir(save_folder):\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    img_name, angle, throttle = load_catalogs(data_folder)\n",
    "    image = load_images(img_name, data_folder)\n",
    "    image = np.array(image)\n",
    "    train_img_cut_orig, train_img_cut_gray, label = data_preprocessing(\n",
    "        throttle, angle, image)\n",
    "    X_train, X_val, y_train, y_val = train_split(\n",
    "        train_img_cut_orig, train_img_cut_gray, label)\n",
    "\n",
    "    # Update input shape for MobileNetV2\n",
    "    model = build_fine_tuned_mobilenetv2_model(input_shape=(144, 224, 3))\n",
    "    trained_model = train_start(model, X_train, X_val, y_train, y_val, \n",
    "                               epochs=2000, save_folder=save_folder)\n",
    "    plot_trained_model(trained_model, show=False, save=True, save_folder=save_folder)\n",
    "    \n",
    "    # Save the last model\n",
    "    model.save(os.path.join(save_folder, \"last_model.h5\"))\n",
    "    \n",
    "    print(f\"Models saved at: {save_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29052ebc-1a23-4a51-a8f7-3063eb661cc0",
   "metadata": {},
   "source": [
    "### Models saved at: model/Sun Jul 21 22:20:22 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d5638-7e83-4021-b8a7-ec408fdcf645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.7.0",
   "language": "python",
   "name": "tensorflow-2.7.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
