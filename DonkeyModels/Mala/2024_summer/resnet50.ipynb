{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e4467d-47e8-47ef-acf4-d4c5536e48b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count: 10645\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 144, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 150, 230, 3)  0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 72, 112, 64)  9472        ['conv1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 72, 112, 64)  256         ['conv1_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 72, 112, 64)  0           ['conv1_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 74, 114, 64)  0           ['conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 36, 56, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 36, 56, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 36, 56, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 36, 56, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 36, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 36, 56, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 36, 56, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 36, 56, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 36, 56, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 36, 56, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 36, 56, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 36, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 36, 56, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 36, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 36, 56, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 36, 56, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 36, 56, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 36, 56, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 36, 56, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 36, 56, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 36, 56, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 18, 28, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 18, 28, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 18, 28, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 18, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 18, 28, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 18, 28, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 18, 28, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 18, 28, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 18, 28, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 18, 28, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 18, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 18, 28, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 18, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 18, 28, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 18, 28, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 18, 28, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 18, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 18, 28, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 18, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 18, 28, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 18, 28, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 18, 28, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 18, 28, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 18, 28, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 18, 28, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 18, 28, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 9, 14, 256)   131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 9, 14, 1024)  525312      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                                                  'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 9, 14, 256)   262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block1_out[0][0]',       \n",
      "                                                                  'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 9, 14, 256)   262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block2_out[0][0]',       \n",
      "                                                                  'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 9, 14, 256)   262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block3_out[0][0]',       \n",
      "                                                                  'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 9, 14, 256)   262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block4_out[0][0]',       \n",
      "                                                                  'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block5_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 9, 14, 256)   262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block5_out[0][0]',       \n",
      "                                                                  'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block6_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 5, 7, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 5, 7, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 5, 7, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 5, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 5, 7, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 5, 7, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 5, 7, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 5, 7, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 5, 7, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 5, 7, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 5, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 5, 7, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 5, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 5, 7, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 5, 7, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 5, 7, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 5, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 5, 7, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 5, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 5, 7, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1024)         2098176     ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 1024)         0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            1025        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25,686,913\n",
      "Trainable params: 2,099,201\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2000\n",
      "566/566 [==============================] - 9s 12ms/step - loss: 0.4388 - mae: 0.4947 - val_loss: 0.1856 - val_mae: 0.3356\n",
      "Epoch 2/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1917 - mae: 0.3431 - val_loss: 0.1667 - val_mae: 0.3151\n",
      "Epoch 3/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1799 - mae: 0.3303 - val_loss: 0.1633 - val_mae: 0.3116\n",
      "Epoch 4/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1691 - mae: 0.3210 - val_loss: 0.1524 - val_mae: 0.2944\n",
      "Epoch 5/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1646 - mae: 0.3147 - val_loss: 0.1546 - val_mae: 0.2960\n",
      "Epoch 6/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1592 - mae: 0.3095 - val_loss: 0.1629 - val_mae: 0.3109\n",
      "Epoch 7/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1572 - mae: 0.3067 - val_loss: 0.1477 - val_mae: 0.2844\n",
      "Epoch 8/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1521 - mae: 0.3011 - val_loss: 0.1442 - val_mae: 0.2877\n",
      "Epoch 9/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1505 - mae: 0.2994 - val_loss: 0.1456 - val_mae: 0.2834\n",
      "Epoch 10/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1483 - mae: 0.2959 - val_loss: 0.1455 - val_mae: 0.2871\n",
      "Epoch 11/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1455 - mae: 0.2934 - val_loss: 0.1588 - val_mae: 0.3079\n",
      "Epoch 12/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1442 - mae: 0.2917 - val_loss: 0.1420 - val_mae: 0.2800\n",
      "Epoch 13/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1407 - mae: 0.2871 - val_loss: 0.1407 - val_mae: 0.2799\n",
      "Epoch 14/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1359 - mae: 0.2826 - val_loss: 0.1388 - val_mae: 0.2772\n",
      "Epoch 15/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1337 - mae: 0.2781 - val_loss: 0.1375 - val_mae: 0.2764\n",
      "Epoch 16/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1321 - mae: 0.2781 - val_loss: 0.1393 - val_mae: 0.2791\n",
      "Epoch 17/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1308 - mae: 0.2764 - val_loss: 0.1370 - val_mae: 0.2771\n",
      "Epoch 18/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1273 - mae: 0.2719 - val_loss: 0.1409 - val_mae: 0.2794\n",
      "Epoch 19/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1259 - mae: 0.2690 - val_loss: 0.1345 - val_mae: 0.2669\n",
      "Epoch 20/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1209 - mae: 0.2644 - val_loss: 0.1327 - val_mae: 0.2700\n",
      "Epoch 21/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1207 - mae: 0.2636 - val_loss: 0.1343 - val_mae: 0.2646\n",
      "Epoch 22/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1199 - mae: 0.2621 - val_loss: 0.1287 - val_mae: 0.2662\n",
      "Epoch 23/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1175 - mae: 0.2592 - val_loss: 0.1316 - val_mae: 0.2685\n",
      "Epoch 24/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1150 - mae: 0.2554 - val_loss: 0.1314 - val_mae: 0.2653\n",
      "Epoch 25/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1128 - mae: 0.2532 - val_loss: 0.1305 - val_mae: 0.2670\n",
      "Epoch 26/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1094 - mae: 0.2516 - val_loss: 0.1277 - val_mae: 0.2587\n",
      "Epoch 27/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1095 - mae: 0.2494 - val_loss: 0.1295 - val_mae: 0.2632\n",
      "Epoch 28/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1090 - mae: 0.2494 - val_loss: 0.1349 - val_mae: 0.2683\n",
      "Epoch 29/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1080 - mae: 0.2475 - val_loss: 0.1279 - val_mae: 0.2635\n",
      "Epoch 30/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1044 - mae: 0.2426 - val_loss: 0.1333 - val_mae: 0.2729\n",
      "Epoch 31/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1024 - mae: 0.2423 - val_loss: 0.1250 - val_mae: 0.2577\n",
      "Epoch 32/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1013 - mae: 0.2391 - val_loss: 0.1302 - val_mae: 0.2656\n",
      "Epoch 33/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0978 - mae: 0.2349 - val_loss: 0.1268 - val_mae: 0.2546\n",
      "Epoch 34/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0975 - mae: 0.2357 - val_loss: 0.1290 - val_mae: 0.2589\n",
      "Epoch 35/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0953 - mae: 0.2322 - val_loss: 0.1265 - val_mae: 0.2591\n",
      "Epoch 36/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0962 - mae: 0.2331 - val_loss: 0.1257 - val_mae: 0.2582\n",
      "Epoch 37/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0936 - mae: 0.2300 - val_loss: 0.1266 - val_mae: 0.2566\n",
      "Epoch 38/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0903 - mae: 0.2254 - val_loss: 0.1278 - val_mae: 0.2623\n",
      "Epoch 39/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0905 - mae: 0.2259 - val_loss: 0.1217 - val_mae: 0.2571\n",
      "Epoch 40/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0866 - mae: 0.2206 - val_loss: 0.1257 - val_mae: 0.2562\n",
      "Epoch 41/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0878 - mae: 0.2225 - val_loss: 0.1307 - val_mae: 0.2624\n",
      "Epoch 42/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0862 - mae: 0.2202 - val_loss: 0.1231 - val_mae: 0.2498\n",
      "Epoch 43/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0853 - mae: 0.2192 - val_loss: 0.1265 - val_mae: 0.2540\n",
      "Epoch 44/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0816 - mae: 0.2140 - val_loss: 0.1241 - val_mae: 0.2530\n",
      "Epoch 45/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0805 - mae: 0.2121 - val_loss: 0.1236 - val_mae: 0.2544\n",
      "Epoch 46/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0808 - mae: 0.2133 - val_loss: 0.1277 - val_mae: 0.2627\n",
      "Epoch 47/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0783 - mae: 0.2101 - val_loss: 0.1243 - val_mae: 0.2521\n",
      "Epoch 48/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0773 - mae: 0.2080 - val_loss: 0.1252 - val_mae: 0.2526\n",
      "Epoch 49/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0747 - mae: 0.2043 - val_loss: 0.1239 - val_mae: 0.2537\n",
      "Epoch 50/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0764 - mae: 0.2062 - val_loss: 0.1227 - val_mae: 0.2512\n",
      "Epoch 51/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0751 - mae: 0.2042 - val_loss: 0.1222 - val_mae: 0.2525\n",
      "Epoch 52/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0746 - mae: 0.2031 - val_loss: 0.1244 - val_mae: 0.2583\n",
      "Epoch 53/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0733 - mae: 0.2021 - val_loss: 0.1252 - val_mae: 0.2589\n",
      "Epoch 54/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0713 - mae: 0.2000 - val_loss: 0.1236 - val_mae: 0.2530\n",
      "Epoch 55/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0700 - mae: 0.1980 - val_loss: 0.1218 - val_mae: 0.2542\n",
      "Epoch 56/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0693 - mae: 0.1962 - val_loss: 0.1234 - val_mae: 0.2522\n",
      "Epoch 57/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0680 - mae: 0.1949 - val_loss: 0.1218 - val_mae: 0.2502\n",
      "Epoch 58/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0673 - mae: 0.1950 - val_loss: 0.1235 - val_mae: 0.2496\n",
      "Epoch 59/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0665 - mae: 0.1930 - val_loss: 0.1310 - val_mae: 0.2631\n",
      "Epoch 60/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0668 - mae: 0.1930 - val_loss: 0.1241 - val_mae: 0.2505\n",
      "Epoch 61/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0652 - mae: 0.1906 - val_loss: 0.1217 - val_mae: 0.2530\n",
      "Epoch 62/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0639 - mae: 0.1885 - val_loss: 0.1246 - val_mae: 0.2564\n",
      "Epoch 63/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0659 - mae: 0.1908 - val_loss: 0.1237 - val_mae: 0.2497\n",
      "Epoch 64/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0627 - mae: 0.1861 - val_loss: 0.1244 - val_mae: 0.2500\n",
      "Epoch 65/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0614 - mae: 0.1849 - val_loss: 0.1281 - val_mae: 0.2575\n",
      "Epoch 66/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0621 - mae: 0.1851 - val_loss: 0.1248 - val_mae: 0.2482\n",
      "Epoch 67/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0615 - mae: 0.1844 - val_loss: 0.1255 - val_mae: 0.2522\n",
      "Epoch 68/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0597 - mae: 0.1820 - val_loss: 0.1239 - val_mae: 0.2515\n",
      "Epoch 69/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0587 - mae: 0.1810 - val_loss: 0.1240 - val_mae: 0.2547\n",
      "Epoch 70/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0589 - mae: 0.1810 - val_loss: 0.1202 - val_mae: 0.2456\n",
      "Epoch 71/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0587 - mae: 0.1799 - val_loss: 0.1182 - val_mae: 0.2481\n",
      "Epoch 72/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0591 - mae: 0.1813 - val_loss: 0.1277 - val_mae: 0.2543\n",
      "Epoch 73/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0575 - mae: 0.1799 - val_loss: 0.1233 - val_mae: 0.2522\n",
      "Epoch 74/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0548 - mae: 0.1731 - val_loss: 0.1221 - val_mae: 0.2483\n",
      "Epoch 75/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0577 - mae: 0.1789 - val_loss: 0.1250 - val_mae: 0.2534\n",
      "Epoch 76/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0559 - mae: 0.1765 - val_loss: 0.1252 - val_mae: 0.2514\n",
      "Epoch 77/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0554 - mae: 0.1753 - val_loss: 0.1242 - val_mae: 0.2482\n",
      "Epoch 78/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0540 - mae: 0.1737 - val_loss: 0.1222 - val_mae: 0.2483\n",
      "Epoch 79/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0536 - mae: 0.1720 - val_loss: 0.1220 - val_mae: 0.2471\n",
      "Epoch 80/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0534 - mae: 0.1718 - val_loss: 0.1195 - val_mae: 0.2461\n",
      "Epoch 81/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0535 - mae: 0.1717 - val_loss: 0.1210 - val_mae: 0.2425\n",
      "Epoch 82/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0530 - mae: 0.1703 - val_loss: 0.1210 - val_mae: 0.2486\n",
      "Epoch 83/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0524 - mae: 0.1702 - val_loss: 0.1223 - val_mae: 0.2474\n",
      "Epoch 84/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0521 - mae: 0.1694 - val_loss: 0.1210 - val_mae: 0.2490\n",
      "Epoch 85/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0525 - mae: 0.1703 - val_loss: 0.1206 - val_mae: 0.2520\n",
      "Epoch 86/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0521 - mae: 0.1704 - val_loss: 0.1213 - val_mae: 0.2452\n",
      "Epoch 87/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0509 - mae: 0.1687 - val_loss: 0.1216 - val_mae: 0.2500\n",
      "Epoch 88/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0489 - mae: 0.1640 - val_loss: 0.1190 - val_mae: 0.2465\n",
      "Epoch 89/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0513 - mae: 0.1688 - val_loss: 0.1215 - val_mae: 0.2470\n",
      "Epoch 90/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0496 - mae: 0.1654 - val_loss: 0.1169 - val_mae: 0.2426\n",
      "Epoch 91/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0497 - mae: 0.1648 - val_loss: 0.1237 - val_mae: 0.2492\n",
      "Epoch 92/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0485 - mae: 0.1647 - val_loss: 0.1231 - val_mae: 0.2494\n",
      "Epoch 93/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0494 - mae: 0.1644 - val_loss: 0.1258 - val_mae: 0.2557\n",
      "Epoch 94/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0487 - mae: 0.1628 - val_loss: 0.1238 - val_mae: 0.2513\n",
      "Epoch 95/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0474 - mae: 0.1611 - val_loss: 0.1221 - val_mae: 0.2481\n",
      "Epoch 96/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0492 - mae: 0.1640 - val_loss: 0.1232 - val_mae: 0.2458\n",
      "Epoch 97/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0472 - mae: 0.1614 - val_loss: 0.1215 - val_mae: 0.2457\n",
      "Epoch 98/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0466 - mae: 0.1603 - val_loss: 0.1211 - val_mae: 0.2427\n",
      "Epoch 99/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0480 - mae: 0.1627 - val_loss: 0.1204 - val_mae: 0.2456\n",
      "Epoch 100/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0449 - mae: 0.1573 - val_loss: 0.1192 - val_mae: 0.2435\n",
      "Epoch 101/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0472 - mae: 0.1619 - val_loss: 0.1197 - val_mae: 0.2423\n",
      "Epoch 102/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0451 - mae: 0.1574 - val_loss: 0.1221 - val_mae: 0.2468\n",
      "Epoch 103/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0452 - mae: 0.1566 - val_loss: 0.1198 - val_mae: 0.2432\n",
      "Epoch 104/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0454 - mae: 0.1573 - val_loss: 0.1195 - val_mae: 0.2444\n",
      "Epoch 105/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0455 - mae: 0.1586 - val_loss: 0.1217 - val_mae: 0.2470\n",
      "Epoch 106/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0456 - mae: 0.1582 - val_loss: 0.1239 - val_mae: 0.2458\n",
      "Epoch 107/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0454 - mae: 0.1574 - val_loss: 0.1192 - val_mae: 0.2414\n",
      "Epoch 108/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0438 - mae: 0.1543 - val_loss: 0.1218 - val_mae: 0.2440\n",
      "Epoch 109/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0420 - mae: 0.1516 - val_loss: 0.1189 - val_mae: 0.2428\n",
      "Epoch 110/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0441 - mae: 0.1549 - val_loss: 0.1206 - val_mae: 0.2421\n",
      "Epoch 111/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0438 - mae: 0.1548 - val_loss: 0.1189 - val_mae: 0.2424\n",
      "Epoch 112/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0425 - mae: 0.1520 - val_loss: 0.1233 - val_mae: 0.2437\n",
      "Epoch 113/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0423 - mae: 0.1514 - val_loss: 0.1203 - val_mae: 0.2391\n",
      "Epoch 114/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0440 - mae: 0.1546 - val_loss: 0.1225 - val_mae: 0.2449\n",
      "Epoch 115/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0414 - mae: 0.1507 - val_loss: 0.1196 - val_mae: 0.2390\n",
      "Epoch 116/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0428 - mae: 0.1536 - val_loss: 0.1213 - val_mae: 0.2428\n",
      "Epoch 117/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0429 - mae: 0.1522 - val_loss: 0.1252 - val_mae: 0.2537\n",
      "Epoch 118/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0429 - mae: 0.1529 - val_loss: 0.1170 - val_mae: 0.2356\n",
      "Epoch 119/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0417 - mae: 0.1502 - val_loss: 0.1215 - val_mae: 0.2459\n",
      "Epoch 120/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0407 - mae: 0.1490 - val_loss: 0.1208 - val_mae: 0.2407\n",
      "Epoch 121/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0401 - mae: 0.1479 - val_loss: 0.1187 - val_mae: 0.2411\n",
      "Epoch 122/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0421 - mae: 0.1511 - val_loss: 0.1222 - val_mae: 0.2463\n",
      "Epoch 123/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0418 - mae: 0.1499 - val_loss: 0.1198 - val_mae: 0.2381\n",
      "Epoch 124/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0415 - mae: 0.1502 - val_loss: 0.1224 - val_mae: 0.2491\n",
      "Epoch 125/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0392 - mae: 0.1455 - val_loss: 0.1225 - val_mae: 0.2448\n",
      "Epoch 126/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0397 - mae: 0.1467 - val_loss: 0.1182 - val_mae: 0.2444\n",
      "Epoch 127/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0401 - mae: 0.1480 - val_loss: 0.1191 - val_mae: 0.2411\n",
      "Epoch 128/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0397 - mae: 0.1465 - val_loss: 0.1246 - val_mae: 0.2475\n",
      "Epoch 129/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0398 - mae: 0.1471 - val_loss: 0.1183 - val_mae: 0.2405\n",
      "Epoch 130/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0411 - mae: 0.1496 - val_loss: 0.1209 - val_mae: 0.2405\n",
      "Epoch 131/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0412 - mae: 0.1487 - val_loss: 0.1221 - val_mae: 0.2433\n",
      "Epoch 132/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0393 - mae: 0.1460 - val_loss: 0.1197 - val_mae: 0.2418\n",
      "Epoch 133/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0399 - mae: 0.1466 - val_loss: 0.1250 - val_mae: 0.2477\n",
      "Epoch 134/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0393 - mae: 0.1449 - val_loss: 0.1193 - val_mae: 0.2421\n",
      "Epoch 135/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0373 - mae: 0.1433 - val_loss: 0.1213 - val_mae: 0.2424\n",
      "Epoch 136/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0373 - mae: 0.1425 - val_loss: 0.1177 - val_mae: 0.2382\n",
      "Epoch 137/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0377 - mae: 0.1420 - val_loss: 0.1206 - val_mae: 0.2413\n",
      "Epoch 138/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0378 - mae: 0.1423 - val_loss: 0.1171 - val_mae: 0.2375\n",
      "Epoch 139/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0389 - mae: 0.1451 - val_loss: 0.1205 - val_mae: 0.2421\n",
      "Epoch 140/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0398 - mae: 0.1466 - val_loss: 0.1190 - val_mae: 0.2371\n",
      "Epoch 141/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0366 - mae: 0.1407 - val_loss: 0.1187 - val_mae: 0.2381\n",
      "Epoch 142/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0372 - mae: 0.1418 - val_loss: 0.1197 - val_mae: 0.2397\n",
      "Epoch 143/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0384 - mae: 0.1434 - val_loss: 0.1180 - val_mae: 0.2384\n",
      "Epoch 144/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0374 - mae: 0.1417 - val_loss: 0.1248 - val_mae: 0.2516\n",
      "Epoch 145/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0390 - mae: 0.1454 - val_loss: 0.1183 - val_mae: 0.2373\n",
      "Epoch 146/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0366 - mae: 0.1410 - val_loss: 0.1212 - val_mae: 0.2436\n",
      "Epoch 147/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0366 - mae: 0.1394 - val_loss: 0.1227 - val_mae: 0.2397\n",
      "Epoch 148/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0370 - mae: 0.1405 - val_loss: 0.1206 - val_mae: 0.2405\n",
      "Epoch 149/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0358 - mae: 0.1386 - val_loss: 0.1189 - val_mae: 0.2389\n",
      "Epoch 150/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0373 - mae: 0.1405 - val_loss: 0.1211 - val_mae: 0.2401\n",
      "Epoch 151/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0364 - mae: 0.1390 - val_loss: 0.1218 - val_mae: 0.2407\n",
      "Epoch 152/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0371 - mae: 0.1408 - val_loss: 0.1185 - val_mae: 0.2376\n",
      "Epoch 153/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0365 - mae: 0.1394 - val_loss: 0.1228 - val_mae: 0.2456\n",
      "Epoch 154/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0369 - mae: 0.1399 - val_loss: 0.1170 - val_mae: 0.2350\n",
      "Epoch 155/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0355 - mae: 0.1371 - val_loss: 0.1205 - val_mae: 0.2423\n",
      "Epoch 156/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0370 - mae: 0.1396 - val_loss: 0.1198 - val_mae: 0.2380\n",
      "Epoch 157/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0355 - mae: 0.1377 - val_loss: 0.1176 - val_mae: 0.2378\n",
      "Epoch 158/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0366 - mae: 0.1403 - val_loss: 0.1185 - val_mae: 0.2408\n",
      "Epoch 159/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0349 - mae: 0.1366 - val_loss: 0.1191 - val_mae: 0.2366\n",
      "Epoch 160/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0365 - mae: 0.1388 - val_loss: 0.1174 - val_mae: 0.2361\n",
      "Epoch 161/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0344 - mae: 0.1349 - val_loss: 0.1199 - val_mae: 0.2394\n",
      "Epoch 162/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0355 - mae: 0.1373 - val_loss: 0.1184 - val_mae: 0.2365\n",
      "Epoch 163/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0355 - mae: 0.1377 - val_loss: 0.1206 - val_mae: 0.2380\n",
      "Epoch 164/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0359 - mae: 0.1389 - val_loss: 0.1190 - val_mae: 0.2382\n",
      "Epoch 165/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0359 - mae: 0.1377 - val_loss: 0.1201 - val_mae: 0.2378\n",
      "Epoch 166/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0357 - mae: 0.1379 - val_loss: 0.1181 - val_mae: 0.2386\n",
      "Epoch 167/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0339 - mae: 0.1346 - val_loss: 0.1189 - val_mae: 0.2356\n",
      "Epoch 168/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0344 - mae: 0.1352 - val_loss: 0.1198 - val_mae: 0.2390\n",
      "Epoch 169/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0370 - mae: 0.1398 - val_loss: 0.1209 - val_mae: 0.2398\n",
      "Epoch 170/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0353 - mae: 0.1364 - val_loss: 0.1205 - val_mae: 0.2439\n",
      "Epoch 171/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0343 - mae: 0.1346 - val_loss: 0.1179 - val_mae: 0.2371\n",
      "Epoch 172/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0350 - mae: 0.1372 - val_loss: 0.1199 - val_mae: 0.2399\n",
      "Epoch 173/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0349 - mae: 0.1362 - val_loss: 0.1187 - val_mae: 0.2378\n",
      "Epoch 174/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0336 - mae: 0.1339 - val_loss: 0.1182 - val_mae: 0.2399\n",
      "Epoch 175/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0344 - mae: 0.1360 - val_loss: 0.1185 - val_mae: 0.2348\n",
      "Epoch 176/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0351 - mae: 0.1366 - val_loss: 0.1188 - val_mae: 0.2411\n",
      "Epoch 177/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0335 - mae: 0.1335 - val_loss: 0.1199 - val_mae: 0.2382\n",
      "Epoch 178/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0336 - mae: 0.1344 - val_loss: 0.1217 - val_mae: 0.2410\n",
      "Epoch 179/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0339 - mae: 0.1339 - val_loss: 0.1196 - val_mae: 0.2406\n",
      "Epoch 180/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0351 - mae: 0.1367 - val_loss: 0.1189 - val_mae: 0.2380\n",
      "Epoch 181/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0340 - mae: 0.1343 - val_loss: 0.1204 - val_mae: 0.2387\n",
      "Epoch 182/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0339 - mae: 0.1334 - val_loss: 0.1191 - val_mae: 0.2372\n",
      "Epoch 183/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0342 - mae: 0.1343 - val_loss: 0.1171 - val_mae: 0.2357\n",
      "Epoch 184/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0325 - mae: 0.1319 - val_loss: 0.1203 - val_mae: 0.2405\n",
      "Epoch 185/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0337 - mae: 0.1340 - val_loss: 0.1199 - val_mae: 0.2392\n",
      "Epoch 186/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0336 - mae: 0.1331 - val_loss: 0.1193 - val_mae: 0.2408\n",
      "Epoch 187/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0339 - mae: 0.1339 - val_loss: 0.1190 - val_mae: 0.2374\n",
      "Epoch 188/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0347 - mae: 0.1338 - val_loss: 0.1166 - val_mae: 0.2345\n",
      "Epoch 189/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0335 - mae: 0.1327 - val_loss: 0.1199 - val_mae: 0.2468\n",
      "Epoch 190/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0319 - mae: 0.1316 - val_loss: 0.1175 - val_mae: 0.2349\n",
      "Epoch 191/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0333 - mae: 0.1327 - val_loss: 0.1186 - val_mae: 0.2375\n",
      "Epoch 192/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0332 - mae: 0.1324 - val_loss: 0.1185 - val_mae: 0.2414\n",
      "Epoch 193/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0330 - mae: 0.1316 - val_loss: 0.1186 - val_mae: 0.2401\n",
      "Epoch 194/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0337 - mae: 0.1327 - val_loss: 0.1216 - val_mae: 0.2430\n",
      "Epoch 195/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0330 - mae: 0.1320 - val_loss: 0.1190 - val_mae: 0.2368\n",
      "Epoch 196/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0324 - mae: 0.1314 - val_loss: 0.1234 - val_mae: 0.2434\n",
      "Epoch 197/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0332 - mae: 0.1324 - val_loss: 0.1255 - val_mae: 0.2474\n",
      "Epoch 198/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0329 - mae: 0.1323 - val_loss: 0.1185 - val_mae: 0.2382\n",
      "Epoch 199/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0331 - mae: 0.1315 - val_loss: 0.1213 - val_mae: 0.2383\n",
      "Epoch 200/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0326 - mae: 0.1300 - val_loss: 0.1200 - val_mae: 0.2384\n",
      "Epoch 201/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0343 - mae: 0.1325 - val_loss: 0.1193 - val_mae: 0.2415\n",
      "Epoch 202/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0316 - mae: 0.1291 - val_loss: 0.1191 - val_mae: 0.2377\n",
      "Epoch 203/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0337 - mae: 0.1320 - val_loss: 0.1187 - val_mae: 0.2410\n",
      "Epoch 204/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0334 - mae: 0.1317 - val_loss: 0.1165 - val_mae: 0.2371\n",
      "Epoch 205/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0338 - mae: 0.1322 - val_loss: 0.1278 - val_mae: 0.2493\n",
      "Epoch 206/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0318 - mae: 0.1291 - val_loss: 0.1179 - val_mae: 0.2384\n",
      "Epoch 207/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0311 - mae: 0.1274 - val_loss: 0.1185 - val_mae: 0.2370\n",
      "Epoch 208/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0313 - mae: 0.1278 - val_loss: 0.1169 - val_mae: 0.2362\n",
      "Epoch 209/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0316 - mae: 0.1284 - val_loss: 0.1191 - val_mae: 0.2393\n",
      "Epoch 210/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0315 - mae: 0.1281 - val_loss: 0.1185 - val_mae: 0.2365\n",
      "Epoch 211/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0319 - mae: 0.1286 - val_loss: 0.1208 - val_mae: 0.2403\n",
      "Epoch 212/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0319 - mae: 0.1294 - val_loss: 0.1186 - val_mae: 0.2378\n",
      "Epoch 213/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0333 - mae: 0.1319 - val_loss: 0.1205 - val_mae: 0.2411\n",
      "Epoch 214/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0323 - mae: 0.1297 - val_loss: 0.1191 - val_mae: 0.2375\n",
      "Epoch 215/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0319 - mae: 0.1293 - val_loss: 0.1185 - val_mae: 0.2364\n",
      "Epoch 216/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0321 - mae: 0.1293 - val_loss: 0.1173 - val_mae: 0.2407\n",
      "Epoch 217/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0316 - mae: 0.1289 - val_loss: 0.1190 - val_mae: 0.2357\n",
      "Epoch 218/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0317 - mae: 0.1285 - val_loss: 0.1188 - val_mae: 0.2373\n",
      "Epoch 219/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0310 - mae: 0.1280 - val_loss: 0.1231 - val_mae: 0.2431\n",
      "Epoch 220/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0308 - mae: 0.1266 - val_loss: 0.1187 - val_mae: 0.2356\n",
      "Epoch 221/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0318 - mae: 0.1282 - val_loss: 0.1184 - val_mae: 0.2339\n",
      "Epoch 222/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0318 - mae: 0.1287 - val_loss: 0.1167 - val_mae: 0.2342\n",
      "Epoch 223/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0329 - mae: 0.1310 - val_loss: 0.1184 - val_mae: 0.2378\n",
      "Epoch 224/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0321 - mae: 0.1288 - val_loss: 0.1208 - val_mae: 0.2437\n",
      "Epoch 225/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0306 - mae: 0.1263 - val_loss: 0.1183 - val_mae: 0.2357\n",
      "Epoch 226/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0301 - mae: 0.1255 - val_loss: 0.1200 - val_mae: 0.2378\n",
      "Epoch 227/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0308 - mae: 0.1268 - val_loss: 0.1163 - val_mae: 0.2347\n",
      "Epoch 228/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0307 - mae: 0.1259 - val_loss: 0.1176 - val_mae: 0.2382\n",
      "Epoch 229/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0318 - mae: 0.1274 - val_loss: 0.1164 - val_mae: 0.2353\n",
      "Epoch 230/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0317 - mae: 0.1291 - val_loss: 0.1188 - val_mae: 0.2402\n",
      "Epoch 231/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0304 - mae: 0.1266 - val_loss: 0.1184 - val_mae: 0.2370\n",
      "Epoch 232/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0321 - mae: 0.1285 - val_loss: 0.1166 - val_mae: 0.2377\n",
      "Epoch 233/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0304 - mae: 0.1260 - val_loss: 0.1181 - val_mae: 0.2397\n",
      "Epoch 234/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0325 - mae: 0.1301 - val_loss: 0.1206 - val_mae: 0.2386\n",
      "Epoch 235/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0301 - mae: 0.1253 - val_loss: 0.1199 - val_mae: 0.2399\n",
      "Epoch 236/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0313 - mae: 0.1259 - val_loss: 0.1212 - val_mae: 0.2411\n",
      "Epoch 237/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0294 - mae: 0.1242 - val_loss: 0.1190 - val_mae: 0.2356\n",
      "Epoch 238/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0317 - mae: 0.1277 - val_loss: 0.1194 - val_mae: 0.2387\n",
      "Epoch 239/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0308 - mae: 0.1259 - val_loss: 0.1218 - val_mae: 0.2424\n",
      "Epoch 240/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0307 - mae: 0.1263 - val_loss: 0.1186 - val_mae: 0.2376\n",
      "Epoch 241/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0304 - mae: 0.1256 - val_loss: 0.1173 - val_mae: 0.2333\n",
      "Epoch 242/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0310 - mae: 0.1263 - val_loss: 0.1206 - val_mae: 0.2420\n",
      "Epoch 243/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0293 - mae: 0.1231 - val_loss: 0.1182 - val_mae: 0.2367\n",
      "Epoch 244/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0299 - mae: 0.1244 - val_loss: 0.1175 - val_mae: 0.2382\n",
      "Epoch 245/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0294 - mae: 0.1234 - val_loss: 0.1226 - val_mae: 0.2403\n",
      "Epoch 246/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0304 - mae: 0.1254 - val_loss: 0.1232 - val_mae: 0.2449\n",
      "Epoch 247/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0298 - mae: 0.1241 - val_loss: 0.1185 - val_mae: 0.2375\n",
      "Epoch 248/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0300 - mae: 0.1249 - val_loss: 0.1204 - val_mae: 0.2369\n",
      "Epoch 249/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0294 - mae: 0.1235 - val_loss: 0.1179 - val_mae: 0.2365\n",
      "Epoch 250/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0291 - mae: 0.1231 - val_loss: 0.1182 - val_mae: 0.2393\n",
      "Epoch 251/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0302 - mae: 0.1247 - val_loss: 0.1167 - val_mae: 0.2333\n",
      "Epoch 252/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0298 - mae: 0.1234 - val_loss: 0.1175 - val_mae: 0.2348\n",
      "Epoch 253/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0296 - mae: 0.1223 - val_loss: 0.1204 - val_mae: 0.2384\n",
      "Epoch 254/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0290 - mae: 0.1216 - val_loss: 0.1169 - val_mae: 0.2336\n",
      "Epoch 255/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0313 - mae: 0.1261 - val_loss: 0.1160 - val_mae: 0.2348\n",
      "Epoch 256/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0302 - mae: 0.1232 - val_loss: 0.1187 - val_mae: 0.2358\n",
      "Epoch 257/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0302 - mae: 0.1247 - val_loss: 0.1194 - val_mae: 0.2372\n",
      "Epoch 258/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0291 - mae: 0.1220 - val_loss: 0.1177 - val_mae: 0.2376\n",
      "Epoch 259/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0303 - mae: 0.1246 - val_loss: 0.1179 - val_mae: 0.2347\n",
      "Epoch 260/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0295 - mae: 0.1227 - val_loss: 0.1159 - val_mae: 0.2346\n",
      "Epoch 261/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0285 - mae: 0.1215 - val_loss: 0.1186 - val_mae: 0.2348\n",
      "Epoch 262/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0307 - mae: 0.1253 - val_loss: 0.1158 - val_mae: 0.2333\n",
      "Epoch 263/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0286 - mae: 0.1211 - val_loss: 0.1188 - val_mae: 0.2382\n",
      "Epoch 264/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0302 - mae: 0.1240 - val_loss: 0.1181 - val_mae: 0.2325\n",
      "Epoch 265/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0297 - mae: 0.1232 - val_loss: 0.1172 - val_mae: 0.2319\n",
      "Epoch 266/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0294 - mae: 0.1234 - val_loss: 0.1161 - val_mae: 0.2324\n",
      "Epoch 267/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0285 - mae: 0.1210 - val_loss: 0.1212 - val_mae: 0.2403\n",
      "Epoch 268/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0297 - mae: 0.1229 - val_loss: 0.1171 - val_mae: 0.2337\n",
      "Epoch 269/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0277 - mae: 0.1196 - val_loss: 0.1182 - val_mae: 0.2328\n",
      "Epoch 270/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0293 - mae: 0.1222 - val_loss: 0.1216 - val_mae: 0.2432\n",
      "Epoch 271/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0289 - mae: 0.1211 - val_loss: 0.1169 - val_mae: 0.2355\n",
      "Epoch 272/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0294 - mae: 0.1226 - val_loss: 0.1206 - val_mae: 0.2414\n",
      "Epoch 273/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0283 - mae: 0.1201 - val_loss: 0.1161 - val_mae: 0.2327\n",
      "Epoch 274/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0288 - mae: 0.1208 - val_loss: 0.1180 - val_mae: 0.2345\n",
      "Epoch 275/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0293 - mae: 0.1213 - val_loss: 0.1194 - val_mae: 0.2373\n",
      "Epoch 276/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0287 - mae: 0.1219 - val_loss: 0.1191 - val_mae: 0.2345\n",
      "Epoch 277/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0290 - mae: 0.1213 - val_loss: 0.1185 - val_mae: 0.2343\n",
      "Epoch 278/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0294 - mae: 0.1227 - val_loss: 0.1191 - val_mae: 0.2377\n",
      "Epoch 279/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0292 - mae: 0.1227 - val_loss: 0.1188 - val_mae: 0.2376\n",
      "Epoch 280/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0280 - mae: 0.1191 - val_loss: 0.1169 - val_mae: 0.2343\n",
      "Epoch 281/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0283 - mae: 0.1199 - val_loss: 0.1207 - val_mae: 0.2377\n",
      "Epoch 282/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0289 - mae: 0.1211 - val_loss: 0.1183 - val_mae: 0.2400\n",
      "Epoch 283/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0289 - mae: 0.1216 - val_loss: 0.1194 - val_mae: 0.2396\n",
      "Epoch 284/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0284 - mae: 0.1203 - val_loss: 0.1191 - val_mae: 0.2362\n",
      "Epoch 285/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0276 - mae: 0.1201 - val_loss: 0.1201 - val_mae: 0.2369\n",
      "Epoch 286/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0280 - mae: 0.1195 - val_loss: 0.1169 - val_mae: 0.2367\n",
      "Epoch 287/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0290 - mae: 0.1208 - val_loss: 0.1186 - val_mae: 0.2385\n",
      "Epoch 288/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0301 - mae: 0.1237 - val_loss: 0.1210 - val_mae: 0.2428\n",
      "Epoch 289/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0276 - mae: 0.1183 - val_loss: 0.1232 - val_mae: 0.2423\n",
      "Epoch 290/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0276 - mae: 0.1192 - val_loss: 0.1181 - val_mae: 0.2367\n",
      "Epoch 291/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0279 - mae: 0.1195 - val_loss: 0.1173 - val_mae: 0.2327\n",
      "Epoch 292/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0282 - mae: 0.1198 - val_loss: 0.1189 - val_mae: 0.2367\n",
      "Epoch 293/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0289 - mae: 0.1205 - val_loss: 0.1160 - val_mae: 0.2316\n",
      "Epoch 294/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0269 - mae: 0.1170 - val_loss: 0.1199 - val_mae: 0.2386\n",
      "Epoch 295/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0294 - mae: 0.1218 - val_loss: 0.1207 - val_mae: 0.2355\n",
      "Epoch 296/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0287 - mae: 0.1207 - val_loss: 0.1210 - val_mae: 0.2387\n",
      "Epoch 297/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0276 - mae: 0.1185 - val_loss: 0.1166 - val_mae: 0.2349\n",
      "Epoch 298/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0275 - mae: 0.1177 - val_loss: 0.1185 - val_mae: 0.2333\n",
      "Epoch 299/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0277 - mae: 0.1189 - val_loss: 0.1198 - val_mae: 0.2384\n",
      "Epoch 300/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0284 - mae: 0.1199 - val_loss: 0.1187 - val_mae: 0.2367\n",
      "Epoch 301/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0278 - mae: 0.1185 - val_loss: 0.1215 - val_mae: 0.2411\n",
      "Epoch 302/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0290 - mae: 0.1215 - val_loss: 0.1173 - val_mae: 0.2355\n",
      "Epoch 303/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0274 - mae: 0.1183 - val_loss: 0.1195 - val_mae: 0.2365\n",
      "Epoch 304/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0277 - mae: 0.1186 - val_loss: 0.1183 - val_mae: 0.2375\n",
      "Epoch 305/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0277 - mae: 0.1187 - val_loss: 0.1205 - val_mae: 0.2406\n",
      "Epoch 306/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0274 - mae: 0.1185 - val_loss: 0.1166 - val_mae: 0.2361\n",
      "Epoch 307/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0287 - mae: 0.1210 - val_loss: 0.1194 - val_mae: 0.2350\n",
      "Epoch 308/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0268 - mae: 0.1163 - val_loss: 0.1180 - val_mae: 0.2360\n",
      "Epoch 309/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0279 - mae: 0.1190 - val_loss: 0.1163 - val_mae: 0.2335\n",
      "Epoch 310/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0272 - mae: 0.1177 - val_loss: 0.1218 - val_mae: 0.2411\n",
      "Epoch 311/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0276 - mae: 0.1185 - val_loss: 0.1180 - val_mae: 0.2356\n",
      "Epoch 312/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0272 - mae: 0.1170 - val_loss: 0.1176 - val_mae: 0.2366\n",
      "Epoch 313/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0279 - mae: 0.1184 - val_loss: 0.1193 - val_mae: 0.2363\n",
      "Epoch 314/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0271 - mae: 0.1172 - val_loss: 0.1200 - val_mae: 0.2407\n",
      "Epoch 315/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0272 - mae: 0.1171 - val_loss: 0.1199 - val_mae: 0.2378\n",
      "Epoch 316/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0277 - mae: 0.1185 - val_loss: 0.1174 - val_mae: 0.2342\n",
      "Epoch 317/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0276 - mae: 0.1175 - val_loss: 0.1182 - val_mae: 0.2350\n",
      "Epoch 318/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0281 - mae: 0.1193 - val_loss: 0.1196 - val_mae: 0.2402\n",
      "Epoch 319/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0275 - mae: 0.1186 - val_loss: 0.1203 - val_mae: 0.2411\n",
      "Epoch 320/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0274 - mae: 0.1177 - val_loss: 0.1194 - val_mae: 0.2394\n",
      "Epoch 321/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0279 - mae: 0.1185 - val_loss: 0.1177 - val_mae: 0.2361\n",
      "Epoch 322/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0277 - mae: 0.1178 - val_loss: 0.1203 - val_mae: 0.2379\n",
      "Epoch 323/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0269 - mae: 0.1170 - val_loss: 0.1180 - val_mae: 0.2363\n",
      "Epoch 324/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0270 - mae: 0.1167 - val_loss: 0.1187 - val_mae: 0.2333\n",
      "Epoch 325/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0270 - mae: 0.1161 - val_loss: 0.1224 - val_mae: 0.2391\n",
      "Epoch 326/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0274 - mae: 0.1177 - val_loss: 0.1162 - val_mae: 0.2328\n",
      "Epoch 327/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0262 - mae: 0.1155 - val_loss: 0.1182 - val_mae: 0.2351\n",
      "Epoch 328/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0272 - mae: 0.1168 - val_loss: 0.1195 - val_mae: 0.2369\n",
      "Epoch 329/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0278 - mae: 0.1176 - val_loss: 0.1191 - val_mae: 0.2381\n",
      "Epoch 330/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0266 - mae: 0.1156 - val_loss: 0.1174 - val_mae: 0.2343\n",
      "Epoch 331/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0261 - mae: 0.1143 - val_loss: 0.1198 - val_mae: 0.2362\n",
      "Epoch 332/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0277 - mae: 0.1176 - val_loss: 0.1180 - val_mae: 0.2397\n",
      "Epoch 333/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0261 - mae: 0.1150 - val_loss: 0.1221 - val_mae: 0.2367\n",
      "Epoch 334/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0273 - mae: 0.1171 - val_loss: 0.1209 - val_mae: 0.2373\n",
      "Epoch 335/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0263 - mae: 0.1147 - val_loss: 0.1180 - val_mae: 0.2352\n",
      "Epoch 336/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0266 - mae: 0.1163 - val_loss: 0.1198 - val_mae: 0.2390\n",
      "Epoch 337/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0270 - mae: 0.1165 - val_loss: 0.1210 - val_mae: 0.2385\n",
      "Epoch 338/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0272 - mae: 0.1170 - val_loss: 0.1223 - val_mae: 0.2416\n",
      "Epoch 339/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0258 - mae: 0.1145 - val_loss: 0.1218 - val_mae: 0.2417\n",
      "Epoch 340/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0266 - mae: 0.1152 - val_loss: 0.1199 - val_mae: 0.2386\n",
      "Epoch 341/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0268 - mae: 0.1163 - val_loss: 0.1185 - val_mae: 0.2333\n",
      "Epoch 342/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0257 - mae: 0.1141 - val_loss: 0.1188 - val_mae: 0.2364\n",
      "Epoch 343/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0272 - mae: 0.1168 - val_loss: 0.1195 - val_mae: 0.2371\n",
      "Epoch 344/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0267 - mae: 0.1158 - val_loss: 0.1195 - val_mae: 0.2378\n",
      "Epoch 345/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0268 - mae: 0.1164 - val_loss: 0.1191 - val_mae: 0.2358\n",
      "Epoch 346/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0268 - mae: 0.1159 - val_loss: 0.1201 - val_mae: 0.2353\n",
      "Epoch 347/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0258 - mae: 0.1141 - val_loss: 0.1194 - val_mae: 0.2347\n",
      "Epoch 348/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0257 - mae: 0.1133 - val_loss: 0.1202 - val_mae: 0.2376\n",
      "Epoch 349/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0271 - mae: 0.1165 - val_loss: 0.1202 - val_mae: 0.2364\n",
      "Epoch 350/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0277 - mae: 0.1176 - val_loss: 0.1183 - val_mae: 0.2377\n",
      "Epoch 351/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0266 - mae: 0.1153 - val_loss: 0.1196 - val_mae: 0.2363\n",
      "Epoch 352/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0272 - mae: 0.1166 - val_loss: 0.1202 - val_mae: 0.2382\n",
      "Epoch 353/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0263 - mae: 0.1149 - val_loss: 0.1190 - val_mae: 0.2363\n",
      "Epoch 354/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0255 - mae: 0.1137 - val_loss: 0.1172 - val_mae: 0.2363\n",
      "Epoch 355/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0260 - mae: 0.1144 - val_loss: 0.1206 - val_mae: 0.2400\n",
      "Epoch 356/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0261 - mae: 0.1145 - val_loss: 0.1179 - val_mae: 0.2350\n",
      "Epoch 357/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0257 - mae: 0.1136 - val_loss: 0.1208 - val_mae: 0.2373\n",
      "Epoch 358/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0262 - mae: 0.1152 - val_loss: 0.1175 - val_mae: 0.2342\n",
      "Epoch 359/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0262 - mae: 0.1144 - val_loss: 0.1182 - val_mae: 0.2365\n",
      "Epoch 360/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0274 - mae: 0.1172 - val_loss: 0.1227 - val_mae: 0.2406\n",
      "Epoch 361/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0255 - mae: 0.1122 - val_loss: 0.1194 - val_mae: 0.2392\n",
      "Epoch 362/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0265 - mae: 0.1155 - val_loss: 0.1171 - val_mae: 0.2354\n",
      "Epoch 363/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0263 - mae: 0.1153 - val_loss: 0.1207 - val_mae: 0.2371\n",
      "Epoch 364/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0248 - mae: 0.1108 - val_loss: 0.1207 - val_mae: 0.2365\n",
      "Epoch 365/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0259 - mae: 0.1138 - val_loss: 0.1193 - val_mae: 0.2369\n",
      "Epoch 366/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0260 - mae: 0.1134 - val_loss: 0.1177 - val_mae: 0.2342\n",
      "Epoch 367/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0260 - mae: 0.1146 - val_loss: 0.1185 - val_mae: 0.2379\n",
      "Epoch 368/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0254 - mae: 0.1125 - val_loss: 0.1194 - val_mae: 0.2409\n",
      "Epoch 369/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0259 - mae: 0.1142 - val_loss: 0.1172 - val_mae: 0.2377\n",
      "Epoch 370/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0258 - mae: 0.1147 - val_loss: 0.1178 - val_mae: 0.2356\n",
      "Epoch 371/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0257 - mae: 0.1134 - val_loss: 0.1180 - val_mae: 0.2355\n",
      "Epoch 372/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0259 - mae: 0.1139 - val_loss: 0.1185 - val_mae: 0.2348\n",
      "Epoch 373/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0264 - mae: 0.1152 - val_loss: 0.1177 - val_mae: 0.2346\n",
      "Epoch 374/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0262 - mae: 0.1144 - val_loss: 0.1192 - val_mae: 0.2383\n",
      "Epoch 375/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0257 - mae: 0.1134 - val_loss: 0.1171 - val_mae: 0.2327\n",
      "Epoch 376/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0257 - mae: 0.1128 - val_loss: 0.1206 - val_mae: 0.2409\n",
      "Epoch 377/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0249 - mae: 0.1114 - val_loss: 0.1192 - val_mae: 0.2377\n",
      "Epoch 378/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0253 - mae: 0.1133 - val_loss: 0.1205 - val_mae: 0.2383\n",
      "Epoch 379/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0247 - mae: 0.1114 - val_loss: 0.1191 - val_mae: 0.2385\n",
      "Epoch 380/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0261 - mae: 0.1142 - val_loss: 0.1212 - val_mae: 0.2415\n",
      "Epoch 381/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0245 - mae: 0.1108 - val_loss: 0.1165 - val_mae: 0.2357\n",
      "Epoch 382/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0267 - mae: 0.1161 - val_loss: 0.1222 - val_mae: 0.2416\n",
      "Epoch 383/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0256 - mae: 0.1128 - val_loss: 0.1176 - val_mae: 0.2367\n",
      "Epoch 384/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0257 - mae: 0.1132 - val_loss: 0.1166 - val_mae: 0.2347\n",
      "Epoch 385/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0262 - mae: 0.1140 - val_loss: 0.1220 - val_mae: 0.2369\n",
      "Epoch 386/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0251 - mae: 0.1119 - val_loss: 0.1182 - val_mae: 0.2355\n",
      "Epoch 387/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0248 - mae: 0.1113 - val_loss: 0.1183 - val_mae: 0.2330\n",
      "Epoch 388/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0259 - mae: 0.1135 - val_loss: 0.1223 - val_mae: 0.2420\n",
      "Epoch 389/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0253 - mae: 0.1125 - val_loss: 0.1189 - val_mae: 0.2360\n",
      "Epoch 390/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0258 - mae: 0.1129 - val_loss: 0.1193 - val_mae: 0.2342\n",
      "Epoch 391/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0252 - mae: 0.1120 - val_loss: 0.1201 - val_mae: 0.2375\n",
      "Epoch 392/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0253 - mae: 0.1127 - val_loss: 0.1197 - val_mae: 0.2432\n",
      "Epoch 393/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0251 - mae: 0.1118 - val_loss: 0.1172 - val_mae: 0.2341\n",
      "Epoch 394/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0251 - mae: 0.1117 - val_loss: 0.1233 - val_mae: 0.2443\n",
      "Epoch 395/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0250 - mae: 0.1120 - val_loss: 0.1201 - val_mae: 0.2391\n",
      "Epoch 396/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0259 - mae: 0.1132 - val_loss: 0.1208 - val_mae: 0.2367\n",
      "Epoch 397/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0254 - mae: 0.1123 - val_loss: 0.1183 - val_mae: 0.2338\n",
      "Epoch 398/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0243 - mae: 0.1097 - val_loss: 0.1181 - val_mae: 0.2349\n",
      "Epoch 399/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0250 - mae: 0.1119 - val_loss: 0.1172 - val_mae: 0.2325\n",
      "Epoch 400/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0255 - mae: 0.1127 - val_loss: 0.1193 - val_mae: 0.2373\n",
      "Epoch 401/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0247 - mae: 0.1111 - val_loss: 0.1174 - val_mae: 0.2356\n",
      "Epoch 402/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0249 - mae: 0.1111 - val_loss: 0.1211 - val_mae: 0.2402\n",
      "Epoch 403/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0254 - mae: 0.1125 - val_loss: 0.1207 - val_mae: 0.2384\n",
      "Epoch 404/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0249 - mae: 0.1105 - val_loss: 0.1206 - val_mae: 0.2403\n",
      "Epoch 405/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0253 - mae: 0.1124 - val_loss: 0.1200 - val_mae: 0.2381\n",
      "Epoch 406/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0246 - mae: 0.1102 - val_loss: 0.1197 - val_mae: 0.2346\n",
      "Epoch 407/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0240 - mae: 0.1092 - val_loss: 0.1191 - val_mae: 0.2362\n",
      "Epoch 408/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0254 - mae: 0.1122 - val_loss: 0.1169 - val_mae: 0.2347\n",
      "Epoch 409/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0258 - mae: 0.1127 - val_loss: 0.1195 - val_mae: 0.2385\n",
      "Epoch 410/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0248 - mae: 0.1110 - val_loss: 0.1197 - val_mae: 0.2355\n",
      "Epoch 411/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0247 - mae: 0.1110 - val_loss: 0.1194 - val_mae: 0.2350\n",
      "Epoch 412/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0244 - mae: 0.1091 - val_loss: 0.1181 - val_mae: 0.2347\n",
      "Epoch 413/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0233 - mae: 0.1078 - val_loss: 0.1190 - val_mae: 0.2347\n",
      "Epoch 414/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0246 - mae: 0.1101 - val_loss: 0.1187 - val_mae: 0.2353\n",
      "Epoch 415/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0253 - mae: 0.1116 - val_loss: 0.1183 - val_mae: 0.2360\n",
      "Epoch 416/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0250 - mae: 0.1122 - val_loss: 0.1171 - val_mae: 0.2368\n",
      "Epoch 417/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0249 - mae: 0.1114 - val_loss: 0.1185 - val_mae: 0.2387\n",
      "Epoch 418/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0247 - mae: 0.1105 - val_loss: 0.1213 - val_mae: 0.2396\n",
      "Epoch 419/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0242 - mae: 0.1105 - val_loss: 0.1191 - val_mae: 0.2331\n",
      "Epoch 420/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0246 - mae: 0.1100 - val_loss: 0.1199 - val_mae: 0.2349\n",
      "Epoch 421/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0250 - mae: 0.1113 - val_loss: 0.1183 - val_mae: 0.2367\n",
      "Epoch 422/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0247 - mae: 0.1114 - val_loss: 0.1189 - val_mae: 0.2340\n",
      "Epoch 423/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0246 - mae: 0.1102 - val_loss: 0.1188 - val_mae: 0.2349\n",
      "Epoch 424/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0248 - mae: 0.1109 - val_loss: 0.1161 - val_mae: 0.2350\n",
      "Epoch 425/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0246 - mae: 0.1107 - val_loss: 0.1187 - val_mae: 0.2335\n",
      "Epoch 426/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0233 - mae: 0.1074 - val_loss: 0.1187 - val_mae: 0.2367\n",
      "Epoch 427/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0251 - mae: 0.1113 - val_loss: 0.1181 - val_mae: 0.2338\n",
      "Epoch 428/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0243 - mae: 0.1098 - val_loss: 0.1183 - val_mae: 0.2357\n",
      "Epoch 429/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0249 - mae: 0.1105 - val_loss: 0.1193 - val_mae: 0.2334\n",
      "Epoch 430/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0241 - mae: 0.1086 - val_loss: 0.1194 - val_mae: 0.2386\n",
      "Epoch 431/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0240 - mae: 0.1095 - val_loss: 0.1170 - val_mae: 0.2346\n",
      "Epoch 432/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0244 - mae: 0.1101 - val_loss: 0.1194 - val_mae: 0.2354\n",
      "Epoch 433/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0234 - mae: 0.1076 - val_loss: 0.1171 - val_mae: 0.2331\n",
      "Epoch 434/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0252 - mae: 0.1110 - val_loss: 0.1185 - val_mae: 0.2352\n",
      "Epoch 435/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0242 - mae: 0.1091 - val_loss: 0.1188 - val_mae: 0.2353\n",
      "Epoch 436/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0235 - mae: 0.1081 - val_loss: 0.1181 - val_mae: 0.2344\n",
      "Epoch 437/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0237 - mae: 0.1083 - val_loss: 0.1200 - val_mae: 0.2358\n",
      "Epoch 438/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0244 - mae: 0.1096 - val_loss: 0.1215 - val_mae: 0.2380\n",
      "Epoch 439/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0241 - mae: 0.1091 - val_loss: 0.1207 - val_mae: 0.2372\n",
      "Epoch 440/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0242 - mae: 0.1095 - val_loss: 0.1215 - val_mae: 0.2377\n",
      "Epoch 441/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0253 - mae: 0.1121 - val_loss: 0.1204 - val_mae: 0.2385\n",
      "Epoch 442/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0246 - mae: 0.1108 - val_loss: 0.1210 - val_mae: 0.2366\n",
      "Epoch 443/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0246 - mae: 0.1103 - val_loss: 0.1204 - val_mae: 0.2417\n",
      "Epoch 444/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0249 - mae: 0.1105 - val_loss: 0.1283 - val_mae: 0.2514\n",
      "Epoch 445/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0246 - mae: 0.1106 - val_loss: 0.1189 - val_mae: 0.2357\n",
      "Epoch 446/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0243 - mae: 0.1094 - val_loss: 0.1197 - val_mae: 0.2346\n",
      "Epoch 447/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0247 - mae: 0.1103 - val_loss: 0.1211 - val_mae: 0.2394\n",
      "Epoch 448/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0246 - mae: 0.1113 - val_loss: 0.1177 - val_mae: 0.2338\n",
      "Epoch 449/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0236 - mae: 0.1079 - val_loss: 0.1206 - val_mae: 0.2370\n",
      "Epoch 450/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0239 - mae: 0.1088 - val_loss: 0.1186 - val_mae: 0.2359\n",
      "Epoch 451/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0233 - mae: 0.1067 - val_loss: 0.1209 - val_mae: 0.2373\n",
      "Epoch 452/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0245 - mae: 0.1095 - val_loss: 0.1193 - val_mae: 0.2383\n",
      "Epoch 453/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0236 - mae: 0.1083 - val_loss: 0.1192 - val_mae: 0.2354\n",
      "Epoch 454/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0231 - mae: 0.1068 - val_loss: 0.1197 - val_mae: 0.2343\n",
      "Epoch 455/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0237 - mae: 0.1085 - val_loss: 0.1185 - val_mae: 0.2352\n",
      "Epoch 456/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0242 - mae: 0.1089 - val_loss: 0.1193 - val_mae: 0.2349\n",
      "Epoch 457/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0240 - mae: 0.1083 - val_loss: 0.1200 - val_mae: 0.2384\n",
      "Epoch 458/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0241 - mae: 0.1086 - val_loss: 0.1182 - val_mae: 0.2341\n",
      "Epoch 459/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0230 - mae: 0.1064 - val_loss: 0.1175 - val_mae: 0.2354\n",
      "Epoch 460/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0243 - mae: 0.1094 - val_loss: 0.1203 - val_mae: 0.2379\n",
      "Epoch 461/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0235 - mae: 0.1083 - val_loss: 0.1212 - val_mae: 0.2350\n",
      "Epoch 462/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0244 - mae: 0.1095 - val_loss: 0.1217 - val_mae: 0.2346\n",
      "Epoch 463/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0244 - mae: 0.1089 - val_loss: 0.1205 - val_mae: 0.2367\n",
      "Epoch 464/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0236 - mae: 0.1080 - val_loss: 0.1180 - val_mae: 0.2348\n",
      "Epoch 465/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0234 - mae: 0.1064 - val_loss: 0.1172 - val_mae: 0.2334\n",
      "Epoch 466/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0242 - mae: 0.1089 - val_loss: 0.1200 - val_mae: 0.2383\n",
      "Epoch 467/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0235 - mae: 0.1071 - val_loss: 0.1189 - val_mae: 0.2370\n",
      "Epoch 468/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0238 - mae: 0.1082 - val_loss: 0.1211 - val_mae: 0.2398\n",
      "Epoch 469/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0236 - mae: 0.1072 - val_loss: 0.1184 - val_mae: 0.2363\n",
      "Epoch 470/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0238 - mae: 0.1086 - val_loss: 0.1201 - val_mae: 0.2363\n",
      "Epoch 471/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0239 - mae: 0.1077 - val_loss: 0.1199 - val_mae: 0.2370\n",
      "Epoch 472/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0233 - mae: 0.1061 - val_loss: 0.1205 - val_mae: 0.2372\n",
      "Epoch 473/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0237 - mae: 0.1078 - val_loss: 0.1191 - val_mae: 0.2356\n",
      "Epoch 474/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0238 - mae: 0.1085 - val_loss: 0.1196 - val_mae: 0.2357\n",
      "Epoch 475/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0247 - mae: 0.1106 - val_loss: 0.1227 - val_mae: 0.2413\n",
      "Epoch 476/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0239 - mae: 0.1083 - val_loss: 0.1207 - val_mae: 0.2401\n",
      "Epoch 477/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0236 - mae: 0.1078 - val_loss: 0.1184 - val_mae: 0.2345\n",
      "Epoch 478/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0231 - mae: 0.1066 - val_loss: 0.1184 - val_mae: 0.2330\n",
      "Epoch 479/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0238 - mae: 0.1074 - val_loss: 0.1202 - val_mae: 0.2395\n",
      "Epoch 480/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0249 - mae: 0.1109 - val_loss: 0.1171 - val_mae: 0.2355\n",
      "Epoch 481/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0233 - mae: 0.1069 - val_loss: 0.1192 - val_mae: 0.2366\n",
      "Epoch 482/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0241 - mae: 0.1084 - val_loss: 0.1203 - val_mae: 0.2357\n",
      "Epoch 483/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0237 - mae: 0.1078 - val_loss: 0.1224 - val_mae: 0.2391\n",
      "Epoch 484/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0232 - mae: 0.1064 - val_loss: 0.1195 - val_mae: 0.2353\n",
      "Epoch 485/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0234 - mae: 0.1074 - val_loss: 0.1195 - val_mae: 0.2377\n",
      "Epoch 486/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0244 - mae: 0.1095 - val_loss: 0.1177 - val_mae: 0.2340\n",
      "Epoch 487/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0232 - mae: 0.1065 - val_loss: 0.1182 - val_mae: 0.2355\n",
      "Epoch 488/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0236 - mae: 0.1075 - val_loss: 0.1201 - val_mae: 0.2402\n",
      "Epoch 489/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0232 - mae: 0.1066 - val_loss: 0.1203 - val_mae: 0.2369\n",
      "Epoch 490/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0234 - mae: 0.1075 - val_loss: 0.1191 - val_mae: 0.2359\n",
      "Epoch 491/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0231 - mae: 0.1067 - val_loss: 0.1178 - val_mae: 0.2358\n",
      "Epoch 492/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0229 - mae: 0.1067 - val_loss: 0.1172 - val_mae: 0.2334\n",
      "Epoch 493/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0228 - mae: 0.1069 - val_loss: 0.1195 - val_mae: 0.2346\n",
      "Epoch 494/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0230 - mae: 0.1070 - val_loss: 0.1206 - val_mae: 0.2363\n",
      "Epoch 495/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0230 - mae: 0.1068 - val_loss: 0.1222 - val_mae: 0.2447\n",
      "Epoch 496/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0238 - mae: 0.1086 - val_loss: 0.1188 - val_mae: 0.2368\n",
      "Epoch 497/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0231 - mae: 0.1065 - val_loss: 0.1161 - val_mae: 0.2296\n",
      "Epoch 498/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0234 - mae: 0.1061 - val_loss: 0.1197 - val_mae: 0.2367\n",
      "Epoch 499/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0234 - mae: 0.1073 - val_loss: 0.1196 - val_mae: 0.2375\n",
      "Epoch 500/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0225 - mae: 0.1045 - val_loss: 0.1192 - val_mae: 0.2339\n",
      "Epoch 501/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0225 - mae: 0.1047 - val_loss: 0.1217 - val_mae: 0.2397\n",
      "Epoch 502/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0232 - mae: 0.1068 - val_loss: 0.1192 - val_mae: 0.2365\n",
      "Epoch 503/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0235 - mae: 0.1068 - val_loss: 0.1187 - val_mae: 0.2357\n",
      "Epoch 504/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0232 - mae: 0.1064 - val_loss: 0.1193 - val_mae: 0.2356\n",
      "Epoch 505/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0230 - mae: 0.1059 - val_loss: 0.1180 - val_mae: 0.2331\n",
      "Epoch 506/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0238 - mae: 0.1084 - val_loss: 0.1218 - val_mae: 0.2417\n",
      "Epoch 507/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0229 - mae: 0.1050 - val_loss: 0.1193 - val_mae: 0.2345\n",
      "Epoch 508/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0221 - mae: 0.1044 - val_loss: 0.1193 - val_mae: 0.2380\n",
      "Epoch 509/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0234 - mae: 0.1067 - val_loss: 0.1175 - val_mae: 0.2343\n",
      "Epoch 510/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0231 - mae: 0.1059 - val_loss: 0.1204 - val_mae: 0.2396\n",
      "Epoch 511/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0228 - mae: 0.1055 - val_loss: 0.1216 - val_mae: 0.2395\n",
      "Epoch 512/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0226 - mae: 0.1055 - val_loss: 0.1211 - val_mae: 0.2374\n",
      "Epoch 513/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0226 - mae: 0.1049 - val_loss: 0.1185 - val_mae: 0.2377\n",
      "Epoch 514/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0231 - mae: 0.1056 - val_loss: 0.1198 - val_mae: 0.2377\n",
      "Epoch 515/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0233 - mae: 0.1074 - val_loss: 0.1225 - val_mae: 0.2398\n",
      "Epoch 516/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0225 - mae: 0.1047 - val_loss: 0.1190 - val_mae: 0.2348\n",
      "Epoch 517/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0225 - mae: 0.1042 - val_loss: 0.1199 - val_mae: 0.2352\n",
      "Epoch 519/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0238 - mae: 0.1077 - val_loss: 0.1212 - val_mae: 0.2418\n",
      "Epoch 520/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0235 - mae: 0.1062 - val_loss: 0.1193 - val_mae: 0.2338\n",
      "Epoch 521/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0224 - mae: 0.1043 - val_loss: 0.1190 - val_mae: 0.2358\n",
      "Epoch 522/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0218 - mae: 0.1025 - val_loss: 0.1187 - val_mae: 0.2342\n",
      "Epoch 523/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0221 - mae: 0.1044 - val_loss: 0.1174 - val_mae: 0.2336\n",
      "Epoch 524/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0229 - mae: 0.1061 - val_loss: 0.1192 - val_mae: 0.2347\n",
      "Epoch 525/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0234 - mae: 0.1063 - val_loss: 0.1220 - val_mae: 0.2397\n",
      "Epoch 526/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0223 - mae: 0.1044 - val_loss: 0.1196 - val_mae: 0.2381\n",
      "Epoch 527/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0224 - mae: 0.1049 - val_loss: 0.1202 - val_mae: 0.2348\n",
      "Epoch 528/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0231 - mae: 0.1061 - val_loss: 0.1193 - val_mae: 0.2350\n",
      "Epoch 529/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0229 - mae: 0.1058 - val_loss: 0.1183 - val_mae: 0.2343\n",
      "Epoch 530/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0231 - mae: 0.1068 - val_loss: 0.1182 - val_mae: 0.2352\n",
      "Epoch 531/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0231 - mae: 0.1058 - val_loss: 0.1176 - val_mae: 0.2325\n",
      "Epoch 532/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0232 - mae: 0.1064 - val_loss: 0.1218 - val_mae: 0.2400\n",
      "Epoch 533/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0214 - mae: 0.1027 - val_loss: 0.1203 - val_mae: 0.2375\n",
      "Epoch 534/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0227 - mae: 0.1046 - val_loss: 0.1220 - val_mae: 0.2381\n",
      "Epoch 535/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0233 - mae: 0.1067 - val_loss: 0.1235 - val_mae: 0.2421\n",
      "Epoch 536/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0223 - mae: 0.1040 - val_loss: 0.1216 - val_mae: 0.2386\n",
      "Epoch 537/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0234 - mae: 0.1062 - val_loss: 0.1211 - val_mae: 0.2411\n",
      "Epoch 538/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0228 - mae: 0.1056 - val_loss: 0.1224 - val_mae: 0.2379\n",
      "Epoch 539/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0223 - mae: 0.1033 - val_loss: 0.1219 - val_mae: 0.2394\n",
      "Epoch 540/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0222 - mae: 0.1038 - val_loss: 0.1223 - val_mae: 0.2405\n",
      "Epoch 541/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0220 - mae: 0.1038 - val_loss: 0.1196 - val_mae: 0.2393\n",
      "Epoch 542/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0226 - mae: 0.1053 - val_loss: 0.1205 - val_mae: 0.2378\n",
      "Epoch 543/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0229 - mae: 0.1055 - val_loss: 0.1175 - val_mae: 0.2331\n",
      "Epoch 544/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0230 - mae: 0.1053 - val_loss: 0.1192 - val_mae: 0.2378\n",
      "Epoch 545/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0222 - mae: 0.1038 - val_loss: 0.1185 - val_mae: 0.2374\n",
      "Epoch 546/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0221 - mae: 0.1033 - val_loss: 0.1195 - val_mae: 0.2359\n",
      "Epoch 547/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0227 - mae: 0.1047 - val_loss: 0.1188 - val_mae: 0.2355\n",
      "Epoch 548/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0224 - mae: 0.1036 - val_loss: 0.1211 - val_mae: 0.2422\n",
      "Epoch 549/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0228 - mae: 0.1056 - val_loss: 0.1186 - val_mae: 0.2343\n",
      "Epoch 550/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0225 - mae: 0.1052 - val_loss: 0.1202 - val_mae: 0.2369\n",
      "Epoch 551/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0220 - mae: 0.1027 - val_loss: 0.1244 - val_mae: 0.2422\n",
      "Epoch 552/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0219 - mae: 0.1038 - val_loss: 0.1196 - val_mae: 0.2332\n",
      "Epoch 553/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0225 - mae: 0.1041 - val_loss: 0.1195 - val_mae: 0.2391\n",
      "Epoch 554/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0231 - mae: 0.1057 - val_loss: 0.1196 - val_mae: 0.2360\n",
      "Epoch 555/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0222 - mae: 0.1037 - val_loss: 0.1180 - val_mae: 0.2362\n",
      "Epoch 556/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0226 - mae: 0.1054 - val_loss: 0.1206 - val_mae: 0.2373\n",
      "Epoch 557/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0219 - mae: 0.1031 - val_loss: 0.1214 - val_mae: 0.2397\n",
      "Epoch 558/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0225 - mae: 0.1040 - val_loss: 0.1193 - val_mae: 0.2369\n",
      "Epoch 559/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0216 - mae: 0.1021 - val_loss: 0.1202 - val_mae: 0.2364\n",
      "Epoch 560/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0233 - mae: 0.1066 - val_loss: 0.1225 - val_mae: 0.2418\n",
      "Epoch 561/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0227 - mae: 0.1047 - val_loss: 0.1167 - val_mae: 0.2367\n",
      "Epoch 562/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0216 - mae: 0.1028 - val_loss: 0.1192 - val_mae: 0.2333\n",
      "Epoch 563/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0221 - mae: 0.1040 - val_loss: 0.1180 - val_mae: 0.2341\n",
      "Epoch 564/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0226 - mae: 0.1048 - val_loss: 0.1194 - val_mae: 0.2365\n",
      "Epoch 565/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0223 - mae: 0.1042 - val_loss: 0.1218 - val_mae: 0.2388\n",
      "Epoch 566/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0227 - mae: 0.1047 - val_loss: 0.1187 - val_mae: 0.2343\n",
      "Epoch 567/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0221 - mae: 0.1035 - val_loss: 0.1207 - val_mae: 0.2349\n",
      "Epoch 568/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0219 - mae: 0.1037 - val_loss: 0.1220 - val_mae: 0.2399\n",
      "Epoch 569/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0224 - mae: 0.1046 - val_loss: 0.1202 - val_mae: 0.2372\n",
      "Epoch 570/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0216 - mae: 0.1023 - val_loss: 0.1206 - val_mae: 0.2400\n",
      "Epoch 571/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0221 - mae: 0.1026 - val_loss: 0.1204 - val_mae: 0.2378\n",
      "Epoch 572/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0234 - mae: 0.1068 - val_loss: 0.1184 - val_mae: 0.2355\n",
      "Epoch 573/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0223 - mae: 0.1035 - val_loss: 0.1181 - val_mae: 0.2354\n",
      "Epoch 574/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0221 - mae: 0.1033 - val_loss: 0.1205 - val_mae: 0.2387\n",
      "Epoch 575/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0230 - mae: 0.1058 - val_loss: 0.1202 - val_mae: 0.2366\n",
      "Epoch 576/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0221 - mae: 0.1029 - val_loss: 0.1219 - val_mae: 0.2384\n",
      "Epoch 577/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0226 - mae: 0.1043 - val_loss: 0.1217 - val_mae: 0.2365\n",
      "Epoch 578/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0227 - mae: 0.1042 - val_loss: 0.1226 - val_mae: 0.2363\n",
      "Epoch 579/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0218 - mae: 0.1022 - val_loss: 0.1233 - val_mae: 0.2426\n",
      "Epoch 580/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0222 - mae: 0.1032 - val_loss: 0.1212 - val_mae: 0.2378\n",
      "Epoch 581/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0222 - mae: 0.1033 - val_loss: 0.1186 - val_mae: 0.2341\n",
      "Epoch 582/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0219 - mae: 0.1023 - val_loss: 0.1199 - val_mae: 0.2359\n",
      "Epoch 583/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0223 - mae: 0.1045 - val_loss: 0.1184 - val_mae: 0.2348\n",
      "Epoch 584/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0222 - mae: 0.1034 - val_loss: 0.1182 - val_mae: 0.2352\n",
      "Epoch 585/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0221 - mae: 0.1036 - val_loss: 0.1211 - val_mae: 0.2380\n",
      "Epoch 586/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0217 - mae: 0.1028 - val_loss: 0.1207 - val_mae: 0.2351\n",
      "Epoch 587/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0221 - mae: 0.1033 - val_loss: 0.1192 - val_mae: 0.2356\n",
      "Epoch 588/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0221 - mae: 0.1037 - val_loss: 0.1193 - val_mae: 0.2352\n",
      "Epoch 589/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0219 - mae: 0.1029 - val_loss: 0.1198 - val_mae: 0.2366\n",
      "Epoch 590/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0219 - mae: 0.1033 - val_loss: 0.1191 - val_mae: 0.2338\n",
      "Epoch 591/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0218 - mae: 0.1026 - val_loss: 0.1199 - val_mae: 0.2393\n",
      "Epoch 592/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0215 - mae: 0.1020 - val_loss: 0.1199 - val_mae: 0.2350\n",
      "Epoch 593/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0223 - mae: 0.1036 - val_loss: 0.1190 - val_mae: 0.2342\n",
      "Epoch 594/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0220 - mae: 0.1032 - val_loss: 0.1245 - val_mae: 0.2427\n",
      "Epoch 595/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0227 - mae: 0.1048 - val_loss: 0.1188 - val_mae: 0.2354\n",
      "Epoch 596/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0218 - mae: 0.1033 - val_loss: 0.1218 - val_mae: 0.2409\n",
      "Epoch 597/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0221 - mae: 0.1030 - val_loss: 0.1187 - val_mae: 0.2373\n",
      "Epoch 598/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0221 - mae: 0.1038 - val_loss: 0.1195 - val_mae: 0.2347\n",
      "Epoch 599/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0220 - mae: 0.1029 - val_loss: 0.1206 - val_mae: 0.2388\n",
      "Epoch 600/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0221 - mae: 0.1034 - val_loss: 0.1189 - val_mae: 0.2367\n",
      "Epoch 601/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0219 - mae: 0.1026 - val_loss: 0.1197 - val_mae: 0.2407\n",
      "Epoch 602/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0219 - mae: 0.1032 - val_loss: 0.1186 - val_mae: 0.2370\n",
      "Epoch 603/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0208 - mae: 0.1009 - val_loss: 0.1180 - val_mae: 0.2360\n",
      "Epoch 604/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0220 - mae: 0.1029 - val_loss: 0.1195 - val_mae: 0.2383\n",
      "Epoch 605/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0224 - mae: 0.1042 - val_loss: 0.1211 - val_mae: 0.2370\n",
      "Epoch 606/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0225 - mae: 0.1046 - val_loss: 0.1191 - val_mae: 0.2353\n",
      "Epoch 607/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0225 - mae: 0.1042 - val_loss: 0.1206 - val_mae: 0.2368\n",
      "Epoch 608/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0220 - mae: 0.1038 - val_loss: 0.1193 - val_mae: 0.2356\n",
      "Epoch 609/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0217 - mae: 0.1022 - val_loss: 0.1187 - val_mae: 0.2356\n",
      "Epoch 610/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0217 - mae: 0.1021 - val_loss: 0.1234 - val_mae: 0.2400\n",
      "Epoch 611/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0218 - mae: 0.1025 - val_loss: 0.1192 - val_mae: 0.2376\n",
      "Epoch 612/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0216 - mae: 0.1015 - val_loss: 0.1182 - val_mae: 0.2334\n",
      "Epoch 613/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0213 - mae: 0.1018 - val_loss: 0.1187 - val_mae: 0.2363\n",
      "Epoch 614/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0213 - mae: 0.1015 - val_loss: 0.1200 - val_mae: 0.2343\n",
      "Epoch 615/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0219 - mae: 0.1028 - val_loss: 0.1199 - val_mae: 0.2359\n",
      "Epoch 616/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0217 - mae: 0.1011 - val_loss: 0.1206 - val_mae: 0.2363\n",
      "Epoch 617/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0219 - mae: 0.1038 - val_loss: 0.1195 - val_mae: 0.2361\n",
      "Epoch 618/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0207 - mae: 0.1002 - val_loss: 0.1210 - val_mae: 0.2362\n",
      "Epoch 619/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0216 - mae: 0.1021 - val_loss: 0.1205 - val_mae: 0.2384\n",
      "Epoch 620/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0214 - mae: 0.1021 - val_loss: 0.1195 - val_mae: 0.2378\n",
      "Epoch 621/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0223 - mae: 0.1037 - val_loss: 0.1195 - val_mae: 0.2366\n",
      "Epoch 622/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0221 - mae: 0.1032 - val_loss: 0.1198 - val_mae: 0.2403\n",
      "Epoch 623/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0217 - mae: 0.1024 - val_loss: 0.1190 - val_mae: 0.2379\n",
      "Epoch 624/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0218 - mae: 0.1019 - val_loss: 0.1188 - val_mae: 0.2345\n",
      "Epoch 625/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0223 - mae: 0.1028 - val_loss: 0.1191 - val_mae: 0.2358\n",
      "Epoch 626/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0215 - mae: 0.1019 - val_loss: 0.1203 - val_mae: 0.2363\n",
      "Epoch 633/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0224 - mae: 0.1035 - val_loss: 0.1191 - val_mae: 0.2355\n",
      "Epoch 634/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0216 - mae: 0.1019 - val_loss: 0.1189 - val_mae: 0.2335\n",
      "Epoch 635/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0220 - mae: 0.1028 - val_loss: 0.1205 - val_mae: 0.2394\n",
      "Epoch 636/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0210 - mae: 0.1004 - val_loss: 0.1218 - val_mae: 0.2390\n",
      "Epoch 637/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0214 - mae: 0.1013 - val_loss: 0.1189 - val_mae: 0.2345\n",
      "Epoch 638/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0219 - mae: 0.1029 - val_loss: 0.1195 - val_mae: 0.2364\n",
      "Epoch 639/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0224 - mae: 0.1037 - val_loss: 0.1219 - val_mae: 0.2396\n",
      "Epoch 640/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0220 - mae: 0.1020 - val_loss: 0.1229 - val_mae: 0.2410\n",
      "Epoch 641/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0217 - mae: 0.1022 - val_loss: 0.1197 - val_mae: 0.2359\n",
      "Epoch 642/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0221 - mae: 0.1029 - val_loss: 0.1200 - val_mae: 0.2358\n",
      "Epoch 643/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0208 - mae: 0.0997 - val_loss: 0.1199 - val_mae: 0.2375\n",
      "Epoch 644/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0210 - mae: 0.1006 - val_loss: 0.1200 - val_mae: 0.2382\n",
      "Epoch 645/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0211 - mae: 0.1004 - val_loss: 0.1203 - val_mae: 0.2342\n",
      "Epoch 646/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0214 - mae: 0.1025 - val_loss: 0.1209 - val_mae: 0.2389\n",
      "Epoch 647/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0215 - mae: 0.1016 - val_loss: 0.1201 - val_mae: 0.2398\n",
      "Epoch 648/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0210 - mae: 0.1009 - val_loss: 0.1210 - val_mae: 0.2388\n",
      "Epoch 649/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0213 - mae: 0.1011 - val_loss: 0.1194 - val_mae: 0.2373\n",
      "Epoch 650/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0212 - mae: 0.1014 - val_loss: 0.1211 - val_mae: 0.2362\n",
      "Epoch 651/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0211 - mae: 0.1012 - val_loss: 0.1202 - val_mae: 0.2390\n",
      "Epoch 652/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0211 - mae: 0.1007 - val_loss: 0.1191 - val_mae: 0.2347\n",
      "Epoch 653/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0204 - mae: 0.1000 - val_loss: 0.1181 - val_mae: 0.2359\n",
      "Epoch 654/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0209 - mae: 0.1003 - val_loss: 0.1191 - val_mae: 0.2365\n",
      "Epoch 655/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0216 - mae: 0.1028 - val_loss: 0.1200 - val_mae: 0.2372\n",
      "Epoch 656/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0212 - mae: 0.1007 - val_loss: 0.1231 - val_mae: 0.2421\n",
      "Epoch 657/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0219 - mae: 0.1022 - val_loss: 0.1175 - val_mae: 0.2343\n",
      "Epoch 663/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0212 - mae: 0.1014 - val_loss: 0.1202 - val_mae: 0.2365\n",
      "Epoch 664/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0206 - mae: 0.1005 - val_loss: 0.1207 - val_mae: 0.2379\n",
      "Epoch 665/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0210 - mae: 0.1000 - val_loss: 0.1195 - val_mae: 0.2364\n",
      "Epoch 666/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0219 - mae: 0.1021 - val_loss: 0.1185 - val_mae: 0.2355\n",
      "Epoch 667/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0218 - mae: 0.1016 - val_loss: 0.1245 - val_mae: 0.2443\n",
      "Epoch 668/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0207 - mae: 0.0996 - val_loss: 0.1180 - val_mae: 0.2339\n",
      "Epoch 669/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0204 - mae: 0.0988 - val_loss: 0.1221 - val_mae: 0.2398\n",
      "Epoch 670/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0217 - mae: 0.1024 - val_loss: 0.1183 - val_mae: 0.2335\n",
      "Epoch 671/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0211 - mae: 0.1006 - val_loss: 0.1194 - val_mae: 0.2387\n",
      "Epoch 672/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0212 - mae: 0.1004 - val_loss: 0.1208 - val_mae: 0.2360\n",
      "Epoch 673/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0214 - mae: 0.1013 - val_loss: 0.1204 - val_mae: 0.2365\n",
      "Epoch 674/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0212 - mae: 0.1003 - val_loss: 0.1204 - val_mae: 0.2366\n",
      "Epoch 675/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0208 - mae: 0.1000 - val_loss: 0.1201 - val_mae: 0.2380\n",
      "Epoch 676/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0207 - mae: 0.0998 - val_loss: 0.1202 - val_mae: 0.2372\n",
      "Epoch 677/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0202 - mae: 0.0984 - val_loss: 0.1193 - val_mae: 0.2342\n",
      "Epoch 678/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0215 - mae: 0.1019 - val_loss: 0.1215 - val_mae: 0.2396\n",
      "Epoch 679/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0198 - mae: 0.0968 - val_loss: 0.1184 - val_mae: 0.2346\n",
      "Epoch 680/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0215 - mae: 0.1015 - val_loss: 0.1193 - val_mae: 0.2373\n",
      "Epoch 681/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0208 - mae: 0.0998 - val_loss: 0.1191 - val_mae: 0.2353\n",
      "Epoch 682/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0209 - mae: 0.1005 - val_loss: 0.1219 - val_mae: 0.2366\n",
      "Epoch 683/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0207 - mae: 0.0994 - val_loss: 0.1201 - val_mae: 0.2351\n",
      "Epoch 684/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0210 - mae: 0.0995 - val_loss: 0.1211 - val_mae: 0.2372\n",
      "Epoch 685/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0207 - mae: 0.0996 - val_loss: 0.1203 - val_mae: 0.2369\n",
      "Epoch 686/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0218 - mae: 0.1020 - val_loss: 0.1187 - val_mae: 0.2359\n",
      "Epoch 687/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0206 - mae: 0.0988 - val_loss: 0.1185 - val_mae: 0.2346\n",
      "Epoch 688/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0211 - mae: 0.1011 - val_loss: 0.1179 - val_mae: 0.2336\n",
      "Epoch 689/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0985 - val_loss: 0.1185 - val_mae: 0.2356\n",
      "Epoch 690/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0209 - mae: 0.0997 - val_loss: 0.1202 - val_mae: 0.2366\n",
      "Epoch 692/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0217 - mae: 0.1010 - val_loss: 0.1216 - val_mae: 0.2388\n",
      "Epoch 693/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0206 - mae: 0.0981 - val_loss: 0.1185 - val_mae: 0.2337\n",
      "Epoch 694/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0204 - mae: 0.0986 - val_loss: 0.1211 - val_mae: 0.2388\n",
      "Epoch 695/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0202 - mae: 0.0986 - val_loss: 0.1214 - val_mae: 0.2392\n",
      "Epoch 696/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0208 - mae: 0.0996 - val_loss: 0.1201 - val_mae: 0.2388\n",
      "Epoch 697/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0205 - mae: 0.0984 - val_loss: 0.1186 - val_mae: 0.2356\n",
      "Epoch 698/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0209 - mae: 0.1006 - val_loss: 0.1206 - val_mae: 0.2357\n",
      "Epoch 699/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0215 - mae: 0.1008 - val_loss: 0.1210 - val_mae: 0.2371\n",
      "Epoch 700/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0210 - mae: 0.1000 - val_loss: 0.1174 - val_mae: 0.2347\n",
      "Epoch 701/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0202 - mae: 0.0980 - val_loss: 0.1194 - val_mae: 0.2369\n",
      "Epoch 702/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0202 - mae: 0.0984 - val_loss: 0.1188 - val_mae: 0.2347\n",
      "Epoch 703/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0205 - mae: 0.0986 - val_loss: 0.1190 - val_mae: 0.2359\n",
      "Epoch 704/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0212 - mae: 0.0996 - val_loss: 0.1191 - val_mae: 0.2383\n",
      "Epoch 705/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0206 - mae: 0.0993 - val_loss: 0.1198 - val_mae: 0.2383\n",
      "Epoch 706/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0209 - mae: 0.1003 - val_loss: 0.1202 - val_mae: 0.2362\n",
      "Epoch 707/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0202 - mae: 0.0978 - val_loss: 0.1219 - val_mae: 0.2376\n",
      "Epoch 708/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0202 - mae: 0.0987 - val_loss: 0.1199 - val_mae: 0.2345\n",
      "Epoch 709/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0209 - mae: 0.0997 - val_loss: 0.1208 - val_mae: 0.2387\n",
      "Epoch 710/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0209 - mae: 0.1000 - val_loss: 0.1213 - val_mae: 0.2356\n",
      "Epoch 711/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0209 - mae: 0.0995 - val_loss: 0.1210 - val_mae: 0.2365\n",
      "Epoch 712/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0205 - mae: 0.0988 - val_loss: 0.1195 - val_mae: 0.2358\n",
      "Epoch 713/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0211 - mae: 0.0995 - val_loss: 0.1187 - val_mae: 0.2345\n",
      "Epoch 714/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0981 - val_loss: 0.1189 - val_mae: 0.2346\n",
      "Epoch 715/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0207 - mae: 0.0994 - val_loss: 0.1194 - val_mae: 0.2366\n",
      "Epoch 716/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0210 - mae: 0.0998 - val_loss: 0.1183 - val_mae: 0.2362\n",
      "Epoch 717/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0981 - val_loss: 0.1189 - val_mae: 0.2345\n",
      "Epoch 718/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0214 - mae: 0.1006 - val_loss: 0.1213 - val_mae: 0.2392\n",
      "Epoch 719/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0205 - mae: 0.0989 - val_loss: 0.1201 - val_mae: 0.2360\n",
      "Epoch 720/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0205 - mae: 0.0985 - val_loss: 0.1182 - val_mae: 0.2335\n",
      "Epoch 721/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0208 - mae: 0.0999 - val_loss: 0.1205 - val_mae: 0.2387\n",
      "Epoch 722/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0208 - mae: 0.0989 - val_loss: 0.1199 - val_mae: 0.2363\n",
      "Epoch 723/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0206 - mae: 0.0990 - val_loss: 0.1202 - val_mae: 0.2355\n",
      "Epoch 724/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0213 - mae: 0.1012 - val_loss: 0.1181 - val_mae: 0.2354\n",
      "Epoch 725/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0202 - mae: 0.0985 - val_loss: 0.1203 - val_mae: 0.2362\n",
      "Epoch 726/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0202 - mae: 0.0978 - val_loss: 0.1232 - val_mae: 0.2416\n",
      "Epoch 727/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0975 - val_loss: 0.1198 - val_mae: 0.2350\n",
      "Epoch 728/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0987 - val_loss: 0.1180 - val_mae: 0.2353\n",
      "Epoch 729/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0989 - val_loss: 0.1197 - val_mae: 0.2343\n",
      "Epoch 730/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0975 - val_loss: 0.1213 - val_mae: 0.2361\n",
      "Epoch 731/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0209 - mae: 0.1004 - val_loss: 0.1207 - val_mae: 0.2382\n",
      "Epoch 732/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0207 - mae: 0.0996 - val_loss: 0.1207 - val_mae: 0.2399\n",
      "Epoch 733/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0985 - val_loss: 0.1199 - val_mae: 0.2359\n",
      "Epoch 734/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0206 - mae: 0.0989 - val_loss: 0.1200 - val_mae: 0.2376\n",
      "Epoch 735/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0208 - mae: 0.0998 - val_loss: 0.1205 - val_mae: 0.2348\n",
      "Epoch 736/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0978 - val_loss: 0.1210 - val_mae: 0.2354\n",
      "Epoch 737/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0215 - mae: 0.1012 - val_loss: 0.1185 - val_mae: 0.2331\n",
      "Epoch 738/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0204 - mae: 0.0981 - val_loss: 0.1196 - val_mae: 0.2330\n",
      "Epoch 739/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0208 - mae: 0.0994 - val_loss: 0.1200 - val_mae: 0.2382\n",
      "Epoch 740/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0984 - val_loss: 0.1208 - val_mae: 0.2373\n",
      "Epoch 741/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0206 - mae: 0.0986 - val_loss: 0.1205 - val_mae: 0.2388\n",
      "Epoch 742/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0211 - mae: 0.0994 - val_loss: 0.1202 - val_mae: 0.2363\n",
      "Epoch 743/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0209 - mae: 0.0995 - val_loss: 0.1189 - val_mae: 0.2342\n",
      "Epoch 744/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0987 - val_loss: 0.1183 - val_mae: 0.2352\n",
      "Epoch 745/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0207 - mae: 0.0992 - val_loss: 0.1199 - val_mae: 0.2343\n",
      "Epoch 746/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0202 - mae: 0.0980 - val_loss: 0.1191 - val_mae: 0.2357\n",
      "Epoch 747/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0215 - mae: 0.1012 - val_loss: 0.1212 - val_mae: 0.2390\n",
      "Epoch 748/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0207 - mae: 0.0989 - val_loss: 0.1205 - val_mae: 0.2357\n",
      "Epoch 749/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0202 - mae: 0.0972 - val_loss: 0.1202 - val_mae: 0.2359\n",
      "Epoch 750/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0205 - mae: 0.0988 - val_loss: 0.1217 - val_mae: 0.2393\n",
      "Epoch 751/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0983 - val_loss: 0.1217 - val_mae: 0.2368\n",
      "Epoch 752/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0209 - mae: 0.0999 - val_loss: 0.1207 - val_mae: 0.2362\n",
      "Epoch 753/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0211 - mae: 0.0997 - val_loss: 0.1194 - val_mae: 0.2348\n",
      "Epoch 754/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0212 - mae: 0.1000 - val_loss: 0.1191 - val_mae: 0.2359\n",
      "Epoch 755/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0207 - mae: 0.0990 - val_loss: 0.1209 - val_mae: 0.2366\n",
      "Epoch 756/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0971 - val_loss: 0.1193 - val_mae: 0.2360\n",
      "Epoch 757/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0204 - mae: 0.0986 - val_loss: 0.1199 - val_mae: 0.2341\n",
      "Epoch 758/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0981 - val_loss: 0.1202 - val_mae: 0.2357\n",
      "Epoch 759/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0978 - val_loss: 0.1204 - val_mae: 0.2348\n",
      "Epoch 760/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0971 - val_loss: 0.1215 - val_mae: 0.2385\n",
      "Epoch 761/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0979 - val_loss: 0.1184 - val_mae: 0.2346\n",
      "Epoch 762/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0976 - val_loss: 0.1194 - val_mae: 0.2370\n",
      "Epoch 763/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0966 - val_loss: 0.1182 - val_mae: 0.2358\n",
      "Epoch 764/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0213 - mae: 0.1010 - val_loss: 0.1198 - val_mae: 0.2361\n",
      "Epoch 765/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0204 - mae: 0.0981 - val_loss: 0.1204 - val_mae: 0.2347\n",
      "Epoch 766/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0981 - val_loss: 0.1198 - val_mae: 0.2371\n",
      "Epoch 767/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0196 - mae: 0.0969 - val_loss: 0.1187 - val_mae: 0.2383\n",
      "Epoch 768/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0212 - mae: 0.0995 - val_loss: 0.1199 - val_mae: 0.2342\n",
      "Epoch 769/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0973 - val_loss: 0.1212 - val_mae: 0.2395\n",
      "Epoch 770/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0198 - mae: 0.0971 - val_loss: 0.1204 - val_mae: 0.2372\n",
      "Epoch 771/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0205 - mae: 0.0989 - val_loss: 0.1180 - val_mae: 0.2357\n",
      "Epoch 772/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0210 - mae: 0.0998 - val_loss: 0.1196 - val_mae: 0.2341\n",
      "Epoch 773/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0211 - mae: 0.0999 - val_loss: 0.1207 - val_mae: 0.2377\n",
      "Epoch 774/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0973 - val_loss: 0.1198 - val_mae: 0.2358\n",
      "Epoch 775/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0205 - mae: 0.0988 - val_loss: 0.1211 - val_mae: 0.2402\n",
      "Epoch 776/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0973 - val_loss: 0.1186 - val_mae: 0.2337\n",
      "Epoch 777/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0973 - val_loss: 0.1199 - val_mae: 0.2366\n",
      "Epoch 778/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0210 - mae: 0.0993 - val_loss: 0.1192 - val_mae: 0.2356\n",
      "Epoch 779/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0980 - val_loss: 0.1197 - val_mae: 0.2334\n",
      "Epoch 780/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0984 - val_loss: 0.1208 - val_mae: 0.2352\n",
      "Epoch 781/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0198 - mae: 0.0973 - val_loss: 0.1181 - val_mae: 0.2338\n",
      "Epoch 782/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0972 - val_loss: 0.1182 - val_mae: 0.2332\n",
      "Epoch 783/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0974 - val_loss: 0.1188 - val_mae: 0.2356\n",
      "Epoch 784/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0214 - mae: 0.1001 - val_loss: 0.1197 - val_mae: 0.2361\n",
      "Epoch 785/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0202 - mae: 0.0983 - val_loss: 0.1215 - val_mae: 0.2408\n",
      "Epoch 786/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0198 - mae: 0.0962 - val_loss: 0.1244 - val_mae: 0.2401\n",
      "Epoch 787/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0202 - mae: 0.0973 - val_loss: 0.1192 - val_mae: 0.2349\n",
      "Epoch 788/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0195 - mae: 0.0960 - val_loss: 0.1212 - val_mae: 0.2380\n",
      "Epoch 789/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0205 - mae: 0.0979 - val_loss: 0.1205 - val_mae: 0.2373\n",
      "Epoch 790/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0974 - val_loss: 0.1205 - val_mae: 0.2377\n",
      "Epoch 791/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0976 - val_loss: 0.1204 - val_mae: 0.2360\n",
      "Epoch 792/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0977 - val_loss: 0.1247 - val_mae: 0.2434\n",
      "Epoch 793/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0207 - mae: 0.0985 - val_loss: 0.1193 - val_mae: 0.2354\n",
      "Epoch 794/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0207 - mae: 0.0993 - val_loss: 0.1194 - val_mae: 0.2372\n",
      "Epoch 795/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0976 - val_loss: 0.1195 - val_mae: 0.2355\n",
      "Epoch 796/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0207 - mae: 0.0995 - val_loss: 0.1180 - val_mae: 0.2360\n",
      "Epoch 797/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0977 - val_loss: 0.1214 - val_mae: 0.2386\n",
      "Epoch 798/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0207 - mae: 0.0986 - val_loss: 0.1196 - val_mae: 0.2356\n",
      "Epoch 799/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0970 - val_loss: 0.1211 - val_mae: 0.2373\n",
      "Epoch 800/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0208 - mae: 0.0986 - val_loss: 0.1209 - val_mae: 0.2385\n",
      "Epoch 801/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0205 - mae: 0.0979 - val_loss: 0.1193 - val_mae: 0.2346\n",
      "Epoch 802/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0979 - val_loss: 0.1212 - val_mae: 0.2383\n",
      "Epoch 803/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0980 - val_loss: 0.1189 - val_mae: 0.2342\n",
      "Epoch 804/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0209 - mae: 0.0989 - val_loss: 0.1207 - val_mae: 0.2354\n",
      "Epoch 805/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0972 - val_loss: 0.1189 - val_mae: 0.2345\n",
      "Epoch 806/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0956 - val_loss: 0.1182 - val_mae: 0.2358\n",
      "Epoch 807/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0979 - val_loss: 0.1200 - val_mae: 0.2342\n",
      "Epoch 808/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0978 - val_loss: 0.1199 - val_mae: 0.2386\n",
      "Epoch 809/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0967 - val_loss: 0.1195 - val_mae: 0.2345\n",
      "Epoch 810/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0196 - mae: 0.0964 - val_loss: 0.1202 - val_mae: 0.2362\n",
      "Epoch 811/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0194 - mae: 0.0958 - val_loss: 0.1214 - val_mae: 0.2381\n",
      "Epoch 812/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0204 - mae: 0.0984 - val_loss: 0.1235 - val_mae: 0.2415\n",
      "Epoch 813/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0976 - val_loss: 0.1209 - val_mae: 0.2384\n",
      "Epoch 814/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0198 - mae: 0.0970 - val_loss: 0.1183 - val_mae: 0.2365\n",
      "Epoch 815/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0985 - val_loss: 0.1237 - val_mae: 0.2414\n",
      "Epoch 816/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0982 - val_loss: 0.1197 - val_mae: 0.2365\n",
      "Epoch 817/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0204 - mae: 0.0971 - val_loss: 0.1190 - val_mae: 0.2341\n",
      "Epoch 818/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0976 - val_loss: 0.1205 - val_mae: 0.2368\n",
      "Epoch 819/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0961 - val_loss: 0.1210 - val_mae: 0.2359\n",
      "Epoch 820/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0966 - val_loss: 0.1209 - val_mae: 0.2361\n",
      "Epoch 821/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0202 - mae: 0.0972 - val_loss: 0.1199 - val_mae: 0.2352\n",
      "Epoch 822/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0961 - val_loss: 0.1209 - val_mae: 0.2372\n",
      "Epoch 823/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0973 - val_loss: 0.1192 - val_mae: 0.2352\n",
      "Epoch 824/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0198 - mae: 0.0962 - val_loss: 0.1183 - val_mae: 0.2346\n",
      "Epoch 825/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0968 - val_loss: 0.1203 - val_mae: 0.2371\n",
      "Epoch 826/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0197 - mae: 0.0959 - val_loss: 0.1215 - val_mae: 0.2380\n",
      "Epoch 827/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0972 - val_loss: 0.1211 - val_mae: 0.2369\n",
      "Epoch 828/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0196 - mae: 0.0966 - val_loss: 0.1199 - val_mae: 0.2366\n",
      "Epoch 829/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0981 - val_loss: 0.1185 - val_mae: 0.2349\n",
      "Epoch 830/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0206 - mae: 0.0975 - val_loss: 0.1210 - val_mae: 0.2364\n",
      "Epoch 831/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0196 - mae: 0.0961 - val_loss: 0.1199 - val_mae: 0.2353\n",
      "Epoch 832/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0975 - val_loss: 0.1221 - val_mae: 0.2362\n",
      "Epoch 833/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0188 - mae: 0.0942 - val_loss: 0.1197 - val_mae: 0.2360\n",
      "Epoch 834/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0195 - mae: 0.0962 - val_loss: 0.1195 - val_mae: 0.2356\n",
      "Epoch 835/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0197 - mae: 0.0967 - val_loss: 0.1178 - val_mae: 0.2343\n",
      "Epoch 836/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0980 - val_loss: 0.1220 - val_mae: 0.2379\n",
      "Epoch 837/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0980 - val_loss: 0.1197 - val_mae: 0.2357\n",
      "Epoch 838/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0968 - val_loss: 0.1218 - val_mae: 0.2386\n",
      "Epoch 839/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0977 - val_loss: 0.1213 - val_mae: 0.2363\n",
      "Epoch 840/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0971 - val_loss: 0.1225 - val_mae: 0.2372\n",
      "Epoch 841/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0202 - mae: 0.0977 - val_loss: 0.1215 - val_mae: 0.2401\n",
      "Epoch 842/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0198 - mae: 0.0964 - val_loss: 0.1218 - val_mae: 0.2378\n",
      "Epoch 843/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0196 - mae: 0.0962 - val_loss: 0.1222 - val_mae: 0.2382\n",
      "Epoch 844/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0955 - val_loss: 0.1217 - val_mae: 0.2379\n",
      "Epoch 845/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0195 - mae: 0.0957 - val_loss: 0.1215 - val_mae: 0.2392\n",
      "Epoch 846/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0197 - mae: 0.0963 - val_loss: 0.1190 - val_mae: 0.2329\n",
      "Epoch 847/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0195 - mae: 0.0962 - val_loss: 0.1178 - val_mae: 0.2346\n",
      "Epoch 848/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0197 - mae: 0.0965 - val_loss: 0.1212 - val_mae: 0.2369\n",
      "Epoch 849/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0197 - mae: 0.0965 - val_loss: 0.1216 - val_mae: 0.2370\n",
      "Epoch 850/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0196 - mae: 0.0961 - val_loss: 0.1213 - val_mae: 0.2405\n",
      "Epoch 851/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0972 - val_loss: 0.1234 - val_mae: 0.2365\n",
      "Epoch 852/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0960 - val_loss: 0.1193 - val_mae: 0.2347\n",
      "Epoch 853/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0196 - mae: 0.0955 - val_loss: 0.1209 - val_mae: 0.2347\n",
      "Epoch 854/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0969 - val_loss: 0.1211 - val_mae: 0.2370\n",
      "Epoch 855/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0951 - val_loss: 0.1201 - val_mae: 0.2362\n",
      "Epoch 856/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0976 - val_loss: 0.1207 - val_mae: 0.2376\n",
      "Epoch 857/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0192 - mae: 0.0952 - val_loss: 0.1212 - val_mae: 0.2380\n",
      "Epoch 858/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0205 - mae: 0.0985 - val_loss: 0.1215 - val_mae: 0.2384\n",
      "Epoch 859/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0196 - mae: 0.0963 - val_loss: 0.1193 - val_mae: 0.2369\n",
      "Epoch 860/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0964 - val_loss: 0.1215 - val_mae: 0.2378\n",
      "Epoch 861/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0189 - mae: 0.0945 - val_loss: 0.1191 - val_mae: 0.2328\n",
      "Epoch 862/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0191 - mae: 0.0939 - val_loss: 0.1195 - val_mae: 0.2339\n",
      "Epoch 863/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0196 - mae: 0.0959 - val_loss: 0.1203 - val_mae: 0.2368\n",
      "Epoch 864/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0192 - mae: 0.0950 - val_loss: 0.1209 - val_mae: 0.2369\n",
      "Epoch 865/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0976 - val_loss: 0.1202 - val_mae: 0.2349\n",
      "Epoch 866/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0197 - mae: 0.0963 - val_loss: 0.1207 - val_mae: 0.2374\n",
      "Epoch 867/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0966 - val_loss: 0.1208 - val_mae: 0.2385\n",
      "Epoch 868/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0972 - val_loss: 0.1213 - val_mae: 0.2366\n",
      "Epoch 869/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0197 - mae: 0.0964 - val_loss: 0.1197 - val_mae: 0.2351\n",
      "Epoch 870/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0954 - val_loss: 0.1204 - val_mae: 0.2353\n",
      "Epoch 871/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0965 - val_loss: 0.1196 - val_mae: 0.2365\n",
      "Epoch 872/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0195 - mae: 0.0956 - val_loss: 0.1194 - val_mae: 0.2353\n",
      "Epoch 873/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0196 - mae: 0.0954 - val_loss: 0.1222 - val_mae: 0.2364\n",
      "Epoch 874/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0191 - mae: 0.0946 - val_loss: 0.1185 - val_mae: 0.2340\n",
      "Epoch 875/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0187 - mae: 0.0942 - val_loss: 0.1194 - val_mae: 0.2348\n",
      "Epoch 876/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0196 - mae: 0.0959 - val_loss: 0.1183 - val_mae: 0.2352\n",
      "Epoch 877/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0198 - mae: 0.0960 - val_loss: 0.1200 - val_mae: 0.2371\n",
      "Epoch 878/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0964 - val_loss: 0.1191 - val_mae: 0.2363\n",
      "Epoch 879/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0197 - mae: 0.0961 - val_loss: 0.1207 - val_mae: 0.2363\n",
      "Epoch 880/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0980 - val_loss: 0.1217 - val_mae: 0.2376\n",
      "Epoch 881/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0188 - mae: 0.0944 - val_loss: 0.1211 - val_mae: 0.2378\n",
      "Epoch 882/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0201 - mae: 0.0971 - val_loss: 0.1201 - val_mae: 0.2345\n",
      "Epoch 883/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0198 - mae: 0.0963 - val_loss: 0.1196 - val_mae: 0.2365\n",
      "Epoch 884/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0190 - mae: 0.0952 - val_loss: 0.1207 - val_mae: 0.2376\n",
      "Epoch 885/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0953 - val_loss: 0.1217 - val_mae: 0.2365\n",
      "Epoch 886/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0188 - mae: 0.0939 - val_loss: 0.1191 - val_mae: 0.2339\n",
      "Epoch 887/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0972 - val_loss: 0.1196 - val_mae: 0.2328\n",
      "Epoch 888/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0964 - val_loss: 0.1193 - val_mae: 0.2337\n",
      "Epoch 889/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0196 - mae: 0.0957 - val_loss: 0.1210 - val_mae: 0.2369\n",
      "Epoch 890/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0969 - val_loss: 0.1199 - val_mae: 0.2348\n",
      "Epoch 891/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0197 - mae: 0.0959 - val_loss: 0.1202 - val_mae: 0.2363\n",
      "Epoch 892/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0196 - mae: 0.0951 - val_loss: 0.1221 - val_mae: 0.2382\n",
      "Epoch 893/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0956 - val_loss: 0.1209 - val_mae: 0.2361\n",
      "Epoch 894/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0194 - mae: 0.0955 - val_loss: 0.1204 - val_mae: 0.2374\n",
      "Epoch 895/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0195 - mae: 0.0957 - val_loss: 0.1208 - val_mae: 0.2367\n",
      "Epoch 896/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0944 - val_loss: 0.1214 - val_mae: 0.2355\n",
      "Epoch 897/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0187 - mae: 0.0941 - val_loss: 0.1213 - val_mae: 0.2375\n",
      "Epoch 898/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0196 - mae: 0.0958 - val_loss: 0.1201 - val_mae: 0.2373\n",
      "Epoch 899/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0196 - mae: 0.0958 - val_loss: 0.1193 - val_mae: 0.2348\n",
      "Epoch 900/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0192 - mae: 0.0942 - val_loss: 0.1189 - val_mae: 0.2360\n",
      "Epoch 901/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0185 - mae: 0.0940 - val_loss: 0.1207 - val_mae: 0.2376\n",
      "Epoch 902/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0191 - mae: 0.0948 - val_loss: 0.1213 - val_mae: 0.2371\n",
      "Epoch 903/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0970 - val_loss: 0.1192 - val_mae: 0.2323\n",
      "Epoch 904/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0950 - val_loss: 0.1189 - val_mae: 0.2340\n",
      "Epoch 905/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0942 - val_loss: 0.1201 - val_mae: 0.2363\n",
      "Epoch 906/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0197 - mae: 0.0959 - val_loss: 0.1195 - val_mae: 0.2374\n",
      "Epoch 907/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0197 - mae: 0.0962 - val_loss: 0.1196 - val_mae: 0.2353\n",
      "Epoch 908/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0196 - mae: 0.0955 - val_loss: 0.1207 - val_mae: 0.2374\n",
      "Epoch 909/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0194 - mae: 0.0952 - val_loss: 0.1197 - val_mae: 0.2359\n",
      "Epoch 910/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0192 - mae: 0.0948 - val_loss: 0.1212 - val_mae: 0.2343\n",
      "Epoch 911/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0198 - mae: 0.0954 - val_loss: 0.1197 - val_mae: 0.2354\n",
      "Epoch 912/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0194 - mae: 0.0952 - val_loss: 0.1195 - val_mae: 0.2345\n",
      "Epoch 913/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0187 - mae: 0.0938 - val_loss: 0.1200 - val_mae: 0.2385\n",
      "Epoch 914/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0950 - val_loss: 0.1212 - val_mae: 0.2386\n",
      "Epoch 915/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0198 - mae: 0.0961 - val_loss: 0.1198 - val_mae: 0.2363\n",
      "Epoch 916/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0203 - mae: 0.0975 - val_loss: 0.1220 - val_mae: 0.2369\n",
      "Epoch 917/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0189 - mae: 0.0941 - val_loss: 0.1191 - val_mae: 0.2349\n",
      "Epoch 918/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0197 - mae: 0.0956 - val_loss: 0.1180 - val_mae: 0.2335\n",
      "Epoch 919/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0954 - val_loss: 0.1233 - val_mae: 0.2436\n",
      "Epoch 920/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0200 - mae: 0.0969 - val_loss: 0.1207 - val_mae: 0.2364\n",
      "Epoch 921/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0958 - val_loss: 0.1196 - val_mae: 0.2337\n",
      "Epoch 922/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0198 - mae: 0.0963 - val_loss: 0.1197 - val_mae: 0.2372\n",
      "Epoch 923/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0190 - mae: 0.0945 - val_loss: 0.1224 - val_mae: 0.2395\n",
      "Epoch 924/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0192 - mae: 0.0948 - val_loss: 0.1200 - val_mae: 0.2383\n",
      "Epoch 925/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0957 - val_loss: 0.1190 - val_mae: 0.2342\n",
      "Epoch 926/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0187 - mae: 0.0936 - val_loss: 0.1202 - val_mae: 0.2381\n",
      "Epoch 927/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0195 - mae: 0.0953 - val_loss: 0.1207 - val_mae: 0.2389\n",
      "Epoch 928/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0188 - mae: 0.0942 - val_loss: 0.1200 - val_mae: 0.2355\n",
      "Epoch 929/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0202 - mae: 0.0965 - val_loss: 0.1185 - val_mae: 0.2357\n",
      "Epoch 930/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0189 - mae: 0.0942 - val_loss: 0.1204 - val_mae: 0.2371\n",
      "Epoch 931/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0197 - mae: 0.0952 - val_loss: 0.1211 - val_mae: 0.2366\n",
      "Epoch 932/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0203 - mae: 0.0980 - val_loss: 0.1213 - val_mae: 0.2402\n",
      "Epoch 933/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0190 - mae: 0.0936 - val_loss: 0.1208 - val_mae: 0.2376\n",
      "Epoch 934/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0184 - mae: 0.0928 - val_loss: 0.1210 - val_mae: 0.2394\n",
      "Epoch 935/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0963 - val_loss: 0.1212 - val_mae: 0.2361\n",
      "Epoch 936/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0194 - mae: 0.0950 - val_loss: 0.1213 - val_mae: 0.2390\n",
      "Epoch 985/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0181 - mae: 0.0924 - val_loss: 0.1203 - val_mae: 0.2380\n",
      "Epoch 986/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0185 - mae: 0.0929 - val_loss: 0.1201 - val_mae: 0.2380\n",
      "Epoch 987/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0189 - mae: 0.0941 - val_loss: 0.1213 - val_mae: 0.2398\n",
      "Epoch 988/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0190 - mae: 0.0939 - val_loss: 0.1204 - val_mae: 0.2370\n",
      "Epoch 989/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0922 - val_loss: 0.1209 - val_mae: 0.2378\n",
      "Epoch 990/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0188 - mae: 0.0930 - val_loss: 0.1219 - val_mae: 0.2376\n",
      "Epoch 991/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0192 - mae: 0.0944 - val_loss: 0.1204 - val_mae: 0.2391\n",
      "Epoch 992/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0195 - mae: 0.0950 - val_loss: 0.1201 - val_mae: 0.2357\n",
      "Epoch 993/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0185 - mae: 0.0932 - val_loss: 0.1206 - val_mae: 0.2366\n",
      "Epoch 994/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0923 - val_loss: 0.1208 - val_mae: 0.2378\n",
      "Epoch 995/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0189 - mae: 0.0937 - val_loss: 0.1198 - val_mae: 0.2368\n",
      "Epoch 996/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0190 - mae: 0.0940 - val_loss: 0.1215 - val_mae: 0.2359\n",
      "Epoch 997/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0187 - mae: 0.0937 - val_loss: 0.1219 - val_mae: 0.2388\n",
      "Epoch 998/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0948 - val_loss: 0.1196 - val_mae: 0.2356\n",
      "Epoch 999/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0189 - mae: 0.0940 - val_loss: 0.1217 - val_mae: 0.2385\n",
      "Epoch 1000/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0925 - val_loss: 0.1195 - val_mae: 0.2350\n",
      "Epoch 1001/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0188 - mae: 0.0929 - val_loss: 0.1217 - val_mae: 0.2382\n",
      "Epoch 1002/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0194 - mae: 0.0944 - val_loss: 0.1201 - val_mae: 0.2367\n",
      "Epoch 1003/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0945 - val_loss: 0.1228 - val_mae: 0.2388\n",
      "Epoch 1004/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0189 - mae: 0.0938 - val_loss: 0.1210 - val_mae: 0.2397\n",
      "Epoch 1005/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0199 - mae: 0.0956 - val_loss: 0.1195 - val_mae: 0.2352\n",
      "Epoch 1006/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0185 - mae: 0.0926 - val_loss: 0.1211 - val_mae: 0.2350\n",
      "Epoch 1007/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0190 - mae: 0.0945 - val_loss: 0.1211 - val_mae: 0.2402\n",
      "Epoch 1008/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0187 - mae: 0.0934 - val_loss: 0.1217 - val_mae: 0.2395\n",
      "Epoch 1020/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0187 - mae: 0.0933 - val_loss: 0.1209 - val_mae: 0.2368\n",
      "Epoch 1021/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0189 - mae: 0.0937 - val_loss: 0.1210 - val_mae: 0.2384\n",
      "Epoch 1022/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0181 - mae: 0.0922 - val_loss: 0.1221 - val_mae: 0.2379\n",
      "Epoch 1023/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0180 - mae: 0.0922 - val_loss: 0.1202 - val_mae: 0.2340\n",
      "Epoch 1024/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0906 - val_loss: 0.1200 - val_mae: 0.2354\n",
      "Epoch 1025/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0193 - mae: 0.0946 - val_loss: 0.1208 - val_mae: 0.2373\n",
      "Epoch 1026/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0189 - mae: 0.0947 - val_loss: 0.1181 - val_mae: 0.2338\n",
      "Epoch 1027/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0186 - mae: 0.0930 - val_loss: 0.1206 - val_mae: 0.2369\n",
      "Epoch 1028/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0192 - mae: 0.0948 - val_loss: 0.1213 - val_mae: 0.2373\n",
      "Epoch 1029/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0190 - mae: 0.0927 - val_loss: 0.1234 - val_mae: 0.2415\n",
      "Epoch 1030/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0185 - mae: 0.0922 - val_loss: 0.1221 - val_mae: 0.2362\n",
      "Epoch 1031/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0198 - mae: 0.0959 - val_loss: 0.1211 - val_mae: 0.2379\n",
      "Epoch 1032/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0194 - mae: 0.0940 - val_loss: 0.1205 - val_mae: 0.2375\n",
      "Epoch 1033/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0186 - mae: 0.0928 - val_loss: 0.1190 - val_mae: 0.2351\n",
      "Epoch 1034/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0185 - mae: 0.0928 - val_loss: 0.1207 - val_mae: 0.2360\n",
      "Epoch 1035/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0185 - mae: 0.0932 - val_loss: 0.1201 - val_mae: 0.2371\n",
      "Epoch 1036/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0903 - val_loss: 0.1192 - val_mae: 0.2394\n",
      "Epoch 1037/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0183 - mae: 0.0922 - val_loss: 0.1189 - val_mae: 0.2341\n",
      "Epoch 1038/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0190 - mae: 0.0936 - val_loss: 0.1207 - val_mae: 0.2368\n",
      "Epoch 1039/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0183 - mae: 0.0927 - val_loss: 0.1199 - val_mae: 0.2372\n",
      "Epoch 1040/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0183 - mae: 0.0930 - val_loss: 0.1214 - val_mae: 0.2365\n",
      "Epoch 1041/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0195 - mae: 0.0951 - val_loss: 0.1205 - val_mae: 0.2380\n",
      "Epoch 1042/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0188 - mae: 0.0935 - val_loss: 0.1234 - val_mae: 0.2386\n",
      "Epoch 1043/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0191 - mae: 0.0939 - val_loss: 0.1211 - val_mae: 0.2374\n",
      "Epoch 1044/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0910 - val_loss: 0.1212 - val_mae: 0.2375\n",
      "Epoch 1045/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0183 - mae: 0.0927 - val_loss: 0.1189 - val_mae: 0.2319\n",
      "Epoch 1046/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0187 - mae: 0.0934 - val_loss: 0.1200 - val_mae: 0.2387\n",
      "Epoch 1047/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0192 - mae: 0.0945 - val_loss: 0.1195 - val_mae: 0.2357\n",
      "Epoch 1048/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0188 - mae: 0.0935 - val_loss: 0.1207 - val_mae: 0.2352\n",
      "Epoch 1055/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0188 - mae: 0.0941 - val_loss: 0.1200 - val_mae: 0.2363\n",
      "Epoch 1056/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0192 - mae: 0.0942 - val_loss: 0.1211 - val_mae: 0.2366\n",
      "Epoch 1057/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0188 - mae: 0.0934 - val_loss: 0.1229 - val_mae: 0.2378\n",
      "Epoch 1058/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0187 - mae: 0.0929 - val_loss: 0.1204 - val_mae: 0.2359\n",
      "Epoch 1059/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0189 - mae: 0.0936 - val_loss: 0.1203 - val_mae: 0.2361\n",
      "Epoch 1060/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0180 - mae: 0.0913 - val_loss: 0.1206 - val_mae: 0.2354\n",
      "Epoch 1061/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0183 - mae: 0.0921 - val_loss: 0.1189 - val_mae: 0.2375\n",
      "Epoch 1062/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0184 - mae: 0.0925 - val_loss: 0.1207 - val_mae: 0.2358\n",
      "Epoch 1063/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0188 - mae: 0.0931 - val_loss: 0.1206 - val_mae: 0.2368\n",
      "Epoch 1064/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0186 - mae: 0.0924 - val_loss: 0.1192 - val_mae: 0.2358\n",
      "Epoch 1065/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0189 - mae: 0.0939 - val_loss: 0.1206 - val_mae: 0.2373\n",
      "Epoch 1066/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0900 - val_loss: 0.1212 - val_mae: 0.2369\n",
      "Epoch 1067/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0183 - mae: 0.0921 - val_loss: 0.1201 - val_mae: 0.2376\n",
      "Epoch 1068/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0185 - mae: 0.0929 - val_loss: 0.1215 - val_mae: 0.2384\n",
      "Epoch 1069/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0920 - val_loss: 0.1221 - val_mae: 0.2377\n",
      "Epoch 1070/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0189 - mae: 0.0926 - val_loss: 0.1197 - val_mae: 0.2366\n",
      "Epoch 1071/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0188 - mae: 0.0933 - val_loss: 0.1219 - val_mae: 0.2356\n",
      "Epoch 1072/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0187 - mae: 0.0921 - val_loss: 0.1201 - val_mae: 0.2378\n",
      "Epoch 1073/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0185 - mae: 0.0924 - val_loss: 0.1216 - val_mae: 0.2372\n",
      "Epoch 1074/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0187 - mae: 0.0926 - val_loss: 0.1204 - val_mae: 0.2368\n",
      "Epoch 1075/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0185 - mae: 0.0927 - val_loss: 0.1188 - val_mae: 0.2350\n",
      "Epoch 1076/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0924 - val_loss: 0.1200 - val_mae: 0.2360\n",
      "Epoch 1077/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0915 - val_loss: 0.1201 - val_mae: 0.2344\n",
      "Epoch 1078/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0188 - mae: 0.0932 - val_loss: 0.1201 - val_mae: 0.2375\n",
      "Epoch 1079/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0186 - mae: 0.0932 - val_loss: 0.1202 - val_mae: 0.2364\n",
      "Epoch 1080/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0186 - mae: 0.0923 - val_loss: 0.1204 - val_mae: 0.2364\n",
      "Epoch 1081/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0183 - mae: 0.0925 - val_loss: 0.1191 - val_mae: 0.2367\n",
      "Epoch 1082/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0185 - mae: 0.0925 - val_loss: 0.1251 - val_mae: 0.2409\n",
      "Epoch 1083/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0907 - val_loss: 0.1229 - val_mae: 0.2391\n",
      "Epoch 1084/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0187 - mae: 0.0929 - val_loss: 0.1202 - val_mae: 0.2348\n",
      "Epoch 1085/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0187 - mae: 0.0933 - val_loss: 0.1203 - val_mae: 0.2364\n",
      "Epoch 1086/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0185 - mae: 0.0936 - val_loss: 0.1209 - val_mae: 0.2377\n",
      "Epoch 1087/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0192 - mae: 0.0951 - val_loss: 0.1220 - val_mae: 0.2398\n",
      "Epoch 1088/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0190 - mae: 0.0930 - val_loss: 0.1205 - val_mae: 0.2370\n",
      "Epoch 1089/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0180 - mae: 0.0915 - val_loss: 0.1217 - val_mae: 0.2356\n",
      "Epoch 1090/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0180 - mae: 0.0904 - val_loss: 0.1220 - val_mae: 0.2359\n",
      "Epoch 1102/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0186 - mae: 0.0929 - val_loss: 0.1185 - val_mae: 0.2349\n",
      "Epoch 1103/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0908 - val_loss: 0.1200 - val_mae: 0.2360\n",
      "Epoch 1104/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0186 - mae: 0.0922 - val_loss: 0.1216 - val_mae: 0.2403\n",
      "Epoch 1105/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0186 - mae: 0.0924 - val_loss: 0.1221 - val_mae: 0.2394\n",
      "Epoch 1106/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0183 - mae: 0.0920 - val_loss: 0.1205 - val_mae: 0.2387\n",
      "Epoch 1107/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0183 - mae: 0.0919 - val_loss: 0.1196 - val_mae: 0.2344\n",
      "Epoch 1108/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0180 - mae: 0.0914 - val_loss: 0.1200 - val_mae: 0.2377\n",
      "Epoch 1109/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0185 - mae: 0.0928 - val_loss: 0.1196 - val_mae: 0.2378\n",
      "Epoch 1110/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0899 - val_loss: 0.1209 - val_mae: 0.2348\n",
      "Epoch 1111/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0184 - mae: 0.0916 - val_loss: 0.1180 - val_mae: 0.2334\n",
      "Epoch 1112/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0911 - val_loss: 0.1224 - val_mae: 0.2400\n",
      "Epoch 1113/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0189 - mae: 0.0929 - val_loss: 0.1194 - val_mae: 0.2368\n",
      "Epoch 1114/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0188 - mae: 0.0935 - val_loss: 0.1225 - val_mae: 0.2408\n",
      "Epoch 1115/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0181 - mae: 0.0911 - val_loss: 0.1208 - val_mae: 0.2393\n",
      "Epoch 1116/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0920 - val_loss: 0.1205 - val_mae: 0.2368\n",
      "Epoch 1117/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0188 - mae: 0.0925 - val_loss: 0.1207 - val_mae: 0.2392\n",
      "Epoch 1118/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0180 - mae: 0.0915 - val_loss: 0.1214 - val_mae: 0.2404\n",
      "Epoch 1119/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0183 - mae: 0.0924 - val_loss: 0.1201 - val_mae: 0.2363\n",
      "Epoch 1120/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0909 - val_loss: 0.1223 - val_mae: 0.2426\n",
      "Epoch 1121/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0898 - val_loss: 0.1227 - val_mae: 0.2389\n",
      "Epoch 1122/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0910 - val_loss: 0.1208 - val_mae: 0.2368\n",
      "Epoch 1123/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0187 - mae: 0.0926 - val_loss: 0.1205 - val_mae: 0.2363\n",
      "Epoch 1124/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0180 - mae: 0.0914 - val_loss: 0.1199 - val_mae: 0.2352\n",
      "Epoch 1125/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0181 - mae: 0.0917 - val_loss: 0.1248 - val_mae: 0.2424\n",
      "Epoch 1126/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0184 - mae: 0.0916 - val_loss: 0.1193 - val_mae: 0.2359\n",
      "Epoch 1127/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0909 - val_loss: 0.1201 - val_mae: 0.2367\n",
      "Epoch 1128/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0185 - mae: 0.0927 - val_loss: 0.1206 - val_mae: 0.2364\n",
      "Epoch 1129/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0181 - mae: 0.0922 - val_loss: 0.1208 - val_mae: 0.2387\n",
      "Epoch 1130/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0909 - val_loss: 0.1209 - val_mae: 0.2370\n",
      "Epoch 1131/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0186 - mae: 0.0926 - val_loss: 0.1184 - val_mae: 0.2349\n",
      "Epoch 1132/2000\n",
      "529/566 [===========================>..] - ETA: 0s - loss: 0.0180 - mae: 0.0905"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0901 - val_loss: 0.1197 - val_mae: 0.2359\n",
      "Epoch 1147/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0184 - mae: 0.0921 - val_loss: 0.1242 - val_mae: 0.2412\n",
      "Epoch 1148/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0180 - mae: 0.0911 - val_loss: 0.1203 - val_mae: 0.2374\n",
      "Epoch 1149/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0892 - val_loss: 0.1198 - val_mae: 0.2381\n",
      "Epoch 1150/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0192 - mae: 0.0935 - val_loss: 0.1200 - val_mae: 0.2365\n",
      "Epoch 1151/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0893 - val_loss: 0.1212 - val_mae: 0.2363\n",
      "Epoch 1163/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0915 - val_loss: 0.1229 - val_mae: 0.2430\n",
      "Epoch 1164/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0181 - mae: 0.0913 - val_loss: 0.1210 - val_mae: 0.2364\n",
      "Epoch 1165/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0908 - val_loss: 0.1207 - val_mae: 0.2394\n",
      "Epoch 1166/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0902 - val_loss: 0.1209 - val_mae: 0.2374\n",
      "Epoch 1167/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0907 - val_loss: 0.1198 - val_mae: 0.2366\n",
      "Epoch 1168/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0901 - val_loss: 0.1202 - val_mae: 0.2367\n",
      "Epoch 1169/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0900 - val_loss: 0.1213 - val_mae: 0.2376\n",
      "Epoch 1170/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0186 - mae: 0.0932 - val_loss: 0.1198 - val_mae: 0.2379\n",
      "Epoch 1171/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0917 - val_loss: 0.1205 - val_mae: 0.2362\n",
      "Epoch 1172/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0909 - val_loss: 0.1208 - val_mae: 0.2382\n",
      "Epoch 1173/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0181 - mae: 0.0912 - val_loss: 0.1195 - val_mae: 0.2370\n",
      "Epoch 1174/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0905 - val_loss: 0.1238 - val_mae: 0.2426\n",
      "Epoch 1175/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0189 - mae: 0.0933 - val_loss: 0.1208 - val_mae: 0.2369\n",
      "Epoch 1176/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0181 - mae: 0.0904 - val_loss: 0.1190 - val_mae: 0.2341\n",
      "Epoch 1177/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0916 - val_loss: 0.1200 - val_mae: 0.2381\n",
      "Epoch 1178/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0908 - val_loss: 0.1188 - val_mae: 0.2369\n",
      "Epoch 1179/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0912 - val_loss: 0.1205 - val_mae: 0.2373\n",
      "Epoch 1180/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0180 - mae: 0.0908 - val_loss: 0.1221 - val_mae: 0.2413\n",
      "Epoch 1181/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0913 - val_loss: 0.1202 - val_mae: 0.2382\n",
      "Epoch 1182/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0913 - val_loss: 0.1194 - val_mae: 0.2343\n",
      "Epoch 1183/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0181 - mae: 0.0912 - val_loss: 0.1198 - val_mae: 0.2356\n",
      "Epoch 1184/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0184 - mae: 0.0916 - val_loss: 0.1198 - val_mae: 0.2371\n",
      "Epoch 1185/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0181 - mae: 0.0912 - val_loss: 0.1210 - val_mae: 0.2371\n",
      "Epoch 1186/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0904 - val_loss: 0.1205 - val_mae: 0.2365\n",
      "Epoch 1187/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0903 - val_loss: 0.1226 - val_mae: 0.2399\n",
      "Epoch 1188/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0897 - val_loss: 0.1236 - val_mae: 0.2401\n",
      "Epoch 1189/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0180 - mae: 0.0911 - val_loss: 0.1205 - val_mae: 0.2366\n",
      "Epoch 1190/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0910 - val_loss: 0.1199 - val_mae: 0.2370\n",
      "Epoch 1191/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0889 - val_loss: 0.1200 - val_mae: 0.2368\n",
      "Epoch 1192/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0180 - mae: 0.0910 - val_loss: 0.1211 - val_mae: 0.2384\n",
      "Epoch 1193/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0913 - val_loss: 0.1232 - val_mae: 0.2415\n",
      "Epoch 1194/2000\n",
      " 49/566 [=>............................] - ETA: 4s - loss: 0.0183 - mae: 0.0941"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0902 - val_loss: 0.1209 - val_mae: 0.2377\n",
      "Epoch 1203/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0901 - val_loss: 0.1202 - val_mae: 0.2371\n",
      "Epoch 1204/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0899 - val_loss: 0.1201 - val_mae: 0.2368\n",
      "Epoch 1205/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0181 - mae: 0.0910 - val_loss: 0.1197 - val_mae: 0.2348\n",
      "Epoch 1206/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0899 - val_loss: 0.1188 - val_mae: 0.2377\n",
      "Epoch 1207/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0910 - val_loss: 0.1197 - val_mae: 0.2370\n",
      "Epoch 1208/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0902 - val_loss: 0.1203 - val_mae: 0.2350\n",
      "Epoch 1209/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0903 - val_loss: 0.1237 - val_mae: 0.2397\n",
      "Epoch 1210/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0181 - mae: 0.0912 - val_loss: 0.1201 - val_mae: 0.2364\n",
      "Epoch 1211/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0908 - val_loss: 0.1217 - val_mae: 0.2393\n",
      "Epoch 1212/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0902 - val_loss: 0.1192 - val_mae: 0.2370\n",
      "Epoch 1213/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0904 - val_loss: 0.1214 - val_mae: 0.2370\n",
      "Epoch 1214/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0904 - val_loss: 0.1202 - val_mae: 0.2359\n",
      "Epoch 1215/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0889 - val_loss: 0.1210 - val_mae: 0.2378\n",
      "Epoch 1216/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0183 - mae: 0.0918 - val_loss: 0.1205 - val_mae: 0.2345\n",
      "Epoch 1217/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0903 - val_loss: 0.1213 - val_mae: 0.2371\n",
      "Epoch 1218/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0901 - val_loss: 0.1221 - val_mae: 0.2381\n",
      "Epoch 1219/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0186 - mae: 0.0912 - val_loss: 0.1206 - val_mae: 0.2373\n",
      "Epoch 1220/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0901 - val_loss: 0.1219 - val_mae: 0.2365\n",
      "Epoch 1221/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0180 - mae: 0.0901 - val_loss: 0.1220 - val_mae: 0.2382\n",
      "Epoch 1222/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0184 - mae: 0.0921 - val_loss: 0.1226 - val_mae: 0.2398\n",
      "Epoch 1223/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0183 - mae: 0.0915 - val_loss: 0.1215 - val_mae: 0.2370\n",
      "Epoch 1224/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0898 - val_loss: 0.1208 - val_mae: 0.2367\n",
      "Epoch 1225/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0895 - val_loss: 0.1221 - val_mae: 0.2384\n",
      "Epoch 1226/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0897 - val_loss: 0.1234 - val_mae: 0.2403\n",
      "Epoch 1227/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0895 - val_loss: 0.1225 - val_mae: 0.2387\n",
      "Epoch 1228/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0887 - val_loss: 0.1211 - val_mae: 0.2374\n",
      "Epoch 1229/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0906 - val_loss: 0.1201 - val_mae: 0.2366\n",
      "Epoch 1230/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0896 - val_loss: 0.1215 - val_mae: 0.2372\n",
      "Epoch 1231/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0180 - mae: 0.0909 - val_loss: 0.1217 - val_mae: 0.2372\n",
      "Epoch 1232/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0898 - val_loss: 0.1239 - val_mae: 0.2397\n",
      "Epoch 1233/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0912 - val_loss: 0.1201 - val_mae: 0.2353\n",
      "Epoch 1234/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0896 - val_loss: 0.1211 - val_mae: 0.2362\n",
      "Epoch 1235/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0186 - mae: 0.0921 - val_loss: 0.1227 - val_mae: 0.2403\n",
      "Epoch 1236/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0188 - mae: 0.0930 - val_loss: 0.1215 - val_mae: 0.2380\n",
      "Epoch 1237/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0181 - mae: 0.0902 - val_loss: 0.1231 - val_mae: 0.2417\n",
      "Epoch 1238/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0900 - val_loss: 0.1211 - val_mae: 0.2357\n",
      "Epoch 1239/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0911 - val_loss: 0.1204 - val_mae: 0.2368\n",
      "Epoch 1240/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0890 - val_loss: 0.1235 - val_mae: 0.2393\n",
      "Epoch 1241/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0894 - val_loss: 0.1215 - val_mae: 0.2388\n",
      "Epoch 1242/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0891 - val_loss: 0.1205 - val_mae: 0.2381\n",
      "Epoch 1243/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0892 - val_loss: 0.1227 - val_mae: 0.2386\n",
      "Epoch 1244/2000\n",
      "566/566 [==============================] - 5s 10ms/step - loss: 0.0184 - mae: 0.0914 - val_loss: 0.1206 - val_mae: 0.2348\n",
      "Epoch 1245/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0904 - val_loss: 0.1200 - val_mae: 0.2361\n",
      "Epoch 1246/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0899 - val_loss: 0.1239 - val_mae: 0.2383\n",
      "Epoch 1247/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0898 - val_loss: 0.1208 - val_mae: 0.2386\n",
      "Epoch 1248/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0888 - val_loss: 0.1218 - val_mae: 0.2373\n",
      "Epoch 1249/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0183 - mae: 0.0912 - val_loss: 0.1191 - val_mae: 0.2346\n",
      "Epoch 1250/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0888 - val_loss: 0.1222 - val_mae: 0.2393\n",
      "Epoch 1251/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0899 - val_loss: 0.1215 - val_mae: 0.2371\n",
      "Epoch 1252/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0902 - val_loss: 0.1224 - val_mae: 0.2409\n",
      "Epoch 1253/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0896 - val_loss: 0.1240 - val_mae: 0.2406\n",
      "Epoch 1254/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0898 - val_loss: 0.1214 - val_mae: 0.2366\n",
      "Epoch 1255/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0892 - val_loss: 0.1222 - val_mae: 0.2376\n",
      "Epoch 1256/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0181 - mae: 0.0906 - val_loss: 0.1231 - val_mae: 0.2396\n",
      "Epoch 1257/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0902 - val_loss: 0.1212 - val_mae: 0.2372\n",
      "Epoch 1258/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0898 - val_loss: 0.1222 - val_mae: 0.2414\n",
      "Epoch 1259/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0884 - val_loss: 0.1227 - val_mae: 0.2392\n",
      "Epoch 1260/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0898 - val_loss: 0.1204 - val_mae: 0.2383\n",
      "Epoch 1261/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0894 - val_loss: 0.1211 - val_mae: 0.2362\n",
      "Epoch 1262/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0894 - val_loss: 0.1213 - val_mae: 0.2398\n",
      "Epoch 1263/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0898 - val_loss: 0.1221 - val_mae: 0.2364\n",
      "Epoch 1264/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0897 - val_loss: 0.1214 - val_mae: 0.2396\n",
      "Epoch 1265/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0898 - val_loss: 0.1215 - val_mae: 0.2391\n",
      "Epoch 1266/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0913 - val_loss: 0.1214 - val_mae: 0.2387\n",
      "Epoch 1267/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0889 - val_loss: 0.1224 - val_mae: 0.2393\n",
      "Epoch 1268/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0174 - mae: 0.0897 - val_loss: 0.1225 - val_mae: 0.2396\n",
      "Epoch 1269/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0902 - val_loss: 0.1233 - val_mae: 0.2415\n",
      "Epoch 1270/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0892 - val_loss: 0.1202 - val_mae: 0.2371\n",
      "Epoch 1271/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0899 - val_loss: 0.1189 - val_mae: 0.2360\n",
      "Epoch 1272/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0890 - val_loss: 0.1215 - val_mae: 0.2379\n",
      "Epoch 1273/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0900 - val_loss: 0.1215 - val_mae: 0.2391\n",
      "Epoch 1274/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0885 - val_loss: 0.1194 - val_mae: 0.2367\n",
      "Epoch 1275/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0181 - mae: 0.0904 - val_loss: 0.1207 - val_mae: 0.2351\n",
      "Epoch 1276/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0875 - val_loss: 0.1198 - val_mae: 0.2344\n",
      "Epoch 1277/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0899 - val_loss: 0.1214 - val_mae: 0.2377\n",
      "Epoch 1278/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0886 - val_loss: 0.1239 - val_mae: 0.2416\n",
      "Epoch 1279/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0899 - val_loss: 0.1215 - val_mae: 0.2378\n",
      "Epoch 1280/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0181 - mae: 0.0904 - val_loss: 0.1210 - val_mae: 0.2362\n",
      "Epoch 1281/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0903 - val_loss: 0.1221 - val_mae: 0.2370\n",
      "Epoch 1282/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0180 - mae: 0.0907 - val_loss: 0.1203 - val_mae: 0.2374\n",
      "Epoch 1283/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0171 - mae: 0.0881 - val_loss: 0.1211 - val_mae: 0.2359\n",
      "Epoch 1284/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0886 - val_loss: 0.1222 - val_mae: 0.2386\n",
      "Epoch 1285/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0905 - val_loss: 0.1233 - val_mae: 0.2393\n",
      "Epoch 1286/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0180 - mae: 0.0909 - val_loss: 0.1209 - val_mae: 0.2371\n",
      "Epoch 1287/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0898 - val_loss: 0.1216 - val_mae: 0.2373\n",
      "Epoch 1288/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0896 - val_loss: 0.1236 - val_mae: 0.2396\n",
      "Epoch 1289/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0887 - val_loss: 0.1232 - val_mae: 0.2403\n",
      "Epoch 1290/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0895 - val_loss: 0.1229 - val_mae: 0.2384\n",
      "Epoch 1291/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0895 - val_loss: 0.1243 - val_mae: 0.2440\n",
      "Epoch 1292/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0901 - val_loss: 0.1214 - val_mae: 0.2364\n",
      "Epoch 1293/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0902 - val_loss: 0.1210 - val_mae: 0.2380\n",
      "Epoch 1294/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0894 - val_loss: 0.1211 - val_mae: 0.2374\n",
      "Epoch 1295/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0904 - val_loss: 0.1204 - val_mae: 0.2357\n",
      "Epoch 1296/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0901 - val_loss: 0.1202 - val_mae: 0.2364\n",
      "Epoch 1297/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0898 - val_loss: 0.1212 - val_mae: 0.2361\n",
      "Epoch 1298/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0182 - mae: 0.0914 - val_loss: 0.1210 - val_mae: 0.2372\n",
      "Epoch 1299/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0890 - val_loss: 0.1199 - val_mae: 0.2355\n",
      "Epoch 1300/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0901 - val_loss: 0.1260 - val_mae: 0.2425\n",
      "Epoch 1301/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0889 - val_loss: 0.1227 - val_mae: 0.2409\n",
      "Epoch 1302/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0903 - val_loss: 0.1212 - val_mae: 0.2392\n",
      "Epoch 1303/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0876 - val_loss: 0.1213 - val_mae: 0.2361\n",
      "Epoch 1304/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0902 - val_loss: 0.1216 - val_mae: 0.2379\n",
      "Epoch 1305/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0185 - mae: 0.0908 - val_loss: 0.1223 - val_mae: 0.2374\n",
      "Epoch 1306/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0894 - val_loss: 0.1208 - val_mae: 0.2365\n",
      "Epoch 1307/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0908 - val_loss: 0.1252 - val_mae: 0.2455\n",
      "Epoch 1308/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0891 - val_loss: 0.1220 - val_mae: 0.2378\n",
      "Epoch 1309/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0902 - val_loss: 0.1222 - val_mae: 0.2372\n",
      "Epoch 1310/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0868 - val_loss: 0.1210 - val_mae: 0.2377\n",
      "Epoch 1311/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0882 - val_loss: 0.1190 - val_mae: 0.2359\n",
      "Epoch 1312/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0888 - val_loss: 0.1196 - val_mae: 0.2369\n",
      "Epoch 1313/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0885 - val_loss: 0.1205 - val_mae: 0.2348\n",
      "Epoch 1314/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0894 - val_loss: 0.1222 - val_mae: 0.2394\n",
      "Epoch 1315/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0179 - mae: 0.0903 - val_loss: 0.1196 - val_mae: 0.2367\n",
      "Epoch 1316/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0902 - val_loss: 0.1199 - val_mae: 0.2370\n",
      "Epoch 1317/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0893 - val_loss: 0.1210 - val_mae: 0.2370\n",
      "Epoch 1318/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0901 - val_loss: 0.1209 - val_mae: 0.2376\n",
      "Epoch 1319/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0872 - val_loss: 0.1215 - val_mae: 0.2384\n",
      "Epoch 1320/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0878 - val_loss: 0.1216 - val_mae: 0.2380\n",
      "Epoch 1321/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0894 - val_loss: 0.1205 - val_mae: 0.2361\n",
      "Epoch 1322/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0908 - val_loss: 0.1208 - val_mae: 0.2375\n",
      "Epoch 1323/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0894 - val_loss: 0.1224 - val_mae: 0.2390\n",
      "Epoch 1324/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0899 - val_loss: 0.1196 - val_mae: 0.2366\n",
      "Epoch 1325/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0876 - val_loss: 0.1193 - val_mae: 0.2375\n",
      "Epoch 1326/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0887 - val_loss: 0.1195 - val_mae: 0.2375\n",
      "Epoch 1327/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0172 - mae: 0.0888 - val_loss: 0.1195 - val_mae: 0.2358\n",
      "Epoch 1328/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0889 - val_loss: 0.1195 - val_mae: 0.2357\n",
      "Epoch 1329/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0883 - val_loss: 0.1227 - val_mae: 0.2401\n",
      "Epoch 1330/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0894 - val_loss: 0.1203 - val_mae: 0.2356\n",
      "Epoch 1331/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0911 - val_loss: 0.1210 - val_mae: 0.2359\n",
      "Epoch 1332/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0893 - val_loss: 0.1214 - val_mae: 0.2370\n",
      "Epoch 1333/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0172 - mae: 0.0888 - val_loss: 0.1212 - val_mae: 0.2381\n",
      "Epoch 1334/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0888 - val_loss: 0.1201 - val_mae: 0.2392\n",
      "Epoch 1335/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0902 - val_loss: 0.1217 - val_mae: 0.2373\n",
      "Epoch 1336/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0903 - val_loss: 0.1187 - val_mae: 0.2347\n",
      "Epoch 1337/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0886 - val_loss: 0.1202 - val_mae: 0.2360\n",
      "Epoch 1338/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0874 - val_loss: 0.1198 - val_mae: 0.2371\n",
      "Epoch 1339/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0888 - val_loss: 0.1209 - val_mae: 0.2375\n",
      "Epoch 1340/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0889 - val_loss: 0.1199 - val_mae: 0.2356\n",
      "Epoch 1341/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0877 - val_loss: 0.1204 - val_mae: 0.2361\n",
      "Epoch 1342/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0880 - val_loss: 0.1257 - val_mae: 0.2431\n",
      "Epoch 1343/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0899 - val_loss: 0.1202 - val_mae: 0.2361\n",
      "Epoch 1344/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0889 - val_loss: 0.1212 - val_mae: 0.2372\n",
      "Epoch 1345/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0173 - mae: 0.0886 - val_loss: 0.1197 - val_mae: 0.2358\n",
      "Epoch 1346/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0881 - val_loss: 0.1216 - val_mae: 0.2383\n",
      "Epoch 1347/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0889 - val_loss: 0.1204 - val_mae: 0.2365\n",
      "Epoch 1348/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0883 - val_loss: 0.1206 - val_mae: 0.2366\n",
      "Epoch 1349/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0878 - val_loss: 0.1199 - val_mae: 0.2355\n",
      "Epoch 1350/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0880 - val_loss: 0.1248 - val_mae: 0.2404\n",
      "Epoch 1351/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0891 - val_loss: 0.1191 - val_mae: 0.2347\n",
      "Epoch 1352/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0896 - val_loss: 0.1219 - val_mae: 0.2368\n",
      "Epoch 1353/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0889 - val_loss: 0.1223 - val_mae: 0.2386\n",
      "Epoch 1354/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0883 - val_loss: 0.1215 - val_mae: 0.2394\n",
      "Epoch 1355/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0894 - val_loss: 0.1204 - val_mae: 0.2386\n",
      "Epoch 1356/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0897 - val_loss: 0.1217 - val_mae: 0.2384\n",
      "Epoch 1357/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0170 - mae: 0.0884 - val_loss: 0.1213 - val_mae: 0.2401\n",
      "Epoch 1358/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0889 - val_loss: 0.1218 - val_mae: 0.2373\n",
      "Epoch 1359/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0890 - val_loss: 0.1228 - val_mae: 0.2410\n",
      "Epoch 1360/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0889 - val_loss: 0.1201 - val_mae: 0.2372\n",
      "Epoch 1361/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0893 - val_loss: 0.1214 - val_mae: 0.2395\n",
      "Epoch 1362/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0891 - val_loss: 0.1186 - val_mae: 0.2348\n",
      "Epoch 1363/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0893 - val_loss: 0.1233 - val_mae: 0.2395\n",
      "Epoch 1364/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0867 - val_loss: 0.1209 - val_mae: 0.2362\n",
      "Epoch 1365/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0879 - val_loss: 0.1226 - val_mae: 0.2385\n",
      "Epoch 1366/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0889 - val_loss: 0.1209 - val_mae: 0.2371\n",
      "Epoch 1367/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0880 - val_loss: 0.1219 - val_mae: 0.2387\n",
      "Epoch 1368/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0892 - val_loss: 0.1219 - val_mae: 0.2370\n",
      "Epoch 1369/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0881 - val_loss: 0.1203 - val_mae: 0.2356\n",
      "Epoch 1370/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0886 - val_loss: 0.1200 - val_mae: 0.2369\n",
      "Epoch 1371/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0881 - val_loss: 0.1200 - val_mae: 0.2350\n",
      "Epoch 1372/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0887 - val_loss: 0.1186 - val_mae: 0.2339\n",
      "Epoch 1373/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0182 - mae: 0.0907 - val_loss: 0.1207 - val_mae: 0.2350\n",
      "Epoch 1374/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0882 - val_loss: 0.1196 - val_mae: 0.2353\n",
      "Epoch 1375/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0893 - val_loss: 0.1210 - val_mae: 0.2389\n",
      "Epoch 1376/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0880 - val_loss: 0.1188 - val_mae: 0.2359\n",
      "Epoch 1377/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0890 - val_loss: 0.1203 - val_mae: 0.2390\n",
      "Epoch 1378/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0887 - val_loss: 0.1196 - val_mae: 0.2359\n",
      "Epoch 1379/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0883 - val_loss: 0.1202 - val_mae: 0.2347\n",
      "Epoch 1380/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0176 - mae: 0.0889 - val_loss: 0.1221 - val_mae: 0.2371\n",
      "Epoch 1381/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0869 - val_loss: 0.1217 - val_mae: 0.2388\n",
      "Epoch 1382/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0890 - val_loss: 0.1235 - val_mae: 0.2406\n",
      "Epoch 1383/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0877 - val_loss: 0.1193 - val_mae: 0.2350\n",
      "Epoch 1384/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0874 - val_loss: 0.1234 - val_mae: 0.2392\n",
      "Epoch 1385/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0875 - val_loss: 0.1209 - val_mae: 0.2368\n",
      "Epoch 1386/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0880 - val_loss: 0.1205 - val_mae: 0.2373\n",
      "Epoch 1387/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0871 - val_loss: 0.1224 - val_mae: 0.2411\n",
      "Epoch 1388/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0893 - val_loss: 0.1223 - val_mae: 0.2404\n",
      "Epoch 1389/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0880 - val_loss: 0.1236 - val_mae: 0.2423\n",
      "Epoch 1390/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0894 - val_loss: 0.1213 - val_mae: 0.2359\n",
      "Epoch 1391/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0178 - mae: 0.0898 - val_loss: 0.1218 - val_mae: 0.2374\n",
      "Epoch 1392/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0891 - val_loss: 0.1213 - val_mae: 0.2379\n",
      "Epoch 1393/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0892 - val_loss: 0.1214 - val_mae: 0.2376\n",
      "Epoch 1394/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0874 - val_loss: 0.1205 - val_mae: 0.2364\n",
      "Epoch 1395/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0874 - val_loss: 0.1207 - val_mae: 0.2366\n",
      "Epoch 1396/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0874 - val_loss: 0.1207 - val_mae: 0.2370\n",
      "Epoch 1397/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0880 - val_loss: 0.1211 - val_mae: 0.2387\n",
      "Epoch 1398/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0874 - val_loss: 0.1210 - val_mae: 0.2367\n",
      "Epoch 1399/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0890 - val_loss: 0.1203 - val_mae: 0.2385\n",
      "Epoch 1400/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0892 - val_loss: 0.1205 - val_mae: 0.2372\n",
      "Epoch 1401/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0883 - val_loss: 0.1222 - val_mae: 0.2392\n",
      "Epoch 1402/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0865 - val_loss: 0.1203 - val_mae: 0.2360\n",
      "Epoch 1403/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0876 - val_loss: 0.1219 - val_mae: 0.2384\n",
      "Epoch 1404/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0169 - mae: 0.0880 - val_loss: 0.1203 - val_mae: 0.2359\n",
      "Epoch 1405/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0867 - val_loss: 0.1214 - val_mae: 0.2384\n",
      "Epoch 1406/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0894 - val_loss: 0.1215 - val_mae: 0.2376\n",
      "Epoch 1407/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0878 - val_loss: 0.1217 - val_mae: 0.2395\n",
      "Epoch 1408/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0875 - val_loss: 0.1215 - val_mae: 0.2365\n",
      "Epoch 1409/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0167 - mae: 0.0866 - val_loss: 0.1215 - val_mae: 0.2381\n",
      "Epoch 1410/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0880 - val_loss: 0.1192 - val_mae: 0.2342\n",
      "Epoch 1411/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0875 - val_loss: 0.1189 - val_mae: 0.2350\n",
      "Epoch 1412/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0873 - val_loss: 0.1204 - val_mae: 0.2368\n",
      "Epoch 1413/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0871 - val_loss: 0.1207 - val_mae: 0.2363\n",
      "Epoch 1414/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0864 - val_loss: 0.1215 - val_mae: 0.2371\n",
      "Epoch 1415/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0887 - val_loss: 0.1206 - val_mae: 0.2345\n",
      "Epoch 1416/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0867 - val_loss: 0.1202 - val_mae: 0.2370\n",
      "Epoch 1417/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0874 - val_loss: 0.1220 - val_mae: 0.2393\n",
      "Epoch 1418/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0889 - val_loss: 0.1222 - val_mae: 0.2379\n",
      "Epoch 1419/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0876 - val_loss: 0.1190 - val_mae: 0.2377\n",
      "Epoch 1420/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0877 - val_loss: 0.1208 - val_mae: 0.2371\n",
      "Epoch 1421/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0877 - val_loss: 0.1224 - val_mae: 0.2387\n",
      "Epoch 1422/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0883 - val_loss: 0.1217 - val_mae: 0.2380\n",
      "Epoch 1423/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0885 - val_loss: 0.1203 - val_mae: 0.2360\n",
      "Epoch 1424/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0886 - val_loss: 0.1205 - val_mae: 0.2341\n",
      "Epoch 1425/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0886 - val_loss: 0.1205 - val_mae: 0.2386\n",
      "Epoch 1426/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0867 - val_loss: 0.1204 - val_mae: 0.2372\n",
      "Epoch 1427/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0881 - val_loss: 0.1221 - val_mae: 0.2379\n",
      "Epoch 1428/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0876 - val_loss: 0.1223 - val_mae: 0.2417\n",
      "Epoch 1429/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0885 - val_loss: 0.1223 - val_mae: 0.2375\n",
      "Epoch 1430/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0878 - val_loss: 0.1209 - val_mae: 0.2376\n",
      "Epoch 1431/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0893 - val_loss: 0.1197 - val_mae: 0.2359\n",
      "Epoch 1432/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0866 - val_loss: 0.1214 - val_mae: 0.2376\n",
      "Epoch 1433/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0167 - mae: 0.0877 - val_loss: 0.1218 - val_mae: 0.2365\n",
      "Epoch 1434/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0885 - val_loss: 0.1220 - val_mae: 0.2384\n",
      "Epoch 1435/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0867 - val_loss: 0.1196 - val_mae: 0.2357\n",
      "Epoch 1436/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0881 - val_loss: 0.1198 - val_mae: 0.2349\n",
      "Epoch 1437/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0873 - val_loss: 0.1215 - val_mae: 0.2381\n",
      "Epoch 1438/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0881 - val_loss: 0.1227 - val_mae: 0.2391\n",
      "Epoch 1439/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0881 - val_loss: 0.1226 - val_mae: 0.2396\n",
      "Epoch 1440/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0888 - val_loss: 0.1204 - val_mae: 0.2358\n",
      "Epoch 1441/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0861 - val_loss: 0.1205 - val_mae: 0.2347\n",
      "Epoch 1442/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0897 - val_loss: 0.1202 - val_mae: 0.2367\n",
      "Epoch 1443/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0885 - val_loss: 0.1209 - val_mae: 0.2386\n",
      "Epoch 1444/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0884 - val_loss: 0.1199 - val_mae: 0.2366\n",
      "Epoch 1445/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0876 - val_loss: 0.1207 - val_mae: 0.2381\n",
      "Epoch 1446/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0863 - val_loss: 0.1187 - val_mae: 0.2347\n",
      "Epoch 1447/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0881 - val_loss: 0.1215 - val_mae: 0.2382\n",
      "Epoch 1448/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0866 - val_loss: 0.1214 - val_mae: 0.2359\n",
      "Epoch 1449/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0885 - val_loss: 0.1240 - val_mae: 0.2384\n",
      "Epoch 1450/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0879 - val_loss: 0.1208 - val_mae: 0.2375\n",
      "Epoch 1451/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0879 - val_loss: 0.1220 - val_mae: 0.2383\n",
      "Epoch 1452/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0874 - val_loss: 0.1230 - val_mae: 0.2417\n",
      "Epoch 1453/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0877 - val_loss: 0.1203 - val_mae: 0.2386\n",
      "Epoch 1454/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0873 - val_loss: 0.1219 - val_mae: 0.2380\n",
      "Epoch 1455/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0882 - val_loss: 0.1221 - val_mae: 0.2384\n",
      "Epoch 1456/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0864 - val_loss: 0.1220 - val_mae: 0.2372\n",
      "Epoch 1457/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0878 - val_loss: 0.1229 - val_mae: 0.2391\n",
      "Epoch 1458/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0878 - val_loss: 0.1218 - val_mae: 0.2369\n",
      "Epoch 1459/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0893 - val_loss: 0.1223 - val_mae: 0.2391\n",
      "Epoch 1460/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0882 - val_loss: 0.1217 - val_mae: 0.2359\n",
      "Epoch 1461/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0887 - val_loss: 0.1214 - val_mae: 0.2369\n",
      "Epoch 1462/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0179 - mae: 0.0906 - val_loss: 0.1223 - val_mae: 0.2397\n",
      "Epoch 1463/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0874 - val_loss: 0.1220 - val_mae: 0.2381\n",
      "Epoch 1464/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0892 - val_loss: 0.1198 - val_mae: 0.2375\n",
      "Epoch 1465/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0867 - val_loss: 0.1202 - val_mae: 0.2361\n",
      "Epoch 1466/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0876 - val_loss: 0.1212 - val_mae: 0.2367\n",
      "Epoch 1467/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0869 - val_loss: 0.1213 - val_mae: 0.2388\n",
      "Epoch 1468/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0867 - val_loss: 0.1205 - val_mae: 0.2369\n",
      "Epoch 1469/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0876 - val_loss: 0.1223 - val_mae: 0.2401\n",
      "Epoch 1470/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0874 - val_loss: 0.1213 - val_mae: 0.2385\n",
      "Epoch 1471/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0876 - val_loss: 0.1221 - val_mae: 0.2383\n",
      "Epoch 1472/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0880 - val_loss: 0.1223 - val_mae: 0.2376\n",
      "Epoch 1473/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0866 - val_loss: 0.1217 - val_mae: 0.2387\n",
      "Epoch 1474/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0884 - val_loss: 0.1203 - val_mae: 0.2349\n",
      "Epoch 1475/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0867 - val_loss: 0.1218 - val_mae: 0.2395\n",
      "Epoch 1476/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0863 - val_loss: 0.1223 - val_mae: 0.2387\n",
      "Epoch 1477/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0872 - val_loss: 0.1214 - val_mae: 0.2377\n",
      "Epoch 1478/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0890 - val_loss: 0.1217 - val_mae: 0.2384\n",
      "Epoch 1479/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0870 - val_loss: 0.1202 - val_mae: 0.2352\n",
      "Epoch 1480/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0866 - val_loss: 0.1203 - val_mae: 0.2382\n",
      "Epoch 1481/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0873 - val_loss: 0.1246 - val_mae: 0.2413\n",
      "Epoch 1482/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0869 - val_loss: 0.1191 - val_mae: 0.2351\n",
      "Epoch 1483/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0176 - mae: 0.0891 - val_loss: 0.1218 - val_mae: 0.2383\n",
      "Epoch 1484/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0862 - val_loss: 0.1214 - val_mae: 0.2364\n",
      "Epoch 1485/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0872 - val_loss: 0.1227 - val_mae: 0.2400\n",
      "Epoch 1486/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0870 - val_loss: 0.1197 - val_mae: 0.2363\n",
      "Epoch 1487/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0857 - val_loss: 0.1207 - val_mae: 0.2341\n",
      "Epoch 1488/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0870 - val_loss: 0.1203 - val_mae: 0.2363\n",
      "Epoch 1489/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0880 - val_loss: 0.1240 - val_mae: 0.2390\n",
      "Epoch 1490/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0874 - val_loss: 0.1220 - val_mae: 0.2373\n",
      "Epoch 1491/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0857 - val_loss: 0.1224 - val_mae: 0.2371\n",
      "Epoch 1492/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0859 - val_loss: 0.1218 - val_mae: 0.2383\n",
      "Epoch 1493/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0860 - val_loss: 0.1219 - val_mae: 0.2358\n",
      "Epoch 1494/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0875 - val_loss: 0.1223 - val_mae: 0.2388\n",
      "Epoch 1495/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0887 - val_loss: 0.1215 - val_mae: 0.2355\n",
      "Epoch 1496/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0878 - val_loss: 0.1227 - val_mae: 0.2410\n",
      "Epoch 1497/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0886 - val_loss: 0.1208 - val_mae: 0.2367\n",
      "Epoch 1498/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0863 - val_loss: 0.1220 - val_mae: 0.2370\n",
      "Epoch 1499/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0863 - val_loss: 0.1233 - val_mae: 0.2389\n",
      "Epoch 1500/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0865 - val_loss: 0.1201 - val_mae: 0.2366\n",
      "Epoch 1501/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0868 - val_loss: 0.1196 - val_mae: 0.2352\n",
      "Epoch 1502/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0177 - mae: 0.0887 - val_loss: 0.1215 - val_mae: 0.2392\n",
      "Epoch 1503/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0863 - val_loss: 0.1228 - val_mae: 0.2403\n",
      "Epoch 1504/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0872 - val_loss: 0.1218 - val_mae: 0.2400\n",
      "Epoch 1505/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0854 - val_loss: 0.1190 - val_mae: 0.2364\n",
      "Epoch 1506/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0870 - val_loss: 0.1211 - val_mae: 0.2382\n",
      "Epoch 1507/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0871 - val_loss: 0.1208 - val_mae: 0.2373\n",
      "Epoch 1508/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0864 - val_loss: 0.1188 - val_mae: 0.2345\n",
      "Epoch 1509/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0872 - val_loss: 0.1199 - val_mae: 0.2359\n",
      "Epoch 1510/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0879 - val_loss: 0.1207 - val_mae: 0.2381\n",
      "Epoch 1511/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0881 - val_loss: 0.1219 - val_mae: 0.2377\n",
      "Epoch 1512/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0874 - val_loss: 0.1200 - val_mae: 0.2363\n",
      "Epoch 1513/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0871 - val_loss: 0.1192 - val_mae: 0.2343\n",
      "Epoch 1514/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0868 - val_loss: 0.1228 - val_mae: 0.2406\n",
      "Epoch 1515/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0882 - val_loss: 0.1191 - val_mae: 0.2364\n",
      "Epoch 1516/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0868 - val_loss: 0.1212 - val_mae: 0.2355\n",
      "Epoch 1517/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0862 - val_loss: 0.1201 - val_mae: 0.2370\n",
      "Epoch 1518/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0869 - val_loss: 0.1211 - val_mae: 0.2384\n",
      "Epoch 1519/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0867 - val_loss: 0.1203 - val_mae: 0.2359\n",
      "Epoch 1520/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0890 - val_loss: 0.1224 - val_mae: 0.2391\n",
      "Epoch 1521/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0857 - val_loss: 0.1201 - val_mae: 0.2378\n",
      "Epoch 1522/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0882 - val_loss: 0.1201 - val_mae: 0.2357\n",
      "Epoch 1523/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0882 - val_loss: 0.1216 - val_mae: 0.2369\n",
      "Epoch 1524/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0872 - val_loss: 0.1208 - val_mae: 0.2366\n",
      "Epoch 1525/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0876 - val_loss: 0.1212 - val_mae: 0.2393\n",
      "Epoch 1526/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0884 - val_loss: 0.1216 - val_mae: 0.2389\n",
      "Epoch 1527/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0862 - val_loss: 0.1204 - val_mae: 0.2356\n",
      "Epoch 1528/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0864 - val_loss: 0.1229 - val_mae: 0.2382\n",
      "Epoch 1529/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0872 - val_loss: 0.1257 - val_mae: 0.2419\n",
      "Epoch 1530/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0856 - val_loss: 0.1208 - val_mae: 0.2385\n",
      "Epoch 1531/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0865 - val_loss: 0.1236 - val_mae: 0.2417\n",
      "Epoch 1532/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0871 - val_loss: 0.1196 - val_mae: 0.2359\n",
      "Epoch 1533/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0872 - val_loss: 0.1203 - val_mae: 0.2382\n",
      "Epoch 1534/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0877 - val_loss: 0.1220 - val_mae: 0.2376\n",
      "Epoch 1535/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0870 - val_loss: 0.1201 - val_mae: 0.2351\n",
      "Epoch 1536/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0861 - val_loss: 0.1208 - val_mae: 0.2366\n",
      "Epoch 1537/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0174 - mae: 0.0880 - val_loss: 0.1207 - val_mae: 0.2382\n",
      "Epoch 1538/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0866 - val_loss: 0.1205 - val_mae: 0.2363\n",
      "Epoch 1539/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0849 - val_loss: 0.1212 - val_mae: 0.2378\n",
      "Epoch 1540/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0875 - val_loss: 0.1208 - val_mae: 0.2376\n",
      "Epoch 1541/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0856 - val_loss: 0.1214 - val_mae: 0.2371\n",
      "Epoch 1542/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0865 - val_loss: 0.1214 - val_mae: 0.2390\n",
      "Epoch 1543/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0873 - val_loss: 0.1238 - val_mae: 0.2410\n",
      "Epoch 1544/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0871 - val_loss: 0.1206 - val_mae: 0.2353\n",
      "Epoch 1545/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0877 - val_loss: 0.1216 - val_mae: 0.2367\n",
      "Epoch 1546/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0864 - val_loss: 0.1226 - val_mae: 0.2371\n",
      "Epoch 1547/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0860 - val_loss: 0.1215 - val_mae: 0.2364\n",
      "Epoch 1548/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0864 - val_loss: 0.1222 - val_mae: 0.2382\n",
      "Epoch 1549/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0866 - val_loss: 0.1200 - val_mae: 0.2348\n",
      "Epoch 1550/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0887 - val_loss: 0.1224 - val_mae: 0.2412\n",
      "Epoch 1551/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0175 - mae: 0.0891 - val_loss: 0.1200 - val_mae: 0.2373\n",
      "Epoch 1552/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0850 - val_loss: 0.1214 - val_mae: 0.2360\n",
      "Epoch 1553/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0849 - val_loss: 0.1224 - val_mae: 0.2397\n",
      "Epoch 1554/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0879 - val_loss: 0.1218 - val_mae: 0.2386\n",
      "Epoch 1555/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0875 - val_loss: 0.1231 - val_mae: 0.2386\n",
      "Epoch 1556/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0864 - val_loss: 0.1231 - val_mae: 0.2399\n",
      "Epoch 1557/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0864 - val_loss: 0.1227 - val_mae: 0.2370\n",
      "Epoch 1558/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0868 - val_loss: 0.1228 - val_mae: 0.2399\n",
      "Epoch 1559/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0855 - val_loss: 0.1238 - val_mae: 0.2389\n",
      "Epoch 1560/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0867 - val_loss: 0.1211 - val_mae: 0.2358\n",
      "Epoch 1561/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0862 - val_loss: 0.1213 - val_mae: 0.2364\n",
      "Epoch 1562/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0869 - val_loss: 0.1228 - val_mae: 0.2393\n",
      "Epoch 1563/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0874 - val_loss: 0.1212 - val_mae: 0.2350\n",
      "Epoch 1564/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0862 - val_loss: 0.1225 - val_mae: 0.2382\n",
      "Epoch 1565/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0859 - val_loss: 0.1219 - val_mae: 0.2372\n",
      "Epoch 1566/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0856 - val_loss: 0.1207 - val_mae: 0.2386\n",
      "Epoch 1567/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0861 - val_loss: 0.1211 - val_mae: 0.2381\n",
      "Epoch 1568/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0875 - val_loss: 0.1217 - val_mae: 0.2381\n",
      "Epoch 1569/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0862 - val_loss: 0.1203 - val_mae: 0.2346\n",
      "Epoch 1570/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0863 - val_loss: 0.1202 - val_mae: 0.2349\n",
      "Epoch 1571/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0868 - val_loss: 0.1222 - val_mae: 0.2372\n",
      "Epoch 1572/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0860 - val_loss: 0.1215 - val_mae: 0.2386\n",
      "Epoch 1573/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0858 - val_loss: 0.1229 - val_mae: 0.2389\n",
      "Epoch 1574/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0865 - val_loss: 0.1210 - val_mae: 0.2383\n",
      "Epoch 1575/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0867 - val_loss: 0.1201 - val_mae: 0.2370\n",
      "Epoch 1576/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0852 - val_loss: 0.1221 - val_mae: 0.2376\n",
      "Epoch 1577/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0842 - val_loss: 0.1206 - val_mae: 0.2363\n",
      "Epoch 1578/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0868 - val_loss: 0.1218 - val_mae: 0.2382\n",
      "Epoch 1579/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0868 - val_loss: 0.1220 - val_mae: 0.2385\n",
      "Epoch 1580/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0857 - val_loss: 0.1208 - val_mae: 0.2364\n",
      "Epoch 1581/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0853 - val_loss: 0.1225 - val_mae: 0.2407\n",
      "Epoch 1582/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0173 - mae: 0.0876 - val_loss: 0.1192 - val_mae: 0.2362\n",
      "Epoch 1583/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0865 - val_loss: 0.1199 - val_mae: 0.2375\n",
      "Epoch 1584/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0861 - val_loss: 0.1202 - val_mae: 0.2361\n",
      "Epoch 1585/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0869 - val_loss: 0.1231 - val_mae: 0.2421\n",
      "Epoch 1586/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0875 - val_loss: 0.1210 - val_mae: 0.2402\n",
      "Epoch 1587/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0872 - val_loss: 0.1207 - val_mae: 0.2367\n",
      "Epoch 1588/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0873 - val_loss: 0.1218 - val_mae: 0.2390\n",
      "Epoch 1589/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0877 - val_loss: 0.1207 - val_mae: 0.2375\n",
      "Epoch 1590/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0862 - val_loss: 0.1230 - val_mae: 0.2391\n",
      "Epoch 1591/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0871 - val_loss: 0.1205 - val_mae: 0.2349\n",
      "Epoch 1592/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0853 - val_loss: 0.1222 - val_mae: 0.2384\n",
      "Epoch 1593/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0862 - val_loss: 0.1223 - val_mae: 0.2399\n",
      "Epoch 1594/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0172 - mae: 0.0881 - val_loss: 0.1209 - val_mae: 0.2349\n",
      "Epoch 1595/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0851 - val_loss: 0.1228 - val_mae: 0.2375\n",
      "Epoch 1596/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0871 - val_loss: 0.1216 - val_mae: 0.2388\n",
      "Epoch 1597/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0856 - val_loss: 0.1202 - val_mae: 0.2346\n",
      "Epoch 1598/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0857 - val_loss: 0.1218 - val_mae: 0.2388\n",
      "Epoch 1599/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0856 - val_loss: 0.1219 - val_mae: 0.2373\n",
      "Epoch 1600/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0868 - val_loss: 0.1210 - val_mae: 0.2372\n",
      "Epoch 1601/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0881 - val_loss: 0.1229 - val_mae: 0.2391\n",
      "Epoch 1602/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0849 - val_loss: 0.1214 - val_mae: 0.2365\n",
      "Epoch 1603/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0859 - val_loss: 0.1232 - val_mae: 0.2413\n",
      "Epoch 1604/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0864 - val_loss: 0.1211 - val_mae: 0.2380\n",
      "Epoch 1605/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0853 - val_loss: 0.1191 - val_mae: 0.2340\n",
      "Epoch 1606/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0866 - val_loss: 0.1220 - val_mae: 0.2388\n",
      "Epoch 1607/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0861 - val_loss: 0.1224 - val_mae: 0.2382\n",
      "Epoch 1608/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0863 - val_loss: 0.1215 - val_mae: 0.2372\n",
      "Epoch 1609/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0843 - val_loss: 0.1222 - val_mae: 0.2380\n",
      "Epoch 1610/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0848 - val_loss: 0.1215 - val_mae: 0.2359\n",
      "Epoch 1611/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0858 - val_loss: 0.1216 - val_mae: 0.2371\n",
      "Epoch 1612/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0858 - val_loss: 0.1226 - val_mae: 0.2367\n",
      "Epoch 1613/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0867 - val_loss: 0.1230 - val_mae: 0.2395\n",
      "Epoch 1614/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0877 - val_loss: 0.1223 - val_mae: 0.2372\n",
      "Epoch 1615/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0860 - val_loss: 0.1213 - val_mae: 0.2394\n",
      "Epoch 1616/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0866 - val_loss: 0.1224 - val_mae: 0.2396\n",
      "Epoch 1617/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0855 - val_loss: 0.1218 - val_mae: 0.2373\n",
      "Epoch 1618/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0865 - val_loss: 0.1214 - val_mae: 0.2387\n",
      "Epoch 1619/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0870 - val_loss: 0.1216 - val_mae: 0.2377\n",
      "Epoch 1620/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0868 - val_loss: 0.1212 - val_mae: 0.2373\n",
      "Epoch 1621/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0853 - val_loss: 0.1233 - val_mae: 0.2411\n",
      "Epoch 1622/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0855 - val_loss: 0.1215 - val_mae: 0.2360\n",
      "Epoch 1623/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0865 - val_loss: 0.1225 - val_mae: 0.2391\n",
      "Epoch 1624/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0866 - val_loss: 0.1236 - val_mae: 0.2386\n",
      "Epoch 1625/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0841 - val_loss: 0.1223 - val_mae: 0.2390\n",
      "Epoch 1626/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0854 - val_loss: 0.1224 - val_mae: 0.2377\n",
      "Epoch 1627/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0871 - val_loss: 0.1209 - val_mae: 0.2376\n",
      "Epoch 1628/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0869 - val_loss: 0.1234 - val_mae: 0.2398\n",
      "Epoch 1629/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0871 - val_loss: 0.1214 - val_mae: 0.2369\n",
      "Epoch 1630/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0843 - val_loss: 0.1227 - val_mae: 0.2399\n",
      "Epoch 1631/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0864 - val_loss: 0.1218 - val_mae: 0.2391\n",
      "Epoch 1632/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0856 - val_loss: 0.1228 - val_mae: 0.2400\n",
      "Epoch 1633/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0862 - val_loss: 0.1205 - val_mae: 0.2360\n",
      "Epoch 1634/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0847 - val_loss: 0.1212 - val_mae: 0.2379\n",
      "Epoch 1635/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0864 - val_loss: 0.1220 - val_mae: 0.2385\n",
      "Epoch 1636/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0868 - val_loss: 0.1212 - val_mae: 0.2369\n",
      "Epoch 1637/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0857 - val_loss: 0.1202 - val_mae: 0.2360\n",
      "Epoch 1638/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0850 - val_loss: 0.1213 - val_mae: 0.2389\n",
      "Epoch 1639/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0851 - val_loss: 0.1199 - val_mae: 0.2368\n",
      "Epoch 1640/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0865 - val_loss: 0.1218 - val_mae: 0.2377\n",
      "Epoch 1641/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0856 - val_loss: 0.1191 - val_mae: 0.2344\n",
      "Epoch 1642/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0873 - val_loss: 0.1236 - val_mae: 0.2406\n",
      "Epoch 1643/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0852 - val_loss: 0.1214 - val_mae: 0.2377\n",
      "Epoch 1644/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0851 - val_loss: 0.1216 - val_mae: 0.2384\n",
      "Epoch 1645/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0860 - val_loss: 0.1214 - val_mae: 0.2375\n",
      "Epoch 1646/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0859 - val_loss: 0.1216 - val_mae: 0.2382\n",
      "Epoch 1647/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0854 - val_loss: 0.1231 - val_mae: 0.2420\n",
      "Epoch 1648/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0847 - val_loss: 0.1220 - val_mae: 0.2365\n",
      "Epoch 1649/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0840 - val_loss: 0.1235 - val_mae: 0.2396\n",
      "Epoch 1650/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0873 - val_loss: 0.1212 - val_mae: 0.2371\n",
      "Epoch 1651/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0852 - val_loss: 0.1214 - val_mae: 0.2404\n",
      "Epoch 1652/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0855 - val_loss: 0.1222 - val_mae: 0.2390\n",
      "Epoch 1653/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0857 - val_loss: 0.1237 - val_mae: 0.2406\n",
      "Epoch 1654/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0855 - val_loss: 0.1229 - val_mae: 0.2404\n",
      "Epoch 1655/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0863 - val_loss: 0.1218 - val_mae: 0.2376\n",
      "Epoch 1656/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0843 - val_loss: 0.1220 - val_mae: 0.2374\n",
      "Epoch 1657/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0857 - val_loss: 0.1227 - val_mae: 0.2365\n",
      "Epoch 1658/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0871 - val_loss: 0.1198 - val_mae: 0.2356\n",
      "Epoch 1659/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0846 - val_loss: 0.1216 - val_mae: 0.2391\n",
      "Epoch 1660/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0855 - val_loss: 0.1217 - val_mae: 0.2374\n",
      "Epoch 1661/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0853 - val_loss: 0.1216 - val_mae: 0.2376\n",
      "Epoch 1662/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0862 - val_loss: 0.1200 - val_mae: 0.2359\n",
      "Epoch 1663/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0844 - val_loss: 0.1201 - val_mae: 0.2374\n",
      "Epoch 1664/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0850 - val_loss: 0.1211 - val_mae: 0.2369\n",
      "Epoch 1665/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0867 - val_loss: 0.1216 - val_mae: 0.2379\n",
      "Epoch 1666/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0873 - val_loss: 0.1208 - val_mae: 0.2380\n",
      "Epoch 1667/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0852 - val_loss: 0.1209 - val_mae: 0.2374\n",
      "Epoch 1668/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0872 - val_loss: 0.1214 - val_mae: 0.2383\n",
      "Epoch 1669/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0853 - val_loss: 0.1222 - val_mae: 0.2383\n",
      "Epoch 1670/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0868 - val_loss: 0.1209 - val_mae: 0.2402\n",
      "Epoch 1671/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0870 - val_loss: 0.1219 - val_mae: 0.2373\n",
      "Epoch 1672/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0861 - val_loss: 0.1223 - val_mae: 0.2375\n",
      "Epoch 1673/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0859 - val_loss: 0.1234 - val_mae: 0.2379\n",
      "Epoch 1674/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0844 - val_loss: 0.1208 - val_mae: 0.2356\n",
      "Epoch 1675/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0841 - val_loss: 0.1200 - val_mae: 0.2357\n",
      "Epoch 1676/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0855 - val_loss: 0.1216 - val_mae: 0.2378\n",
      "Epoch 1677/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0848 - val_loss: 0.1220 - val_mae: 0.2378\n",
      "Epoch 1678/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0857 - val_loss: 0.1213 - val_mae: 0.2369\n",
      "Epoch 1679/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0845 - val_loss: 0.1211 - val_mae: 0.2359\n",
      "Epoch 1680/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0847 - val_loss: 0.1227 - val_mae: 0.2378\n",
      "Epoch 1681/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0852 - val_loss: 0.1215 - val_mae: 0.2384\n",
      "Epoch 1682/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0842 - val_loss: 0.1213 - val_mae: 0.2377\n",
      "Epoch 1683/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0868 - val_loss: 0.1210 - val_mae: 0.2375\n",
      "Epoch 1684/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0829 - val_loss: 0.1202 - val_mae: 0.2362\n",
      "Epoch 1685/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0845 - val_loss: 0.1239 - val_mae: 0.2399\n",
      "Epoch 1686/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0861 - val_loss: 0.1237 - val_mae: 0.2378\n",
      "Epoch 1687/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0847 - val_loss: 0.1226 - val_mae: 0.2374\n",
      "Epoch 1688/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0857 - val_loss: 0.1213 - val_mae: 0.2376\n",
      "Epoch 1689/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0859 - val_loss: 0.1213 - val_mae: 0.2399\n",
      "Epoch 1690/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0160 - mae: 0.0846 - val_loss: 0.1192 - val_mae: 0.2375\n",
      "Epoch 1691/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0859 - val_loss: 0.1213 - val_mae: 0.2377\n",
      "Epoch 1692/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0851 - val_loss: 0.1217 - val_mae: 0.2366\n",
      "Epoch 1693/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0857 - val_loss: 0.1214 - val_mae: 0.2381\n",
      "Epoch 1694/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0852 - val_loss: 0.1209 - val_mae: 0.2377\n",
      "Epoch 1695/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0854 - val_loss: 0.1208 - val_mae: 0.2358\n",
      "Epoch 1696/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0170 - mae: 0.0863 - val_loss: 0.1200 - val_mae: 0.2359\n",
      "Epoch 1697/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0849 - val_loss: 0.1219 - val_mae: 0.2368\n",
      "Epoch 1698/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0851 - val_loss: 0.1214 - val_mae: 0.2372\n",
      "Epoch 1699/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0857 - val_loss: 0.1220 - val_mae: 0.2380\n",
      "Epoch 1700/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0860 - val_loss: 0.1220 - val_mae: 0.2364\n",
      "Epoch 1701/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0840 - val_loss: 0.1219 - val_mae: 0.2376\n",
      "Epoch 1702/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0162 - mae: 0.0854 - val_loss: 0.1220 - val_mae: 0.2388\n",
      "Epoch 1703/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0846 - val_loss: 0.1226 - val_mae: 0.2381\n",
      "Epoch 1704/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0872 - val_loss: 0.1205 - val_mae: 0.2374\n",
      "Epoch 1705/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0840 - val_loss: 0.1232 - val_mae: 0.2377\n",
      "Epoch 1706/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0853 - val_loss: 0.1217 - val_mae: 0.2368\n",
      "Epoch 1707/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0858 - val_loss: 0.1244 - val_mae: 0.2416\n",
      "Epoch 1708/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0160 - mae: 0.0847 - val_loss: 0.1222 - val_mae: 0.2395\n",
      "Epoch 1709/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0864 - val_loss: 0.1210 - val_mae: 0.2373\n",
      "Epoch 1710/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0849 - val_loss: 0.1227 - val_mae: 0.2389\n",
      "Epoch 1711/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0857 - val_loss: 0.1239 - val_mae: 0.2373\n",
      "Epoch 1712/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0858 - val_loss: 0.1223 - val_mae: 0.2390\n",
      "Epoch 1713/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0845 - val_loss: 0.1208 - val_mae: 0.2373\n",
      "Epoch 1714/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0859 - val_loss: 0.1243 - val_mae: 0.2422\n",
      "Epoch 1715/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0843 - val_loss: 0.1211 - val_mae: 0.2369\n",
      "Epoch 1716/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0837 - val_loss: 0.1207 - val_mae: 0.2386\n",
      "Epoch 1717/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0169 - mae: 0.0872 - val_loss: 0.1212 - val_mae: 0.2352\n",
      "Epoch 1718/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0844 - val_loss: 0.1198 - val_mae: 0.2364\n",
      "Epoch 1719/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0855 - val_loss: 0.1225 - val_mae: 0.2363\n",
      "Epoch 1720/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0861 - val_loss: 0.1235 - val_mae: 0.2393\n",
      "Epoch 1721/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0847 - val_loss: 0.1242 - val_mae: 0.2390\n",
      "Epoch 1722/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0848 - val_loss: 0.1243 - val_mae: 0.2422\n",
      "Epoch 1723/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0844 - val_loss: 0.1224 - val_mae: 0.2388\n",
      "Epoch 1724/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0850 - val_loss: 0.1216 - val_mae: 0.2394\n",
      "Epoch 1725/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0865 - val_loss: 0.1211 - val_mae: 0.2374\n",
      "Epoch 1726/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0853 - val_loss: 0.1209 - val_mae: 0.2382\n",
      "Epoch 1727/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0861 - val_loss: 0.1221 - val_mae: 0.2385\n",
      "Epoch 1728/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0848 - val_loss: 0.1212 - val_mae: 0.2356\n",
      "Epoch 1729/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0856 - val_loss: 0.1227 - val_mae: 0.2386\n",
      "Epoch 1730/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0837 - val_loss: 0.1220 - val_mae: 0.2390\n",
      "Epoch 1731/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0857 - val_loss: 0.1250 - val_mae: 0.2439\n",
      "Epoch 1732/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0161 - mae: 0.0847 - val_loss: 0.1218 - val_mae: 0.2376\n",
      "Epoch 1733/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0847 - val_loss: 0.1220 - val_mae: 0.2399\n",
      "Epoch 1734/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0860 - val_loss: 0.1222 - val_mae: 0.2371\n",
      "Epoch 1735/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0155 - mae: 0.0832 - val_loss: 0.1214 - val_mae: 0.2364\n",
      "Epoch 1736/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0840 - val_loss: 0.1223 - val_mae: 0.2386\n",
      "Epoch 1737/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0857 - val_loss: 0.1215 - val_mae: 0.2366\n",
      "Epoch 1738/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0156 - mae: 0.0840 - val_loss: 0.1216 - val_mae: 0.2400\n",
      "Epoch 1739/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0859 - val_loss: 0.1209 - val_mae: 0.2361\n",
      "Epoch 1740/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0159 - mae: 0.0835 - val_loss: 0.1207 - val_mae: 0.2382\n",
      "Epoch 1741/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0854 - val_loss: 0.1228 - val_mae: 0.2405\n",
      "Epoch 1742/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0833 - val_loss: 0.1226 - val_mae: 0.2390\n",
      "Epoch 1743/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0838 - val_loss: 0.1216 - val_mae: 0.2392\n",
      "Epoch 1744/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0156 - mae: 0.0840 - val_loss: 0.1229 - val_mae: 0.2372\n",
      "Epoch 1745/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0171 - mae: 0.0876 - val_loss: 0.1224 - val_mae: 0.2395\n",
      "Epoch 1746/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0848 - val_loss: 0.1227 - val_mae: 0.2400\n",
      "Epoch 1747/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0853 - val_loss: 0.1226 - val_mae: 0.2388\n",
      "Epoch 1748/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0847 - val_loss: 0.1220 - val_mae: 0.2393\n",
      "Epoch 1749/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0853 - val_loss: 0.1209 - val_mae: 0.2361\n",
      "Epoch 1750/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0848 - val_loss: 0.1211 - val_mae: 0.2358\n",
      "Epoch 1751/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0834 - val_loss: 0.1226 - val_mae: 0.2404\n",
      "Epoch 1752/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0856 - val_loss: 0.1207 - val_mae: 0.2368\n",
      "Epoch 1753/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0870 - val_loss: 0.1228 - val_mae: 0.2388\n",
      "Epoch 1754/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0853 - val_loss: 0.1211 - val_mae: 0.2375\n",
      "Epoch 1755/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0846 - val_loss: 0.1211 - val_mae: 0.2361\n",
      "Epoch 1756/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0842 - val_loss: 0.1217 - val_mae: 0.2378\n",
      "Epoch 1757/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0838 - val_loss: 0.1213 - val_mae: 0.2369\n",
      "Epoch 1758/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0856 - val_loss: 0.1211 - val_mae: 0.2377\n",
      "Epoch 1759/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0851 - val_loss: 0.1207 - val_mae: 0.2362\n",
      "Epoch 1760/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0850 - val_loss: 0.1225 - val_mae: 0.2369\n",
      "Epoch 1761/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0853 - val_loss: 0.1220 - val_mae: 0.2377\n",
      "Epoch 1762/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0854 - val_loss: 0.1211 - val_mae: 0.2397\n",
      "Epoch 1763/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0845 - val_loss: 0.1244 - val_mae: 0.2403\n",
      "Epoch 1764/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0844 - val_loss: 0.1212 - val_mae: 0.2387\n",
      "Epoch 1765/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0844 - val_loss: 0.1221 - val_mae: 0.2368\n",
      "Epoch 1766/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0838 - val_loss: 0.1209 - val_mae: 0.2364\n",
      "Epoch 1767/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0155 - mae: 0.0832 - val_loss: 0.1219 - val_mae: 0.2399\n",
      "Epoch 1768/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0857 - val_loss: 0.1206 - val_mae: 0.2372\n",
      "Epoch 1769/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0155 - mae: 0.0827 - val_loss: 0.1224 - val_mae: 0.2387\n",
      "Epoch 1770/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0853 - val_loss: 0.1207 - val_mae: 0.2377\n",
      "Epoch 1771/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0866 - val_loss: 0.1228 - val_mae: 0.2397\n",
      "Epoch 1772/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0842 - val_loss: 0.1222 - val_mae: 0.2364\n",
      "Epoch 1773/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0168 - mae: 0.0868 - val_loss: 0.1221 - val_mae: 0.2405\n",
      "Epoch 1774/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0852 - val_loss: 0.1228 - val_mae: 0.2375\n",
      "Epoch 1775/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0842 - val_loss: 0.1234 - val_mae: 0.2391\n",
      "Epoch 1776/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0858 - val_loss: 0.1212 - val_mae: 0.2380\n",
      "Epoch 1777/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0843 - val_loss: 0.1208 - val_mae: 0.2363\n",
      "Epoch 1778/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0851 - val_loss: 0.1220 - val_mae: 0.2407\n",
      "Epoch 1779/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0844 - val_loss: 0.1232 - val_mae: 0.2396\n",
      "Epoch 1780/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0848 - val_loss: 0.1238 - val_mae: 0.2397\n",
      "Epoch 1781/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0838 - val_loss: 0.1205 - val_mae: 0.2371\n",
      "Epoch 1782/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0845 - val_loss: 0.1213 - val_mae: 0.2360\n",
      "Epoch 1783/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0844 - val_loss: 0.1218 - val_mae: 0.2377\n",
      "Epoch 1784/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0839 - val_loss: 0.1194 - val_mae: 0.2353\n",
      "Epoch 1785/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0843 - val_loss: 0.1209 - val_mae: 0.2369\n",
      "Epoch 1786/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0844 - val_loss: 0.1221 - val_mae: 0.2399\n",
      "Epoch 1787/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0157 - mae: 0.0837 - val_loss: 0.1231 - val_mae: 0.2381\n",
      "Epoch 1788/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0837 - val_loss: 0.1256 - val_mae: 0.2432\n",
      "Epoch 1789/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0154 - mae: 0.0829 - val_loss: 0.1243 - val_mae: 0.2407\n",
      "Epoch 1790/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0839 - val_loss: 0.1236 - val_mae: 0.2393\n",
      "Epoch 1791/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0854 - val_loss: 0.1209 - val_mae: 0.2384\n",
      "Epoch 1792/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0846 - val_loss: 0.1219 - val_mae: 0.2375\n",
      "Epoch 1793/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0161 - mae: 0.0845 - val_loss: 0.1219 - val_mae: 0.2367\n",
      "Epoch 1794/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0848 - val_loss: 0.1234 - val_mae: 0.2393\n",
      "Epoch 1795/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0838 - val_loss: 0.1228 - val_mae: 0.2387\n",
      "Epoch 1796/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0842 - val_loss: 0.1207 - val_mae: 0.2370\n",
      "Epoch 1797/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0850 - val_loss: 0.1207 - val_mae: 0.2369\n",
      "Epoch 1798/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0840 - val_loss: 0.1244 - val_mae: 0.2418\n",
      "Epoch 1799/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0851 - val_loss: 0.1218 - val_mae: 0.2361\n",
      "Epoch 1800/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0839 - val_loss: 0.1219 - val_mae: 0.2372\n",
      "Epoch 1801/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0836 - val_loss: 0.1218 - val_mae: 0.2377\n",
      "Epoch 1802/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0847 - val_loss: 0.1211 - val_mae: 0.2376\n",
      "Epoch 1803/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0857 - val_loss: 0.1255 - val_mae: 0.2430\n",
      "Epoch 1804/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0867 - val_loss: 0.1209 - val_mae: 0.2373\n",
      "Epoch 1805/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0848 - val_loss: 0.1233 - val_mae: 0.2404\n",
      "Epoch 1806/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0860 - val_loss: 0.1231 - val_mae: 0.2388\n",
      "Epoch 1807/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0154 - mae: 0.0833 - val_loss: 0.1209 - val_mae: 0.2363\n",
      "Epoch 1808/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0843 - val_loss: 0.1212 - val_mae: 0.2361\n",
      "Epoch 1809/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0850 - val_loss: 0.1217 - val_mae: 0.2365\n",
      "Epoch 1810/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0163 - mae: 0.0854 - val_loss: 0.1209 - val_mae: 0.2378\n",
      "Epoch 1811/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0846 - val_loss: 0.1199 - val_mae: 0.2349\n",
      "Epoch 1812/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0843 - val_loss: 0.1217 - val_mae: 0.2396\n",
      "Epoch 1813/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0165 - mae: 0.0861 - val_loss: 0.1216 - val_mae: 0.2377\n",
      "Epoch 1814/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0841 - val_loss: 0.1212 - val_mae: 0.2378\n",
      "Epoch 1815/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0851 - val_loss: 0.1211 - val_mae: 0.2372\n",
      "Epoch 1816/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0842 - val_loss: 0.1214 - val_mae: 0.2369\n",
      "Epoch 1817/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0842 - val_loss: 0.1207 - val_mae: 0.2363\n",
      "Epoch 1818/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0840 - val_loss: 0.1246 - val_mae: 0.2409\n",
      "Epoch 1819/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0155 - mae: 0.0837 - val_loss: 0.1229 - val_mae: 0.2388\n",
      "Epoch 1820/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0153 - mae: 0.0829 - val_loss: 0.1205 - val_mae: 0.2355\n",
      "Epoch 1821/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0845 - val_loss: 0.1218 - val_mae: 0.2378\n",
      "Epoch 1822/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0857 - val_loss: 0.1203 - val_mae: 0.2372\n",
      "Epoch 1823/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0841 - val_loss: 0.1207 - val_mae: 0.2382\n",
      "Epoch 1824/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0858 - val_loss: 0.1219 - val_mae: 0.2393\n",
      "Epoch 1825/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0848 - val_loss: 0.1211 - val_mae: 0.2384\n",
      "Epoch 1826/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0854 - val_loss: 0.1205 - val_mae: 0.2376\n",
      "Epoch 1827/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0852 - val_loss: 0.1201 - val_mae: 0.2367\n",
      "Epoch 1828/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0846 - val_loss: 0.1233 - val_mae: 0.2397\n",
      "Epoch 1829/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0836 - val_loss: 0.1229 - val_mae: 0.2370\n",
      "Epoch 1830/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0836 - val_loss: 0.1238 - val_mae: 0.2394\n",
      "Epoch 1831/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0154 - mae: 0.0832 - val_loss: 0.1210 - val_mae: 0.2377\n",
      "Epoch 1832/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0152 - mae: 0.0827 - val_loss: 0.1221 - val_mae: 0.2367\n",
      "Epoch 1833/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0854 - val_loss: 0.1215 - val_mae: 0.2358\n",
      "Epoch 1834/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0846 - val_loss: 0.1220 - val_mae: 0.2382\n",
      "Epoch 1835/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0837 - val_loss: 0.1231 - val_mae: 0.2407\n",
      "Epoch 1836/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0856 - val_loss: 0.1229 - val_mae: 0.2382\n",
      "Epoch 1837/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0153 - mae: 0.0829 - val_loss: 0.1219 - val_mae: 0.2369\n",
      "Epoch 1838/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0846 - val_loss: 0.1231 - val_mae: 0.2398\n",
      "Epoch 1839/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0849 - val_loss: 0.1234 - val_mae: 0.2398\n",
      "Epoch 1840/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0841 - val_loss: 0.1217 - val_mae: 0.2370\n",
      "Epoch 1841/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0154 - mae: 0.0827 - val_loss: 0.1215 - val_mae: 0.2379\n",
      "Epoch 1842/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0838 - val_loss: 0.1230 - val_mae: 0.2387\n",
      "Epoch 1843/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0843 - val_loss: 0.1231 - val_mae: 0.2400\n",
      "Epoch 1844/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0154 - mae: 0.0831 - val_loss: 0.1220 - val_mae: 0.2395\n",
      "Epoch 1845/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0163 - mae: 0.0850 - val_loss: 0.1220 - val_mae: 0.2388\n",
      "Epoch 1846/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0836 - val_loss: 0.1220 - val_mae: 0.2354\n",
      "Epoch 1847/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0844 - val_loss: 0.1218 - val_mae: 0.2376\n",
      "Epoch 1848/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0155 - mae: 0.0831 - val_loss: 0.1224 - val_mae: 0.2371\n",
      "Epoch 1849/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0853 - val_loss: 0.1220 - val_mae: 0.2380\n",
      "Epoch 1850/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0841 - val_loss: 0.1223 - val_mae: 0.2398\n",
      "Epoch 1851/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0854 - val_loss: 0.1225 - val_mae: 0.2388\n",
      "Epoch 1852/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0849 - val_loss: 0.1222 - val_mae: 0.2378\n",
      "Epoch 1853/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0833 - val_loss: 0.1207 - val_mae: 0.2382\n",
      "Epoch 1854/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0845 - val_loss: 0.1228 - val_mae: 0.2378\n",
      "Epoch 1855/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0850 - val_loss: 0.1230 - val_mae: 0.2403\n",
      "Epoch 1856/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0850 - val_loss: 0.1224 - val_mae: 0.2384\n",
      "Epoch 1857/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0840 - val_loss: 0.1218 - val_mae: 0.2380\n",
      "Epoch 1858/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0857 - val_loss: 0.1223 - val_mae: 0.2392\n",
      "Epoch 1859/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0833 - val_loss: 0.1213 - val_mae: 0.2363\n",
      "Epoch 1860/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0846 - val_loss: 0.1210 - val_mae: 0.2361\n",
      "Epoch 1861/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0850 - val_loss: 0.1220 - val_mae: 0.2391\n",
      "Epoch 1862/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0838 - val_loss: 0.1215 - val_mae: 0.2386\n",
      "Epoch 1863/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0832 - val_loss: 0.1209 - val_mae: 0.2379\n",
      "Epoch 1864/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0852 - val_loss: 0.1220 - val_mae: 0.2391\n",
      "Epoch 1865/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0844 - val_loss: 0.1223 - val_mae: 0.2385\n",
      "Epoch 1866/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0848 - val_loss: 0.1234 - val_mae: 0.2425\n",
      "Epoch 1867/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0836 - val_loss: 0.1244 - val_mae: 0.2404\n",
      "Epoch 1868/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0835 - val_loss: 0.1218 - val_mae: 0.2377\n",
      "Epoch 1869/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0831 - val_loss: 0.1228 - val_mae: 0.2403\n",
      "Epoch 1870/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0862 - val_loss: 0.1220 - val_mae: 0.2378\n",
      "Epoch 1871/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0843 - val_loss: 0.1207 - val_mae: 0.2372\n",
      "Epoch 1872/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0858 - val_loss: 0.1216 - val_mae: 0.2373\n",
      "Epoch 1873/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0840 - val_loss: 0.1215 - val_mae: 0.2372\n",
      "Epoch 1874/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0151 - mae: 0.0819 - val_loss: 0.1200 - val_mae: 0.2371\n",
      "Epoch 1875/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0839 - val_loss: 0.1209 - val_mae: 0.2384\n",
      "Epoch 1876/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0849 - val_loss: 0.1220 - val_mae: 0.2362\n",
      "Epoch 1877/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0850 - val_loss: 0.1220 - val_mae: 0.2373\n",
      "Epoch 1878/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0836 - val_loss: 0.1236 - val_mae: 0.2401\n",
      "Epoch 1879/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0153 - mae: 0.0823 - val_loss: 0.1207 - val_mae: 0.2368\n",
      "Epoch 1880/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0840 - val_loss: 0.1200 - val_mae: 0.2361\n",
      "Epoch 1881/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0170 - mae: 0.0868 - val_loss: 0.1223 - val_mae: 0.2405\n",
      "Epoch 1882/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0833 - val_loss: 0.1215 - val_mae: 0.2392\n",
      "Epoch 1883/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0153 - mae: 0.0826 - val_loss: 0.1200 - val_mae: 0.2346\n",
      "Epoch 1884/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0154 - mae: 0.0834 - val_loss: 0.1224 - val_mae: 0.2402\n",
      "Epoch 1885/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0844 - val_loss: 0.1223 - val_mae: 0.2366\n",
      "Epoch 1886/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0167 - mae: 0.0858 - val_loss: 0.1214 - val_mae: 0.2380\n",
      "Epoch 1887/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0838 - val_loss: 0.1212 - val_mae: 0.2365\n",
      "Epoch 1888/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0848 - val_loss: 0.1221 - val_mae: 0.2377\n",
      "Epoch 1889/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0847 - val_loss: 0.1210 - val_mae: 0.2381\n",
      "Epoch 1890/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0155 - mae: 0.0835 - val_loss: 0.1217 - val_mae: 0.2390\n",
      "Epoch 1891/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0849 - val_loss: 0.1226 - val_mae: 0.2391\n",
      "Epoch 1892/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0155 - mae: 0.0832 - val_loss: 0.1226 - val_mae: 0.2387\n",
      "Epoch 1893/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0155 - mae: 0.0828 - val_loss: 0.1217 - val_mae: 0.2384\n",
      "Epoch 1894/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0845 - val_loss: 0.1220 - val_mae: 0.2375\n",
      "Epoch 1895/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0848 - val_loss: 0.1211 - val_mae: 0.2376\n",
      "Epoch 1896/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0154 - mae: 0.0826 - val_loss: 0.1215 - val_mae: 0.2382\n",
      "Epoch 1897/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0166 - mae: 0.0853 - val_loss: 0.1216 - val_mae: 0.2403\n",
      "Epoch 1898/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0851 - val_loss: 0.1227 - val_mae: 0.2401\n",
      "Epoch 1899/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0152 - mae: 0.0826 - val_loss: 0.1231 - val_mae: 0.2413\n",
      "Epoch 1900/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0155 - mae: 0.0833 - val_loss: 0.1237 - val_mae: 0.2409\n",
      "Epoch 1901/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0155 - mae: 0.0833 - val_loss: 0.1210 - val_mae: 0.2371\n",
      "Epoch 1902/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0152 - mae: 0.0822 - val_loss: 0.1227 - val_mae: 0.2388\n",
      "Epoch 1903/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0841 - val_loss: 0.1223 - val_mae: 0.2380\n",
      "Epoch 1904/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0843 - val_loss: 0.1218 - val_mae: 0.2371\n",
      "Epoch 1905/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0848 - val_loss: 0.1227 - val_mae: 0.2384\n",
      "Epoch 1906/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0152 - mae: 0.0835 - val_loss: 0.1211 - val_mae: 0.2381\n",
      "Epoch 1907/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0835 - val_loss: 0.1224 - val_mae: 0.2388\n",
      "Epoch 1908/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0836 - val_loss: 0.1223 - val_mae: 0.2390\n",
      "Epoch 1909/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0853 - val_loss: 0.1212 - val_mae: 0.2369\n",
      "Epoch 1910/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0154 - mae: 0.0825 - val_loss: 0.1219 - val_mae: 0.2410\n",
      "Epoch 1911/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0152 - mae: 0.0829 - val_loss: 0.1224 - val_mae: 0.2386\n",
      "Epoch 1912/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0854 - val_loss: 0.1203 - val_mae: 0.2375\n",
      "Epoch 1913/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0846 - val_loss: 0.1214 - val_mae: 0.2386\n",
      "Epoch 1914/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0821 - val_loss: 0.1214 - val_mae: 0.2388\n",
      "Epoch 1915/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0845 - val_loss: 0.1205 - val_mae: 0.2375\n",
      "Epoch 1916/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0844 - val_loss: 0.1230 - val_mae: 0.2403\n",
      "Epoch 1917/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0843 - val_loss: 0.1235 - val_mae: 0.2391\n",
      "Epoch 1918/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0830 - val_loss: 0.1219 - val_mae: 0.2391\n",
      "Epoch 1919/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0843 - val_loss: 0.1198 - val_mae: 0.2368\n",
      "Epoch 1920/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0838 - val_loss: 0.1214 - val_mae: 0.2383\n",
      "Epoch 1921/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0839 - val_loss: 0.1214 - val_mae: 0.2377\n",
      "Epoch 1922/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0839 - val_loss: 0.1220 - val_mae: 0.2381\n",
      "Epoch 1923/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0155 - mae: 0.0833 - val_loss: 0.1220 - val_mae: 0.2399\n",
      "Epoch 1924/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0841 - val_loss: 0.1218 - val_mae: 0.2390\n",
      "Epoch 1925/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0848 - val_loss: 0.1235 - val_mae: 0.2399\n",
      "Epoch 1926/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0154 - mae: 0.0824 - val_loss: 0.1214 - val_mae: 0.2380\n",
      "Epoch 1927/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0153 - mae: 0.0825 - val_loss: 0.1207 - val_mae: 0.2377\n",
      "Epoch 1928/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0838 - val_loss: 0.1214 - val_mae: 0.2381\n",
      "Epoch 1929/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0845 - val_loss: 0.1223 - val_mae: 0.2381\n",
      "Epoch 1930/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0841 - val_loss: 0.1218 - val_mae: 0.2357\n",
      "Epoch 1931/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0839 - val_loss: 0.1225 - val_mae: 0.2381\n",
      "Epoch 1932/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0153 - mae: 0.0827 - val_loss: 0.1209 - val_mae: 0.2368\n",
      "Epoch 1933/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0842 - val_loss: 0.1225 - val_mae: 0.2380\n",
      "Epoch 1934/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0163 - mae: 0.0851 - val_loss: 0.1232 - val_mae: 0.2406\n",
      "Epoch 1935/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0849 - val_loss: 0.1214 - val_mae: 0.2373\n",
      "Epoch 1936/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0841 - val_loss: 0.1215 - val_mae: 0.2393\n",
      "Epoch 1937/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0838 - val_loss: 0.1226 - val_mae: 0.2401\n",
      "Epoch 1938/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0150 - mae: 0.0817 - val_loss: 0.1210 - val_mae: 0.2381\n",
      "Epoch 1939/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0839 - val_loss: 0.1219 - val_mae: 0.2371\n",
      "Epoch 1940/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0841 - val_loss: 0.1207 - val_mae: 0.2360\n",
      "Epoch 1941/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0836 - val_loss: 0.1233 - val_mae: 0.2391\n",
      "Epoch 1942/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0154 - mae: 0.0828 - val_loss: 0.1241 - val_mae: 0.2387\n",
      "Epoch 1943/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0850 - val_loss: 0.1228 - val_mae: 0.2374\n",
      "Epoch 1944/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0845 - val_loss: 0.1216 - val_mae: 0.2389\n",
      "Epoch 1945/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0834 - val_loss: 0.1210 - val_mae: 0.2367\n",
      "Epoch 1946/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0847 - val_loss: 0.1225 - val_mae: 0.2381\n",
      "Epoch 1947/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0151 - mae: 0.0818 - val_loss: 0.1199 - val_mae: 0.2379\n",
      "Epoch 1948/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0831 - val_loss: 0.1216 - val_mae: 0.2375\n",
      "Epoch 1949/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0837 - val_loss: 0.1217 - val_mae: 0.2377\n",
      "Epoch 1950/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0839 - val_loss: 0.1222 - val_mae: 0.2390\n",
      "Epoch 1951/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0850 - val_loss: 0.1205 - val_mae: 0.2374\n",
      "Epoch 1952/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0835 - val_loss: 0.1212 - val_mae: 0.2383\n",
      "Epoch 1953/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0848 - val_loss: 0.1224 - val_mae: 0.2376\n",
      "Epoch 1954/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0153 - mae: 0.0828 - val_loss: 0.1217 - val_mae: 0.2383\n",
      "Epoch 1955/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0153 - mae: 0.0829 - val_loss: 0.1214 - val_mae: 0.2381\n",
      "Epoch 1956/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0848 - val_loss: 0.1215 - val_mae: 0.2372\n",
      "Epoch 1957/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0846 - val_loss: 0.1211 - val_mae: 0.2355\n",
      "Epoch 1958/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0830 - val_loss: 0.1205 - val_mae: 0.2357\n",
      "Epoch 1959/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0150 - mae: 0.0818 - val_loss: 0.1225 - val_mae: 0.2401\n",
      "Epoch 1960/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0852 - val_loss: 0.1220 - val_mae: 0.2372\n",
      "Epoch 1961/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0834 - val_loss: 0.1223 - val_mae: 0.2389\n",
      "Epoch 1962/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0843 - val_loss: 0.1235 - val_mae: 0.2393\n",
      "Epoch 1963/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0155 - mae: 0.0830 - val_loss: 0.1235 - val_mae: 0.2365\n",
      "Epoch 1964/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0846 - val_loss: 0.1219 - val_mae: 0.2370\n",
      "Epoch 1965/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0155 - mae: 0.0826 - val_loss: 0.1255 - val_mae: 0.2433\n",
      "Epoch 1966/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0152 - mae: 0.0825 - val_loss: 0.1243 - val_mae: 0.2393\n",
      "Epoch 1967/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0154 - mae: 0.0823 - val_loss: 0.1229 - val_mae: 0.2402\n",
      "Epoch 1968/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0839 - val_loss: 0.1216 - val_mae: 0.2390\n",
      "Epoch 1969/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0841 - val_loss: 0.1223 - val_mae: 0.2387\n",
      "Epoch 1970/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0836 - val_loss: 0.1215 - val_mae: 0.2377\n",
      "Epoch 1971/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0836 - val_loss: 0.1227 - val_mae: 0.2382\n",
      "Epoch 1972/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0837 - val_loss: 0.1216 - val_mae: 0.2369\n",
      "Epoch 1973/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0832 - val_loss: 0.1213 - val_mae: 0.2357\n",
      "Epoch 1974/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0150 - mae: 0.0817 - val_loss: 0.1210 - val_mae: 0.2380\n",
      "Epoch 1975/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0834 - val_loss: 0.1208 - val_mae: 0.2362\n",
      "Epoch 1976/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0831 - val_loss: 0.1221 - val_mae: 0.2374\n",
      "Epoch 1977/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0845 - val_loss: 0.1207 - val_mae: 0.2362\n",
      "Epoch 1978/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0837 - val_loss: 0.1222 - val_mae: 0.2392\n",
      "Epoch 1979/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0844 - val_loss: 0.1229 - val_mae: 0.2396\n",
      "Epoch 1980/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0154 - mae: 0.0832 - val_loss: 0.1201 - val_mae: 0.2364\n",
      "Epoch 1981/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0842 - val_loss: 0.1206 - val_mae: 0.2368\n",
      "Epoch 1982/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0153 - mae: 0.0824 - val_loss: 0.1228 - val_mae: 0.2376\n",
      "Epoch 1983/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0829 - val_loss: 0.1217 - val_mae: 0.2362\n",
      "Epoch 1984/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0852 - val_loss: 0.1210 - val_mae: 0.2377\n",
      "Epoch 1985/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0154 - mae: 0.0825 - val_loss: 0.1209 - val_mae: 0.2358\n",
      "Epoch 1986/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0159 - mae: 0.0840 - val_loss: 0.1219 - val_mae: 0.2373\n",
      "Epoch 1987/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0829 - val_loss: 0.1220 - val_mae: 0.2359\n",
      "Epoch 1988/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0155 - mae: 0.0833 - val_loss: 0.1225 - val_mae: 0.2373\n",
      "Epoch 1989/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0155 - mae: 0.0829 - val_loss: 0.1220 - val_mae: 0.2368\n",
      "Epoch 1990/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0162 - mae: 0.0840 - val_loss: 0.1229 - val_mae: 0.2421\n",
      "Epoch 1991/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0155 - mae: 0.0831 - val_loss: 0.1218 - val_mae: 0.2372\n",
      "Epoch 1992/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0160 - mae: 0.0839 - val_loss: 0.1213 - val_mae: 0.2383\n",
      "Epoch 1993/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0157 - mae: 0.0832 - val_loss: 0.1229 - val_mae: 0.2399\n",
      "Epoch 1994/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0164 - mae: 0.0852 - val_loss: 0.1220 - val_mae: 0.2378\n",
      "Epoch 1995/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0846 - val_loss: 0.1219 - val_mae: 0.2365\n",
      "Epoch 1996/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0161 - mae: 0.0847 - val_loss: 0.1238 - val_mae: 0.2404\n",
      "Epoch 1997/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0832 - val_loss: 0.1233 - val_mae: 0.2393\n",
      "Epoch 1998/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0156 - mae: 0.0834 - val_loss: 0.1236 - val_mae: 0.2394\n",
      "Epoch 1999/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0158 - mae: 0.0835 - val_loss: 0.1218 - val_mae: 0.2394\n",
      "Epoch 2000/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0154 - mae: 0.0829 - val_loss: 0.1224 - val_mae: 0.2394\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_save_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m train_start(model, X_train, X_val, y_train, y_val, \n\u001b[1;32m    137\u001b[0m                            epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m    138\u001b[0m plot_trained_model(trained_model, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 139\u001b[0m model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mmodel_save_folder\u001b[49m, \n\u001b[1;32m    140\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_save_folder' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "def load_catalogs(folder: str):\n",
    "    _img_name = []\n",
    "    _angle = []\n",
    "    _throttle = []\n",
    "\n",
    "    for _file in sorted(glob.glob(f\"{folder}/*.catalog\"),\n",
    "                        key=lambda x: [\n",
    "                            int(c) if c.isdigit()\n",
    "                            else c for c in re.split(r'(\\d+)', x)]):\n",
    "        with open(_file) as f:\n",
    "            for _line in f:\n",
    "                _img_name.append(_line.split()[7][1:-2])\n",
    "                _angle.append(float(_line.split()[9][0:-1]))\n",
    "                _throttle.append(float(_line.split()[13][0:-1]))\n",
    "\n",
    "    print(f'Image count: {len(_img_name)}')\n",
    "    return _img_name, _angle, _throttle\n",
    "\n",
    "\n",
    "def load_images(_img_name: list, folder: str):\n",
    "    _image = []\n",
    "    for i in range(len(_img_name)):\n",
    "        _img = cv2.imread(os.path.join(f\"{folder}/images\", _img_name[i]))\n",
    "        assert _img.shape == (224, 224, 3),\\\n",
    "            \"img %s has shape %r\" % (_img_name[i], _img.shape)\n",
    "        _image.append(_img)\n",
    "    return _image\n",
    "\n",
    "\n",
    "def data_preprocessing(_throttle, _angle, _image):\n",
    "    _throttle = np.array(_throttle)\n",
    "    _steering = np.array(_angle)\n",
    "    _train_img = np.array(_image)\n",
    "    _label = _steering\n",
    "    _cut_height = 80\n",
    "    _train_img_cut_orig = _train_img[:, _cut_height:224, :]\n",
    "    # _train_img_cut_gray = np.dot(_train_img_cut_orig[..., :3],\n",
    "    #                              [0.299, 0.587, 0.114])\n",
    "    _train_img_cut_gray = _train_img_cut_orig\n",
    "    return _train_img_cut_orig, _train_img_cut_gray, _label\n",
    "\n",
    "\n",
    "def train_split(_train_img_cut_orig, _train_img_cut_gray, _label):\n",
    "    _X_train, _X_val, _y_train, _y_val = train_test_split(\n",
    "        _train_img_cut_gray, _label,\n",
    "        test_size=0.15, random_state=42)\n",
    "    return _X_train, _X_val, _y_train, _y_val\n",
    "\n",
    "\n",
    "def build_fine_tuned_resnet50_model(input_shape):\n",
    "    base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    _x = base_model.output\n",
    "    _x = GlobalAveragePooling2D()(_x)\n",
    "    _x = Dense(1024, activation='relu')(_x)\n",
    "    _x = Dropout(0.5)(_x)\n",
    "    _outputs = Dense(1, activation='linear')(_x)\n",
    "\n",
    "    _model = Model(inputs=base_model.input, outputs=_outputs)\n",
    "    return _model\n",
    "\n",
    "\n",
    "def train_start(_model, _X_train, _X_val, _y_train, _y_val, \n",
    "                epochs: int=100, batch_size: int=16):\n",
    "    _optimizer = tf.optimizers.Adam(learning_rate=0.0001,\n",
    "                                    beta_1=0.9, beta_2=0.999)\n",
    "    _model.compile(optimizer=_optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
    "    _model.summary()\n",
    "    _trained_model = _model.fit(_X_train, _y_train,\n",
    "                                epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(_X_val, _y_val))\n",
    "    return _trained_model\n",
    "\n",
    "\n",
    "def plot_trained_model(_trained_model, \n",
    "                       show: bool=False,\n",
    "                       save: bool=True\n",
    "                       save_folder: str=''):\n",
    "    \n",
    "    history = _trained_model.history\n",
    "\n",
    "    plt.plot(history['loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(os.path,join(save_folder, f'Loss.png'),\n",
    "                    bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    plt.plot(history['mae'], label='Train MAE')\n",
    "    plt.plot(history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(save_folder, f'MAE.png'),\n",
    "                    bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"../data/data_0202\"\n",
    "    save_folder = f\"model/{time.ctime(time.time())}\"\n",
    "    # create save path\n",
    "    if not os.path.isdir(save_folder):\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    img_name, angle, throttle = load_catalogs(data_folder)\n",
    "    image = load_images(img_name, data_folder)\n",
    "    image = np.array(image)\n",
    "    train_img_cut_orig, train_img_cut_gray, label = data_preprocessing(\n",
    "        throttle, angle, image)\n",
    "    X_train, X_val, y_train, y_val = train_split(\n",
    "        train_img_cut_orig, train_img_cut_gray, label)\n",
    "\n",
    "    # Update input shape for ResNet50\n",
    "    model = build_fine_tuned_resnet50_model(input_shape=(144, 224, 3))\n",
    "    trained_model = train_start(model, X_train, X_val, y_train, y_val, \n",
    "                               epochs=2000)\n",
    "    plot_trained_model(trained_model, show=False, save=True)\n",
    "    model.save(os.path.join(save_folder, \n",
    "                            f\"model.h5\"))\n",
    "    print(f\"Save at: {save_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94afd989-f9cb-4e36-a9ff-f4682bfa2719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/tensorflow/2.7.0/lib/python3.9/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    }
   ],
   "source": [
    "model.save(os.path.join(save_folder, \n",
    "                            f\"model.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf46d26b-5bd8-41f2-a340-37f5e2317357",
   "metadata": {},
   "source": [
    "### Save at: Desktop/orange_iruchkin/jhong.dongyou/hipergator/vehicle/auto_middle_line/model/Sat Jul 20 05:26:34 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43dcd366-5961-4b74-9ad0-45c67b8554ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count: 10645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 01:39:04.258006: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-21 01:39:06.479035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78902 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:47:00.0, compute capability: 8.0\n",
      "2024-07-21 01:39:06.480810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78902 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:4e:00.0, compute capability: 8.0\n",
      "2024-07-21 01:39:06.482421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 78902 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:b7:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 144, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 150, 230, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 72, 112, 64)  9472        ['conv1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 72, 112, 64)  256         ['conv1_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 72, 112, 64)  0           ['conv1_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 74, 114, 64)  0           ['conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 36, 56, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 36, 56, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 36, 56, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 36, 56, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 36, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 36, 56, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 36, 56, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 36, 56, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 36, 56, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 36, 56, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 36, 56, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 36, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 36, 56, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 36, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 36, 56, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 36, 56, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 36, 56, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 36, 56, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 36, 56, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 36, 56, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 36, 56, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 18, 28, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 18, 28, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 18, 28, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 18, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 18, 28, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 18, 28, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 18, 28, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 18, 28, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 18, 28, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 18, 28, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 18, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 18, 28, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 18, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 18, 28, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 18, 28, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 18, 28, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 18, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 18, 28, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 18, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 18, 28, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 18, 28, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 18, 28, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 18, 28, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 18, 28, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 18, 28, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 18, 28, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 9, 14, 256)   131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 9, 14, 1024)  525312      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                                                  'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 9, 14, 256)   262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block1_out[0][0]',       \n",
      "                                                                  'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 9, 14, 256)   262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block2_out[0][0]',       \n",
      "                                                                  'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 9, 14, 256)   262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block3_out[0][0]',       \n",
      "                                                                  'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 9, 14, 256)   262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block4_out[0][0]',       \n",
      "                                                                  'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block5_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 9, 14, 256)   262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block5_out[0][0]',       \n",
      "                                                                  'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block6_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 5, 7, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 5, 7, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 5, 7, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 5, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 5, 7, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 5, 7, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 5, 7, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 5, 7, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 5, 7, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 5, 7, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 5, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 5, 7, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 5, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 5, 7, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 5, 7, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 5, 7, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 5, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 5, 7, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 5, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 5, 7, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         2098176     ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            1025        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25,686,913\n",
      "Trainable params: 2,099,201\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 01:39:11.039753: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18/566 [..............................] - ETA: 5s - loss: 1.1912 - mae: 0.8571 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 01:39:12.712056: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566/566 [==============================] - 11s 12ms/step - loss: 0.3657 - mae: 0.4608 - val_loss: 0.1886 - val_mae: 0.3335\n",
      "Epoch 2/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1928 - mae: 0.3413 - val_loss: 0.1599 - val_mae: 0.3016\n",
      "Epoch 3/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1749 - mae: 0.3239 - val_loss: 0.1710 - val_mae: 0.3158\n",
      "Epoch 4/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1679 - mae: 0.3179 - val_loss: 0.1734 - val_mae: 0.3200\n",
      "Epoch 5/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1620 - mae: 0.3118 - val_loss: 0.1540 - val_mae: 0.2976\n",
      "Epoch 6/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1579 - mae: 0.3067 - val_loss: 0.1512 - val_mae: 0.2976\n",
      "Epoch 7/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1544 - mae: 0.3032 - val_loss: 0.1555 - val_mae: 0.3002\n",
      "Epoch 8/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1524 - mae: 0.3025 - val_loss: 0.1473 - val_mae: 0.2904\n",
      "Epoch 9/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1500 - mae: 0.2983 - val_loss: 0.1405 - val_mae: 0.2779\n",
      "Epoch 10/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1450 - mae: 0.2922 - val_loss: 0.1488 - val_mae: 0.2994\n",
      "Epoch 11/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1430 - mae: 0.2915 - val_loss: 0.1440 - val_mae: 0.2860\n",
      "Epoch 12/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1414 - mae: 0.2888 - val_loss: 0.1492 - val_mae: 0.2946\n",
      "Epoch 13/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1373 - mae: 0.2839 - val_loss: 0.1350 - val_mae: 0.2729\n",
      "Epoch 14/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1340 - mae: 0.2802 - val_loss: 0.1362 - val_mae: 0.2792\n",
      "Epoch 15/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1342 - mae: 0.2802 - val_loss: 0.1462 - val_mae: 0.2871\n",
      "Epoch 16/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1285 - mae: 0.2733 - val_loss: 0.1354 - val_mae: 0.2745\n",
      "Epoch 17/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1277 - mae: 0.2725 - val_loss: 0.1330 - val_mae: 0.2681\n",
      "Epoch 18/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1265 - mae: 0.2688 - val_loss: 0.1339 - val_mae: 0.2637\n",
      "Epoch 19/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1231 - mae: 0.2667 - val_loss: 0.1350 - val_mae: 0.2715\n",
      "Epoch 20/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1217 - mae: 0.2644 - val_loss: 0.1304 - val_mae: 0.2647\n",
      "Epoch 21/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1204 - mae: 0.2627 - val_loss: 0.1311 - val_mae: 0.2670\n",
      "Epoch 22/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1157 - mae: 0.2579 - val_loss: 0.1337 - val_mae: 0.2699\n",
      "Epoch 23/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1146 - mae: 0.2559 - val_loss: 0.1344 - val_mae: 0.2681\n",
      "Epoch 24/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1141 - mae: 0.2562 - val_loss: 0.1304 - val_mae: 0.2646\n",
      "Epoch 25/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1110 - mae: 0.2519 - val_loss: 0.1325 - val_mae: 0.2677\n",
      "Epoch 26/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1083 - mae: 0.2486 - val_loss: 0.1330 - val_mae: 0.2627\n",
      "Epoch 27/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1058 - mae: 0.2455 - val_loss: 0.1263 - val_mae: 0.2613\n",
      "Epoch 28/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1045 - mae: 0.2439 - val_loss: 0.1279 - val_mae: 0.2586\n",
      "Epoch 29/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1044 - mae: 0.2429 - val_loss: 0.1279 - val_mae: 0.2666\n",
      "Epoch 30/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1020 - mae: 0.2407 - val_loss: 0.1232 - val_mae: 0.2552\n",
      "Epoch 31/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0994 - mae: 0.2386 - val_loss: 0.1299 - val_mae: 0.2604\n",
      "Epoch 32/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0997 - mae: 0.2373 - val_loss: 0.1265 - val_mae: 0.2524\n",
      "Epoch 33/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0971 - mae: 0.2336 - val_loss: 0.1247 - val_mae: 0.2555\n",
      "Epoch 34/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0965 - mae: 0.2330 - val_loss: 0.1292 - val_mae: 0.2648\n",
      "Epoch 35/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0932 - mae: 0.2301 - val_loss: 0.1261 - val_mae: 0.2611\n",
      "Epoch 36/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0916 - mae: 0.2265 - val_loss: 0.1319 - val_mae: 0.2636\n",
      "Epoch 37/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0900 - mae: 0.2263 - val_loss: 0.1233 - val_mae: 0.2542\n",
      "Epoch 38/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0881 - mae: 0.2234 - val_loss: 0.1256 - val_mae: 0.2512\n",
      "Epoch 39/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0898 - mae: 0.2244 - val_loss: 0.1230 - val_mae: 0.2565\n",
      "Epoch 40/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0846 - mae: 0.2182 - val_loss: 0.1263 - val_mae: 0.2586\n",
      "Epoch 41/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0855 - mae: 0.2185 - val_loss: 0.1216 - val_mae: 0.2486\n",
      "Epoch 42/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0831 - mae: 0.2154 - val_loss: 0.1341 - val_mae: 0.2668\n",
      "Epoch 43/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0835 - mae: 0.2166 - val_loss: 0.1227 - val_mae: 0.2503\n",
      "Epoch 44/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0819 - mae: 0.2149 - val_loss: 0.1328 - val_mae: 0.2620\n",
      "Epoch 45/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0808 - mae: 0.2132 - val_loss: 0.1202 - val_mae: 0.2472\n",
      "Epoch 46/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0780 - mae: 0.2084 - val_loss: 0.1236 - val_mae: 0.2520\n",
      "Epoch 47/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0780 - mae: 0.2094 - val_loss: 0.1393 - val_mae: 0.2760\n",
      "Epoch 48/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0767 - mae: 0.2073 - val_loss: 0.1243 - val_mae: 0.2546\n",
      "Epoch 49/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0763 - mae: 0.2069 - val_loss: 0.1250 - val_mae: 0.2515\n",
      "Epoch 50/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0747 - mae: 0.2035 - val_loss: 0.1260 - val_mae: 0.2522\n",
      "Epoch 51/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0740 - mae: 0.2038 - val_loss: 0.1241 - val_mae: 0.2564\n",
      "Epoch 52/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0748 - mae: 0.2042 - val_loss: 0.1236 - val_mae: 0.2540\n",
      "Epoch 53/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0712 - mae: 0.1987 - val_loss: 0.1257 - val_mae: 0.2598\n",
      "Epoch 54/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0716 - mae: 0.1998 - val_loss: 0.1239 - val_mae: 0.2512\n",
      "Epoch 55/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0706 - mae: 0.1975 - val_loss: 0.1212 - val_mae: 0.2463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/tensorflow/2.7.0/lib/python3.9/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save at: model/Sun Jul 21 01:38:49 2024\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "def load_catalogs(folder: str):\n",
    "    _img_name = []\n",
    "    _angle = []\n",
    "    _throttle = []\n",
    "\n",
    "    for _file in sorted(glob.glob(f\"{folder}/*.catalog\"),\n",
    "                        key=lambda x: [\n",
    "                            int(c) if c.isdigit()\n",
    "                            else c for c in re.split(r'(\\d+)', x)]):\n",
    "        with open(_file) as f:\n",
    "            for _line in f:\n",
    "                _img_name.append(_line.split()[7][1:-2])\n",
    "                _angle.append(float(_line.split()[9][0:-1]))\n",
    "                _throttle.append(float(_line.split()[13][0:-1]))\n",
    "\n",
    "    print(f'Image count: {len(_img_name)}')\n",
    "    return _img_name, _angle, _throttle\n",
    "\n",
    "\n",
    "def load_images(_img_name: list, folder: str):\n",
    "    _image = []\n",
    "    for i in range(len(_img_name)):\n",
    "        _img = cv2.imread(os.path.join(f\"{folder}/images\", _img_name[i]))\n",
    "        assert _img.shape == (224, 224, 3),\\\n",
    "            \"img %s has shape %r\" % (_img_name[i], _img.shape)\n",
    "        _image.append(_img)\n",
    "    return _image\n",
    "\n",
    "\n",
    "def data_preprocessing(_throttle, _angle, _image):\n",
    "    _throttle = np.array(_throttle)\n",
    "    _steering = np.array(_angle)\n",
    "    _train_img = np.array(_image)\n",
    "    _label = _steering\n",
    "    _cut_height = 80\n",
    "    _train_img_cut_orig = _train_img[:, _cut_height:224, :]\n",
    "    # _train_img_cut_gray = np.dot(_train_img_cut_orig[..., :3],\n",
    "    #                              [0.299, 0.587, 0.114])\n",
    "    _train_img_cut_gray = _train_img_cut_orig\n",
    "    return _train_img_cut_orig, _train_img_cut_gray, _label\n",
    "\n",
    "\n",
    "def train_split(_train_img_cut_orig, _train_img_cut_gray, _label):\n",
    "    _X_train, _X_val, _y_train, _y_val = train_test_split(\n",
    "        _train_img_cut_gray, _label,\n",
    "        test_size=0.15, random_state=42)\n",
    "    return _X_train, _X_val, _y_train, _y_val\n",
    "\n",
    "\n",
    "def build_fine_tuned_resnet50_model(input_shape):\n",
    "    base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    _x = base_model.output\n",
    "    _x = GlobalAveragePooling2D()(_x)\n",
    "    _x = Dense(1024, activation='relu')(_x)\n",
    "    _x = Dropout(0.5)(_x)\n",
    "    _outputs = Dense(1, activation='linear')(_x)\n",
    "\n",
    "    _model = Model(inputs=base_model.input, outputs=_outputs)\n",
    "    return _model\n",
    "\n",
    "\n",
    "def train_start(_model, _X_train, _X_val, _y_train, _y_val, \n",
    "                epochs: int=100, batch_size: int=16, patience: int=100):\n",
    "    _optimizer = tf.optimizers.Adam(learning_rate=0.0001,\n",
    "                                    beta_1=0.9, beta_2=0.999)\n",
    "    _model.compile(optimizer=_optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
    "    _model.summary()\n",
    "    \n",
    "    # Add EarlyStopping callback\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                  patience=patience, \n",
    "                                                  restore_best_weights=True)\n",
    "    \n",
    "    _trained_model = _model.fit(_X_train, _y_train,\n",
    "                                epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(_X_val, _y_val),\n",
    "                                callbacks=[early_stop])\n",
    "    return _trained_model\n",
    "\n",
    "\n",
    "def plot_trained_model(_trained_model, \n",
    "                       show: bool=False,\n",
    "                       save: bool=True,\n",
    "                       save_folder: str=''):\n",
    "    \n",
    "    history = _trained_model.history\n",
    "\n",
    "    plt.plot(history['loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(save_folder, f'Loss.png'),\n",
    "                    bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    plt.plot(history['mae'], label='Train MAE')\n",
    "    plt.plot(history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(save_folder, f'MAE.png'),\n",
    "                    bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"../data/data_0202\"\n",
    "    save_folder = f\"model/{time.ctime(time.time())}\"\n",
    "    # create save path\n",
    "    if not os.path.isdir(save_folder):\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    img_name, angle, throttle = load_catalogs(data_folder)\n",
    "    image = load_images(img_name, data_folder)\n",
    "    image = np.array(image)\n",
    "    train_img_cut_orig, train_img_cut_gray, label = data_preprocessing(\n",
    "        throttle, angle, image)\n",
    "    X_train, X_val, y_train, y_val = train_split(\n",
    "        train_img_cut_orig, train_img_cut_gray, label)\n",
    "\n",
    "    # Update input shape for ResNet50\n",
    "    model = build_fine_tuned_resnet50_model(input_shape=(144, 224, 3))\n",
    "    trained_model = train_start(model, X_train, X_val, y_train, y_val, \n",
    "                               epochs=2000)\n",
    "    plot_trained_model(trained_model, show=False, save=True, save_folder=save_folder)\n",
    "    model.save(os.path.join(save_folder, f\"model.h5\"))\n",
    "    print(f\"Save at: {save_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5d1a59-843a-438d-8929-a5cc6ad3a3fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count: 10645\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 144, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 150, 230, 3)  0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 72, 112, 64)  9472        ['conv1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 72, 112, 64)  256         ['conv1_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 72, 112, 64)  0           ['conv1_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 74, 114, 64)  0           ['conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 36, 56, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 36, 56, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 36, 56, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 36, 56, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 36, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 36, 56, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 36, 56, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 36, 56, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 36, 56, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 36, 56, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 36, 56, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 36, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 36, 56, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 36, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 36, 56, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 36, 56, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 36, 56, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 36, 56, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 36, 56, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 36, 56, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 36, 56, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 36, 56, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 18, 28, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 18, 28, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 18, 28, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 18, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 18, 28, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 18, 28, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 18, 28, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 18, 28, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 18, 28, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 18, 28, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 18, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 18, 28, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 18, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 18, 28, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 18, 28, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 18, 28, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 18, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 18, 28, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 18, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 18, 28, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 18, 28, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 18, 28, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 18, 28, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 18, 28, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 18, 28, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 18, 28, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 9, 14, 256)   131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 9, 14, 1024)  525312      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                                                  'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 9, 14, 256)   262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block1_out[0][0]',       \n",
      "                                                                  'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 9, 14, 256)   262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block2_out[0][0]',       \n",
      "                                                                  'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 9, 14, 256)   262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block3_out[0][0]',       \n",
      "                                                                  'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 9, 14, 256)   262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block4_out[0][0]',       \n",
      "                                                                  'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block5_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 9, 14, 256)   262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 9, 14, 256)   590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 9, 14, 256)  1024        ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 9, 14, 1024)  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 9, 14, 1024)  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 9, 14, 1024)  0           ['conv4_block5_out[0][0]',       \n",
      "                                                                  'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 9, 14, 1024)  0           ['conv4_block6_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 5, 7, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 5, 7, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 5, 7, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 5, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 5, 7, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 5, 7, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 5, 7, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 5, 7, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 5, 7, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 5, 7, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 5, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 5, 7, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 5, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 5, 7, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 5, 7, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 5, 7, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 5, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 5, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 5, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 5, 7, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 5, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 5, 7, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1024)         2098176     ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 1024)         0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            1025        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 25,686,913\n",
      "Trainable params: 2,099,201\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2000\n",
      "566/566 [==============================] - 9s 13ms/step - loss: 0.4252 - mae: 0.4895 - val_loss: 0.1928 - val_mae: 0.3479\n",
      "Epoch 2/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1915 - mae: 0.3428 - val_loss: 0.1735 - val_mae: 0.3243\n",
      "Epoch 3/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1776 - mae: 0.3300 - val_loss: 0.1652 - val_mae: 0.3095\n",
      "Epoch 4/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1688 - mae: 0.3195 - val_loss: 0.1528 - val_mae: 0.3010\n",
      "Epoch 5/2000\n",
      "566/566 [==============================] - 7s 12ms/step - loss: 0.1623 - mae: 0.3135 - val_loss: 0.1526 - val_mae: 0.2939\n",
      "Epoch 6/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1610 - mae: 0.3096 - val_loss: 0.1540 - val_mae: 0.3015\n",
      "Epoch 7/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1556 - mae: 0.3059 - val_loss: 0.1583 - val_mae: 0.3086\n",
      "Epoch 8/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1550 - mae: 0.3053 - val_loss: 0.1477 - val_mae: 0.2918\n",
      "Epoch 9/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1515 - mae: 0.3010 - val_loss: 0.1624 - val_mae: 0.3095\n",
      "Epoch 10/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1482 - mae: 0.2971 - val_loss: 0.1440 - val_mae: 0.2825\n",
      "Epoch 11/2000\n",
      "566/566 [==============================] - 7s 12ms/step - loss: 0.1427 - mae: 0.2909 - val_loss: 0.1405 - val_mae: 0.2829\n",
      "Epoch 12/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1406 - mae: 0.2873 - val_loss: 0.1478 - val_mae: 0.2887\n",
      "Epoch 13/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1373 - mae: 0.2829 - val_loss: 0.1425 - val_mae: 0.2812\n",
      "Epoch 14/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1355 - mae: 0.2801 - val_loss: 0.1427 - val_mae: 0.2805\n",
      "Epoch 15/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1338 - mae: 0.2782 - val_loss: 0.1414 - val_mae: 0.2802\n",
      "Epoch 16/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1322 - mae: 0.2772 - val_loss: 0.1371 - val_mae: 0.2726\n",
      "Epoch 17/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1289 - mae: 0.2718 - val_loss: 0.1345 - val_mae: 0.2752\n",
      "Epoch 18/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1267 - mae: 0.2701 - val_loss: 0.1338 - val_mae: 0.2717\n",
      "Epoch 19/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1246 - mae: 0.2690 - val_loss: 0.1414 - val_mae: 0.2808\n",
      "Epoch 20/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1229 - mae: 0.2660 - val_loss: 0.1310 - val_mae: 0.2713\n",
      "Epoch 21/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1192 - mae: 0.2634 - val_loss: 0.1327 - val_mae: 0.2650\n",
      "Epoch 22/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1183 - mae: 0.2622 - val_loss: 0.1377 - val_mae: 0.2769\n",
      "Epoch 23/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1170 - mae: 0.2592 - val_loss: 0.1309 - val_mae: 0.2647\n",
      "Epoch 24/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1149 - mae: 0.2560 - val_loss: 0.1272 - val_mae: 0.2649\n",
      "Epoch 25/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1126 - mae: 0.2536 - val_loss: 0.1288 - val_mae: 0.2605\n",
      "Epoch 26/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1107 - mae: 0.2504 - val_loss: 0.1303 - val_mae: 0.2624\n",
      "Epoch 27/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1075 - mae: 0.2478 - val_loss: 0.1321 - val_mae: 0.2642\n",
      "Epoch 28/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1081 - mae: 0.2483 - val_loss: 0.1314 - val_mae: 0.2684\n",
      "Epoch 29/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1054 - mae: 0.2446 - val_loss: 0.1303 - val_mae: 0.2622\n",
      "Epoch 30/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1039 - mae: 0.2418 - val_loss: 0.1275 - val_mae: 0.2613\n",
      "Epoch 31/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.1038 - mae: 0.2422 - val_loss: 0.1248 - val_mae: 0.2580\n",
      "Epoch 32/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.1007 - mae: 0.2389 - val_loss: 0.1283 - val_mae: 0.2550\n",
      "Epoch 33/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0990 - mae: 0.2364 - val_loss: 0.1315 - val_mae: 0.2637\n",
      "Epoch 34/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0969 - mae: 0.2344 - val_loss: 0.1251 - val_mae: 0.2519\n",
      "Epoch 35/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0957 - mae: 0.2333 - val_loss: 0.1254 - val_mae: 0.2590\n",
      "Epoch 36/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0932 - mae: 0.2286 - val_loss: 0.1246 - val_mae: 0.2524\n",
      "Epoch 37/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0932 - mae: 0.2294 - val_loss: 0.1230 - val_mae: 0.2561\n",
      "Epoch 38/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0890 - mae: 0.2239 - val_loss: 0.1224 - val_mae: 0.2569\n",
      "Epoch 39/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0878 - mae: 0.2233 - val_loss: 0.1259 - val_mae: 0.2558\n",
      "Epoch 40/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0863 - mae: 0.2199 - val_loss: 0.1237 - val_mae: 0.2551\n",
      "Epoch 41/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0868 - mae: 0.2217 - val_loss: 0.1259 - val_mae: 0.2547\n",
      "Epoch 42/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0858 - mae: 0.2190 - val_loss: 0.1282 - val_mae: 0.2616\n",
      "Epoch 43/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0843 - mae: 0.2163 - val_loss: 0.1237 - val_mae: 0.2512\n",
      "Epoch 44/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0815 - mae: 0.2145 - val_loss: 0.1259 - val_mae: 0.2496\n",
      "Epoch 45/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0815 - mae: 0.2139 - val_loss: 0.1329 - val_mae: 0.2669\n",
      "Epoch 46/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0801 - mae: 0.2115 - val_loss: 0.1258 - val_mae: 0.2540\n",
      "Epoch 47/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0797 - mae: 0.2100 - val_loss: 0.1241 - val_mae: 0.2552\n",
      "Epoch 48/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0777 - mae: 0.2072 - val_loss: 0.1221 - val_mae: 0.2530\n",
      "Epoch 49/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0779 - mae: 0.2090 - val_loss: 0.1242 - val_mae: 0.2526\n",
      "Epoch 50/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0755 - mae: 0.2061 - val_loss: 0.1257 - val_mae: 0.2572\n",
      "Epoch 51/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0754 - mae: 0.2065 - val_loss: 0.1230 - val_mae: 0.2510\n",
      "Epoch 52/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0745 - mae: 0.2030 - val_loss: 0.1253 - val_mae: 0.2521\n",
      "Epoch 53/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0718 - mae: 0.2003 - val_loss: 0.1236 - val_mae: 0.2501\n",
      "Epoch 54/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0708 - mae: 0.1977 - val_loss: 0.1216 - val_mae: 0.2463\n",
      "Epoch 55/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0707 - mae: 0.1978 - val_loss: 0.1214 - val_mae: 0.2499\n",
      "Epoch 56/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0709 - mae: 0.1991 - val_loss: 0.1202 - val_mae: 0.2448\n",
      "Epoch 57/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0673 - mae: 0.1951 - val_loss: 0.1208 - val_mae: 0.2524\n",
      "Epoch 58/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0680 - mae: 0.1941 - val_loss: 0.1244 - val_mae: 0.2492\n",
      "Epoch 59/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0664 - mae: 0.1925 - val_loss: 0.1272 - val_mae: 0.2530\n",
      "Epoch 60/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0656 - mae: 0.1920 - val_loss: 0.1187 - val_mae: 0.2432\n",
      "Epoch 61/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0657 - mae: 0.1910 - val_loss: 0.1232 - val_mae: 0.2486\n",
      "Epoch 62/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0655 - mae: 0.1916 - val_loss: 0.1309 - val_mae: 0.2592\n",
      "Epoch 63/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0644 - mae: 0.1889 - val_loss: 0.1192 - val_mae: 0.2476\n",
      "Epoch 64/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0621 - mae: 0.1863 - val_loss: 0.1267 - val_mae: 0.2549\n",
      "Epoch 65/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0619 - mae: 0.1852 - val_loss: 0.1200 - val_mae: 0.2477\n",
      "Epoch 66/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0611 - mae: 0.1831 - val_loss: 0.1194 - val_mae: 0.2480\n",
      "Epoch 67/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0596 - mae: 0.1816 - val_loss: 0.1250 - val_mae: 0.2493\n",
      "Epoch 68/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0610 - mae: 0.1832 - val_loss: 0.1233 - val_mae: 0.2475\n",
      "Epoch 69/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0602 - mae: 0.1829 - val_loss: 0.1220 - val_mae: 0.2438\n",
      "Epoch 70/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0600 - mae: 0.1823 - val_loss: 0.1266 - val_mae: 0.2551\n",
      "Epoch 71/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0583 - mae: 0.1790 - val_loss: 0.1237 - val_mae: 0.2472\n",
      "Epoch 72/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0578 - mae: 0.1795 - val_loss: 0.1196 - val_mae: 0.2473\n",
      "Epoch 73/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0584 - mae: 0.1796 - val_loss: 0.1198 - val_mae: 0.2425\n",
      "Epoch 74/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0550 - mae: 0.1749 - val_loss: 0.1209 - val_mae: 0.2459\n",
      "Epoch 75/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0551 - mae: 0.1745 - val_loss: 0.1209 - val_mae: 0.2453\n",
      "Epoch 76/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0558 - mae: 0.1753 - val_loss: 0.1206 - val_mae: 0.2487\n",
      "Epoch 77/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0548 - mae: 0.1737 - val_loss: 0.1215 - val_mae: 0.2446\n",
      "Epoch 78/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0525 - mae: 0.1707 - val_loss: 0.1246 - val_mae: 0.2473\n",
      "Epoch 79/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0542 - mae: 0.1731 - val_loss: 0.1238 - val_mae: 0.2504\n",
      "Epoch 80/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0528 - mae: 0.1711 - val_loss: 0.1200 - val_mae: 0.2428\n",
      "Epoch 81/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0529 - mae: 0.1706 - val_loss: 0.1204 - val_mae: 0.2419\n",
      "Epoch 82/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0525 - mae: 0.1710 - val_loss: 0.1208 - val_mae: 0.2433\n",
      "Epoch 83/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0536 - mae: 0.1719 - val_loss: 0.1207 - val_mae: 0.2417\n",
      "Epoch 84/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0515 - mae: 0.1681 - val_loss: 0.1220 - val_mae: 0.2462\n",
      "Epoch 85/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0528 - mae: 0.1718 - val_loss: 0.1189 - val_mae: 0.2449\n",
      "Epoch 86/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0518 - mae: 0.1690 - val_loss: 0.1256 - val_mae: 0.2496\n",
      "Epoch 87/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0504 - mae: 0.1664 - val_loss: 0.1236 - val_mae: 0.2473\n",
      "Epoch 88/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0512 - mae: 0.1670 - val_loss: 0.1221 - val_mae: 0.2471\n",
      "Epoch 89/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0505 - mae: 0.1668 - val_loss: 0.1220 - val_mae: 0.2457\n",
      "Epoch 90/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0495 - mae: 0.1660 - val_loss: 0.1235 - val_mae: 0.2466\n",
      "Epoch 91/2000\n",
      "566/566 [==============================] - 7s 12ms/step - loss: 0.0490 - mae: 0.1641 - val_loss: 0.1186 - val_mae: 0.2410\n",
      "Epoch 92/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0491 - mae: 0.1639 - val_loss: 0.1201 - val_mae: 0.2451\n",
      "Epoch 93/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0497 - mae: 0.1644 - val_loss: 0.1251 - val_mae: 0.2502\n",
      "Epoch 94/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0467 - mae: 0.1600 - val_loss: 0.1265 - val_mae: 0.2472\n",
      "Epoch 95/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0500 - mae: 0.1655 - val_loss: 0.1226 - val_mae: 0.2445\n",
      "Epoch 96/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0475 - mae: 0.1621 - val_loss: 0.1207 - val_mae: 0.2426\n",
      "Epoch 97/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0463 - mae: 0.1593 - val_loss: 0.1220 - val_mae: 0.2439\n",
      "Epoch 98/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0459 - mae: 0.1579 - val_loss: 0.1258 - val_mae: 0.2501\n",
      "Epoch 99/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0458 - mae: 0.1590 - val_loss: 0.1201 - val_mae: 0.2447\n",
      "Epoch 100/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0467 - mae: 0.1595 - val_loss: 0.1220 - val_mae: 0.2441\n",
      "Epoch 101/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0459 - mae: 0.1578 - val_loss: 0.1207 - val_mae: 0.2446\n",
      "Epoch 102/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0449 - mae: 0.1567 - val_loss: 0.1214 - val_mae: 0.2448\n",
      "Epoch 103/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0448 - mae: 0.1561 - val_loss: 0.1212 - val_mae: 0.2453\n",
      "Epoch 104/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0459 - mae: 0.1590 - val_loss: 0.1210 - val_mae: 0.2439\n",
      "Epoch 105/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0448 - mae: 0.1566 - val_loss: 0.1200 - val_mae: 0.2439\n",
      "Epoch 106/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0445 - mae: 0.1556 - val_loss: 0.1241 - val_mae: 0.2523\n",
      "Epoch 107/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0476 - mae: 0.1611 - val_loss: 0.1210 - val_mae: 0.2459\n",
      "Epoch 108/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0440 - mae: 0.1551 - val_loss: 0.1249 - val_mae: 0.2466\n",
      "Epoch 109/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0439 - mae: 0.1547 - val_loss: 0.1254 - val_mae: 0.2492\n",
      "Epoch 110/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0433 - mae: 0.1537 - val_loss: 0.1219 - val_mae: 0.2423\n",
      "Epoch 111/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0434 - mae: 0.1529 - val_loss: 0.1224 - val_mae: 0.2429\n",
      "Epoch 112/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0434 - mae: 0.1543 - val_loss: 0.1221 - val_mae: 0.2406\n",
      "Epoch 113/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0427 - mae: 0.1524 - val_loss: 0.1206 - val_mae: 0.2400\n",
      "Epoch 114/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0422 - mae: 0.1505 - val_loss: 0.1198 - val_mae: 0.2437\n",
      "Epoch 115/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0430 - mae: 0.1524 - val_loss: 0.1190 - val_mae: 0.2437\n",
      "Epoch 116/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0428 - mae: 0.1528 - val_loss: 0.1214 - val_mae: 0.2400\n",
      "Epoch 117/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0430 - mae: 0.1529 - val_loss: 0.1270 - val_mae: 0.2520\n",
      "Epoch 118/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0408 - mae: 0.1483 - val_loss: 0.1175 - val_mae: 0.2359\n",
      "Epoch 119/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0421 - mae: 0.1510 - val_loss: 0.1282 - val_mae: 0.2534\n",
      "Epoch 120/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0429 - mae: 0.1523 - val_loss: 0.1231 - val_mae: 0.2439\n",
      "Epoch 121/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0427 - mae: 0.1513 - val_loss: 0.1194 - val_mae: 0.2428\n",
      "Epoch 122/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0429 - mae: 0.1520 - val_loss: 0.1188 - val_mae: 0.2412\n",
      "Epoch 123/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0429 - mae: 0.1521 - val_loss: 0.1243 - val_mae: 0.2502\n",
      "Epoch 124/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0418 - mae: 0.1493 - val_loss: 0.1233 - val_mae: 0.2458\n",
      "Epoch 125/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0420 - mae: 0.1509 - val_loss: 0.1202 - val_mae: 0.2415\n",
      "Epoch 126/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0398 - mae: 0.1464 - val_loss: 0.1275 - val_mae: 0.2486\n",
      "Epoch 127/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0413 - mae: 0.1488 - val_loss: 0.1232 - val_mae: 0.2419\n",
      "Epoch 128/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0408 - mae: 0.1490 - val_loss: 0.1204 - val_mae: 0.2432\n",
      "Epoch 129/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0393 - mae: 0.1457 - val_loss: 0.1195 - val_mae: 0.2392\n",
      "Epoch 130/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0396 - mae: 0.1452 - val_loss: 0.1237 - val_mae: 0.2452\n",
      "Epoch 131/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0413 - mae: 0.1489 - val_loss: 0.1205 - val_mae: 0.2428\n",
      "Epoch 132/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0419 - mae: 0.1498 - val_loss: 0.1209 - val_mae: 0.2423\n",
      "Epoch 133/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0404 - mae: 0.1474 - val_loss: 0.1244 - val_mae: 0.2463\n",
      "Epoch 134/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0389 - mae: 0.1444 - val_loss: 0.1234 - val_mae: 0.2416\n",
      "Epoch 135/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0394 - mae: 0.1455 - val_loss: 0.1213 - val_mae: 0.2413\n",
      "Epoch 136/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0391 - mae: 0.1443 - val_loss: 0.1229 - val_mae: 0.2444\n",
      "Epoch 137/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0390 - mae: 0.1465 - val_loss: 0.1217 - val_mae: 0.2424\n",
      "Epoch 138/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0400 - mae: 0.1453 - val_loss: 0.1200 - val_mae: 0.2430\n",
      "Epoch 139/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0382 - mae: 0.1435 - val_loss: 0.1176 - val_mae: 0.2401\n",
      "Epoch 140/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0383 - mae: 0.1433 - val_loss: 0.1192 - val_mae: 0.2414\n",
      "Epoch 141/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0390 - mae: 0.1452 - val_loss: 0.1237 - val_mae: 0.2420\n",
      "Epoch 142/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0381 - mae: 0.1424 - val_loss: 0.1202 - val_mae: 0.2393\n",
      "Epoch 143/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0388 - mae: 0.1442 - val_loss: 0.1225 - val_mae: 0.2462\n",
      "Epoch 144/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0379 - mae: 0.1422 - val_loss: 0.1222 - val_mae: 0.2414\n",
      "Epoch 145/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0385 - mae: 0.1436 - val_loss: 0.1177 - val_mae: 0.2395\n",
      "Epoch 146/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0381 - mae: 0.1423 - val_loss: 0.1216 - val_mae: 0.2391\n",
      "Epoch 147/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0373 - mae: 0.1411 - val_loss: 0.1222 - val_mae: 0.2430\n",
      "Epoch 148/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0365 - mae: 0.1404 - val_loss: 0.1213 - val_mae: 0.2433\n",
      "Epoch 149/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0385 - mae: 0.1433 - val_loss: 0.1194 - val_mae: 0.2423\n",
      "Epoch 150/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0373 - mae: 0.1420 - val_loss: 0.1217 - val_mae: 0.2421\n",
      "Epoch 151/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0375 - mae: 0.1407 - val_loss: 0.1198 - val_mae: 0.2413\n",
      "Epoch 152/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0369 - mae: 0.1403 - val_loss: 0.1181 - val_mae: 0.2387\n",
      "Epoch 153/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0373 - mae: 0.1417 - val_loss: 0.1197 - val_mae: 0.2446\n",
      "Epoch 154/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0370 - mae: 0.1413 - val_loss: 0.1181 - val_mae: 0.2377\n",
      "Epoch 155/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0365 - mae: 0.1389 - val_loss: 0.1243 - val_mae: 0.2455\n",
      "Epoch 156/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0376 - mae: 0.1409 - val_loss: 0.1205 - val_mae: 0.2406\n",
      "Epoch 157/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0357 - mae: 0.1386 - val_loss: 0.1201 - val_mae: 0.2413\n",
      "Epoch 158/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0364 - mae: 0.1393 - val_loss: 0.1217 - val_mae: 0.2440\n",
      "Epoch 159/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0372 - mae: 0.1413 - val_loss: 0.1205 - val_mae: 0.2391\n",
      "Epoch 160/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0358 - mae: 0.1376 - val_loss: 0.1177 - val_mae: 0.2380\n",
      "Epoch 161/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0355 - mae: 0.1388 - val_loss: 0.1192 - val_mae: 0.2405\n",
      "Epoch 162/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0378 - mae: 0.1418 - val_loss: 0.1226 - val_mae: 0.2437\n",
      "Epoch 163/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0381 - mae: 0.1424 - val_loss: 0.1217 - val_mae: 0.2377\n",
      "Epoch 164/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0354 - mae: 0.1377 - val_loss: 0.1199 - val_mae: 0.2410\n",
      "Epoch 165/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0368 - mae: 0.1403 - val_loss: 0.1212 - val_mae: 0.2397\n",
      "Epoch 166/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0338 - mae: 0.1346 - val_loss: 0.1223 - val_mae: 0.2458\n",
      "Epoch 167/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0355 - mae: 0.1370 - val_loss: 0.1179 - val_mae: 0.2388\n",
      "Epoch 168/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0348 - mae: 0.1361 - val_loss: 0.1181 - val_mae: 0.2385\n",
      "Epoch 169/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0370 - mae: 0.1404 - val_loss: 0.1199 - val_mae: 0.2401\n",
      "Epoch 170/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0354 - mae: 0.1365 - val_loss: 0.1182 - val_mae: 0.2412\n",
      "Epoch 171/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0347 - mae: 0.1351 - val_loss: 0.1207 - val_mae: 0.2363\n",
      "Epoch 172/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0361 - mae: 0.1378 - val_loss: 0.1186 - val_mae: 0.2388\n",
      "Epoch 173/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0358 - mae: 0.1380 - val_loss: 0.1247 - val_mae: 0.2438\n",
      "Epoch 174/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0339 - mae: 0.1338 - val_loss: 0.1218 - val_mae: 0.2407\n",
      "Epoch 175/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0342 - mae: 0.1341 - val_loss: 0.1197 - val_mae: 0.2396\n",
      "Epoch 176/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0344 - mae: 0.1346 - val_loss: 0.1189 - val_mae: 0.2373\n",
      "Epoch 177/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0352 - mae: 0.1372 - val_loss: 0.1190 - val_mae: 0.2410\n",
      "Epoch 178/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0348 - mae: 0.1359 - val_loss: 0.1194 - val_mae: 0.2397\n",
      "Epoch 179/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0337 - mae: 0.1333 - val_loss: 0.1196 - val_mae: 0.2379\n",
      "Epoch 180/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0341 - mae: 0.1357 - val_loss: 0.1224 - val_mae: 0.2415\n",
      "Epoch 181/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0352 - mae: 0.1363 - val_loss: 0.1238 - val_mae: 0.2462\n",
      "Epoch 182/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0340 - mae: 0.1342 - val_loss: 0.1204 - val_mae: 0.2381\n",
      "Epoch 183/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0350 - mae: 0.1358 - val_loss: 0.1184 - val_mae: 0.2376\n",
      "Epoch 184/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0345 - mae: 0.1348 - val_loss: 0.1265 - val_mae: 0.2479\n",
      "Epoch 185/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0344 - mae: 0.1335 - val_loss: 0.1229 - val_mae: 0.2461\n",
      "Epoch 186/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0327 - mae: 0.1316 - val_loss: 0.1213 - val_mae: 0.2391\n",
      "Epoch 187/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0339 - mae: 0.1339 - val_loss: 0.1208 - val_mae: 0.2429\n",
      "Epoch 188/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0335 - mae: 0.1335 - val_loss: 0.1217 - val_mae: 0.2401\n",
      "Epoch 189/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0338 - mae: 0.1336 - val_loss: 0.1201 - val_mae: 0.2406\n",
      "Epoch 190/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0346 - mae: 0.1359 - val_loss: 0.1196 - val_mae: 0.2404\n",
      "Epoch 191/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0344 - mae: 0.1349 - val_loss: 0.1198 - val_mae: 0.2388\n",
      "Epoch 192/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0338 - mae: 0.1335 - val_loss: 0.1224 - val_mae: 0.2386\n",
      "Epoch 193/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0326 - mae: 0.1304 - val_loss: 0.1232 - val_mae: 0.2413\n",
      "Epoch 194/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0341 - mae: 0.1347 - val_loss: 0.1202 - val_mae: 0.2387\n",
      "Epoch 195/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0330 - mae: 0.1315 - val_loss: 0.1207 - val_mae: 0.2366\n",
      "Epoch 196/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0333 - mae: 0.1330 - val_loss: 0.1233 - val_mae: 0.2411\n",
      "Epoch 197/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0341 - mae: 0.1346 - val_loss: 0.1185 - val_mae: 0.2368\n",
      "Epoch 198/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0342 - mae: 0.1347 - val_loss: 0.1189 - val_mae: 0.2395\n",
      "Epoch 199/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0329 - mae: 0.1319 - val_loss: 0.1202 - val_mae: 0.2403\n",
      "Epoch 200/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0333 - mae: 0.1319 - val_loss: 0.1182 - val_mae: 0.2368\n",
      "Epoch 201/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0332 - mae: 0.1322 - val_loss: 0.1174 - val_mae: 0.2338\n",
      "Epoch 202/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0321 - mae: 0.1292 - val_loss: 0.1214 - val_mae: 0.2438\n",
      "Epoch 203/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0331 - mae: 0.1313 - val_loss: 0.1171 - val_mae: 0.2367\n",
      "Epoch 204/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0334 - mae: 0.1318 - val_loss: 0.1206 - val_mae: 0.2416\n",
      "Epoch 205/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0327 - mae: 0.1313 - val_loss: 0.1200 - val_mae: 0.2408\n",
      "Epoch 206/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0313 - mae: 0.1289 - val_loss: 0.1220 - val_mae: 0.2430\n",
      "Epoch 207/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0324 - mae: 0.1302 - val_loss: 0.1199 - val_mae: 0.2364\n",
      "Epoch 208/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0329 - mae: 0.1309 - val_loss: 0.1198 - val_mae: 0.2379\n",
      "Epoch 209/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0343 - mae: 0.1342 - val_loss: 0.1232 - val_mae: 0.2442\n",
      "Epoch 210/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0316 - mae: 0.1288 - val_loss: 0.1218 - val_mae: 0.2418\n",
      "Epoch 211/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0330 - mae: 0.1319 - val_loss: 0.1188 - val_mae: 0.2374\n",
      "Epoch 212/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0332 - mae: 0.1317 - val_loss: 0.1194 - val_mae: 0.2378\n",
      "Epoch 213/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0322 - mae: 0.1301 - val_loss: 0.1184 - val_mae: 0.2363\n",
      "Epoch 214/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0312 - mae: 0.1281 - val_loss: 0.1216 - val_mae: 0.2403\n",
      "Epoch 215/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0327 - mae: 0.1303 - val_loss: 0.1199 - val_mae: 0.2395\n",
      "Epoch 216/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0319 - mae: 0.1294 - val_loss: 0.1233 - val_mae: 0.2454\n",
      "Epoch 217/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0319 - mae: 0.1299 - val_loss: 0.1213 - val_mae: 0.2372\n",
      "Epoch 218/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0319 - mae: 0.1293 - val_loss: 0.1223 - val_mae: 0.2422\n",
      "Epoch 219/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0315 - mae: 0.1277 - val_loss: 0.1214 - val_mae: 0.2442\n",
      "Epoch 220/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0321 - mae: 0.1292 - val_loss: 0.1194 - val_mae: 0.2398\n",
      "Epoch 221/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0318 - mae: 0.1293 - val_loss: 0.1208 - val_mae: 0.2397\n",
      "Epoch 222/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0320 - mae: 0.1300 - val_loss: 0.1204 - val_mae: 0.2382\n",
      "Epoch 223/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0304 - mae: 0.1261 - val_loss: 0.1197 - val_mae: 0.2397\n",
      "Epoch 224/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0311 - mae: 0.1278 - val_loss: 0.1196 - val_mae: 0.2417\n",
      "Epoch 225/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0310 - mae: 0.1274 - val_loss: 0.1225 - val_mae: 0.2424\n",
      "Epoch 226/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0315 - mae: 0.1277 - val_loss: 0.1190 - val_mae: 0.2379\n",
      "Epoch 227/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0302 - mae: 0.1260 - val_loss: 0.1223 - val_mae: 0.2436\n",
      "Epoch 228/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0315 - mae: 0.1287 - val_loss: 0.1194 - val_mae: 0.2421\n",
      "Epoch 229/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0301 - mae: 0.1257 - val_loss: 0.1200 - val_mae: 0.2348\n",
      "Epoch 230/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0310 - mae: 0.1273 - val_loss: 0.1213 - val_mae: 0.2390\n",
      "Epoch 231/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0317 - mae: 0.1283 - val_loss: 0.1228 - val_mae: 0.2474\n",
      "Epoch 232/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0319 - mae: 0.1293 - val_loss: 0.1216 - val_mae: 0.2374\n",
      "Epoch 233/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0303 - mae: 0.1254 - val_loss: 0.1211 - val_mae: 0.2418\n",
      "Epoch 234/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0309 - mae: 0.1261 - val_loss: 0.1223 - val_mae: 0.2439\n",
      "Epoch 235/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0297 - mae: 0.1244 - val_loss: 0.1190 - val_mae: 0.2365\n",
      "Epoch 236/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0303 - mae: 0.1262 - val_loss: 0.1199 - val_mae: 0.2367\n",
      "Epoch 237/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0300 - mae: 0.1250 - val_loss: 0.1209 - val_mae: 0.2398\n",
      "Epoch 238/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0310 - mae: 0.1273 - val_loss: 0.1198 - val_mae: 0.2361\n",
      "Epoch 239/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0316 - mae: 0.1271 - val_loss: 0.1194 - val_mae: 0.2394\n",
      "Epoch 240/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0313 - mae: 0.1287 - val_loss: 0.1214 - val_mae: 0.2391\n",
      "Epoch 241/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0314 - mae: 0.1276 - val_loss: 0.1201 - val_mae: 0.2431\n",
      "Epoch 242/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0311 - mae: 0.1264 - val_loss: 0.1223 - val_mae: 0.2392\n",
      "Epoch 243/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0297 - mae: 0.1243 - val_loss: 0.1227 - val_mae: 0.2393\n",
      "Epoch 244/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0302 - mae: 0.1261 - val_loss: 0.1210 - val_mae: 0.2378\n",
      "Epoch 245/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0296 - mae: 0.1226 - val_loss: 0.1179 - val_mae: 0.2351\n",
      "Epoch 246/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0307 - mae: 0.1266 - val_loss: 0.1194 - val_mae: 0.2392\n",
      "Epoch 247/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0299 - mae: 0.1243 - val_loss: 0.1199 - val_mae: 0.2365\n",
      "Epoch 248/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0303 - mae: 0.1253 - val_loss: 0.1219 - val_mae: 0.2394\n",
      "Epoch 249/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0293 - mae: 0.1231 - val_loss: 0.1208 - val_mae: 0.2365\n",
      "Epoch 250/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0296 - mae: 0.1234 - val_loss: 0.1175 - val_mae: 0.2360\n",
      "Epoch 251/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0301 - mae: 0.1253 - val_loss: 0.1210 - val_mae: 0.2384\n",
      "Epoch 252/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0304 - mae: 0.1254 - val_loss: 0.1200 - val_mae: 0.2388\n",
      "Epoch 253/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0292 - mae: 0.1224 - val_loss: 0.1212 - val_mae: 0.2394\n",
      "Epoch 254/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0296 - mae: 0.1238 - val_loss: 0.1201 - val_mae: 0.2396\n",
      "Epoch 255/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0299 - mae: 0.1246 - val_loss: 0.1196 - val_mae: 0.2375\n",
      "Epoch 256/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0305 - mae: 0.1250 - val_loss: 0.1196 - val_mae: 0.2392\n",
      "Epoch 257/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0290 - mae: 0.1224 - val_loss: 0.1202 - val_mae: 0.2407\n",
      "Epoch 258/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0293 - mae: 0.1231 - val_loss: 0.1186 - val_mae: 0.2371\n",
      "Epoch 259/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0293 - mae: 0.1228 - val_loss: 0.1179 - val_mae: 0.2368\n",
      "Epoch 260/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0293 - mae: 0.1234 - val_loss: 0.1184 - val_mae: 0.2354\n",
      "Epoch 261/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0294 - mae: 0.1240 - val_loss: 0.1224 - val_mae: 0.2408\n",
      "Epoch 262/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0310 - mae: 0.1263 - val_loss: 0.1249 - val_mae: 0.2464\n",
      "Epoch 263/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0300 - mae: 0.1248 - val_loss: 0.1190 - val_mae: 0.2371\n",
      "Epoch 264/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0290 - mae: 0.1231 - val_loss: 0.1182 - val_mae: 0.2364\n",
      "Epoch 265/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0300 - mae: 0.1249 - val_loss: 0.1201 - val_mae: 0.2403\n",
      "Epoch 266/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0294 - mae: 0.1233 - val_loss: 0.1235 - val_mae: 0.2443\n",
      "Epoch 267/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0295 - mae: 0.1232 - val_loss: 0.1215 - val_mae: 0.2389\n",
      "Epoch 268/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0291 - mae: 0.1233 - val_loss: 0.1183 - val_mae: 0.2360\n",
      "Epoch 269/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0296 - mae: 0.1238 - val_loss: 0.1196 - val_mae: 0.2376\n",
      "Epoch 270/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0293 - mae: 0.1225 - val_loss: 0.1193 - val_mae: 0.2364\n",
      "Epoch 271/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0293 - mae: 0.1232 - val_loss: 0.1203 - val_mae: 0.2378\n",
      "Epoch 272/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0294 - mae: 0.1235 - val_loss: 0.1210 - val_mae: 0.2393\n",
      "Epoch 273/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0290 - mae: 0.1227 - val_loss: 0.1218 - val_mae: 0.2421\n",
      "Epoch 274/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0286 - mae: 0.1220 - val_loss: 0.1194 - val_mae: 0.2385\n",
      "Epoch 275/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0294 - mae: 0.1233 - val_loss: 0.1175 - val_mae: 0.2347\n",
      "Epoch 276/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0289 - mae: 0.1224 - val_loss: 0.1207 - val_mae: 0.2406\n",
      "Epoch 277/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0290 - mae: 0.1222 - val_loss: 0.1199 - val_mae: 0.2375\n",
      "Epoch 278/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0293 - mae: 0.1231 - val_loss: 0.1219 - val_mae: 0.2408\n",
      "Epoch 279/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0290 - mae: 0.1221 - val_loss: 0.1216 - val_mae: 0.2412\n",
      "Epoch 280/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0296 - mae: 0.1228 - val_loss: 0.1217 - val_mae: 0.2379\n",
      "Epoch 281/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0295 - mae: 0.1234 - val_loss: 0.1173 - val_mae: 0.2360\n",
      "Epoch 282/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0287 - mae: 0.1217 - val_loss: 0.1221 - val_mae: 0.2426\n",
      "Epoch 283/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0296 - mae: 0.1230 - val_loss: 0.1201 - val_mae: 0.2392\n",
      "Epoch 284/2000\n",
      "566/566 [==============================] - 6s 11ms/step - loss: 0.0284 - mae: 0.1223 - val_loss: 0.1209 - val_mae: 0.2392\n",
      "Epoch 285/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0280 - mae: 0.1204 - val_loss: 0.1208 - val_mae: 0.2391\n",
      "Epoch 286/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0286 - mae: 0.1214 - val_loss: 0.1229 - val_mae: 0.2416\n",
      "Epoch 287/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0290 - mae: 0.1223 - val_loss: 0.1200 - val_mae: 0.2356\n",
      "Epoch 288/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0300 - mae: 0.1236 - val_loss: 0.1190 - val_mae: 0.2375\n",
      "Epoch 289/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0289 - mae: 0.1210 - val_loss: 0.1219 - val_mae: 0.2392\n",
      "Epoch 290/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0286 - mae: 0.1207 - val_loss: 0.1183 - val_mae: 0.2352\n",
      "Epoch 291/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0282 - mae: 0.1203 - val_loss: 0.1220 - val_mae: 0.2441\n",
      "Epoch 292/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0285 - mae: 0.1205 - val_loss: 0.1225 - val_mae: 0.2402\n",
      "Epoch 293/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0285 - mae: 0.1221 - val_loss: 0.1192 - val_mae: 0.2386\n",
      "Epoch 294/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0288 - mae: 0.1220 - val_loss: 0.1175 - val_mae: 0.2382\n",
      "Epoch 295/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0277 - mae: 0.1191 - val_loss: 0.1177 - val_mae: 0.2352\n",
      "Epoch 296/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0286 - mae: 0.1213 - val_loss: 0.1196 - val_mae: 0.2355\n",
      "Epoch 297/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0297 - mae: 0.1224 - val_loss: 0.1218 - val_mae: 0.2430\n",
      "Epoch 298/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0268 - mae: 0.1181 - val_loss: 0.1199 - val_mae: 0.2363\n",
      "Epoch 299/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0282 - mae: 0.1207 - val_loss: 0.1196 - val_mae: 0.2418\n",
      "Epoch 300/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0278 - mae: 0.1205 - val_loss: 0.1208 - val_mae: 0.2385\n",
      "Epoch 301/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0281 - mae: 0.1208 - val_loss: 0.1228 - val_mae: 0.2447\n",
      "Epoch 302/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0288 - mae: 0.1213 - val_loss: 0.1191 - val_mae: 0.2411\n",
      "Epoch 303/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0271 - mae: 0.1190 - val_loss: 0.1176 - val_mae: 0.2352\n",
      "Models saved at: model/Sun Jul 21 01:52:12 2024\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "def load_catalogs(folder: str):\n",
    "    _img_name = []\n",
    "    _angle = []\n",
    "    _throttle = []\n",
    "\n",
    "    for _file in sorted(glob.glob(f\"{folder}/*.catalog\"),\n",
    "                        key=lambda x: [\n",
    "                            int(c) if c.isdigit()\n",
    "                            else c for c in re.split(r'(\\d+)', x)]):\n",
    "        with open(_file) as f:\n",
    "            for _line in f:\n",
    "                _img_name.append(_line.split()[7][1:-2])\n",
    "                _angle.append(float(_line.split()[9][0:-1]))\n",
    "                _throttle.append(float(_line.split()[13][0:-1]))\n",
    "\n",
    "    print(f'Image count: {len(_img_name)}')\n",
    "    return _img_name, _angle, _throttle\n",
    "\n",
    "\n",
    "def load_images(_img_name: list, folder: str):\n",
    "    _image = []\n",
    "    for i in range(len(_img_name)):\n",
    "        _img = cv2.imread(os.path.join(f\"{folder}/images\", _img_name[i]))\n",
    "        assert _img.shape == (224, 224, 3),\\\n",
    "            \"img %s has shape %r\" % (_img_name[i], _img.shape)\n",
    "        _image.append(_img)\n",
    "    return _image\n",
    "\n",
    "\n",
    "def data_preprocessing(_throttle, _angle, _image):\n",
    "    _throttle = np.array(_throttle)\n",
    "    _steering = np.array(_angle)\n",
    "    _train_img = np.array(_image)\n",
    "    _label = _steering\n",
    "    _cut_height = 80\n",
    "    _train_img_cut_orig = _train_img[:, _cut_height:224, :]\n",
    "    # _train_img_cut_gray = np.dot(_train_img_cut_orig[..., :3],\n",
    "    #                              [0.299, 0.587, 0.114])\n",
    "    _train_img_cut_gray = _train_img_cut_orig\n",
    "    return _train_img_cut_orig, _train_img_cut_gray, _label\n",
    "\n",
    "\n",
    "def train_split(_train_img_cut_orig, _train_img_cut_gray, _label):\n",
    "    _X_train, _X_val, _y_train, _y_val = train_test_split(\n",
    "        _train_img_cut_gray, _label,\n",
    "        test_size=0.15, random_state=42)\n",
    "    return _X_train, _X_val, _y_train, _y_val\n",
    "\n",
    "\n",
    "def build_fine_tuned_resnet50_model(input_shape):\n",
    "    base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    _x = base_model.output\n",
    "    _x = GlobalAveragePooling2D()(_x)\n",
    "    _x = Dense(1024, activation='relu')(_x)\n",
    "    _x = Dropout(0.5)(_x)\n",
    "    _outputs = Dense(1, activation='linear')(_x)\n",
    "\n",
    "    _model = Model(inputs=base_model.input, outputs=_outputs)\n",
    "    return _model\n",
    "\n",
    "\n",
    "def train_start(_model, _X_train, _X_val, _y_train, _y_val, \n",
    "                epochs: int=100, batch_size: int=16, patience: int=100, save_folder: str=''):\n",
    "    _optimizer = tf.optimizers.Adam(learning_rate=0.0001,\n",
    "                                    beta_1=0.9, beta_2=0.999)\n",
    "    _model.compile(optimizer=_optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
    "    _model.summary()\n",
    "    \n",
    "    # Add EarlyStopping callback\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                  patience=patience, \n",
    "                                                  restore_best_weights=True)\n",
    "    \n",
    "    # Add ModelCheckpoint callback to save the best model\n",
    "    best_model_path = os.path.join(save_folder, \"best_model.h5\")\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_path, \n",
    "                                                          monitor='val_loss', \n",
    "                                                          save_best_only=True)\n",
    "    \n",
    "    _trained_model = _model.fit(_X_train, _y_train,\n",
    "                                epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(_X_val, _y_val),\n",
    "                                callbacks=[early_stop, model_checkpoint])\n",
    "    return _trained_model\n",
    "\n",
    "\n",
    "def plot_trained_model(_trained_model, \n",
    "                       show: bool=False,\n",
    "                       save: bool=True,\n",
    "                       save_folder: str=''):\n",
    "    \n",
    "    history = _trained_model.history\n",
    "\n",
    "    plt.plot(history['loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(save_folder, f'Loss.png'),\n",
    "                    bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    plt.plot(history['mae'], label='Train MAE')\n",
    "    plt.plot(history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(save_folder, f'MAE.png'),\n",
    "                    bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"../data/data_0202\"\n",
    "    save_folder = f\"model/{time.ctime(time.time())}\"\n",
    "    # create save path\n",
    "    if not os.path.isdir(save_folder):\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    img_name, angle, throttle = load_catalogs(data_folder)\n",
    "    image = load_images(img_name, data_folder)\n",
    "    image = np.array(image)\n",
    "    train_img_cut_orig, train_img_cut_gray, label = data_preprocessing(\n",
    "        throttle, angle, image)\n",
    "    X_train, X_val, y_train, y_val = train_split(\n",
    "        train_img_cut_orig, train_img_cut_gray, label)\n",
    "\n",
    "    # Update input shape for ResNet50\n",
    "    model = build_fine_tuned_resnet50_model(input_shape=(144, 224, 3))\n",
    "    trained_model = train_start(model, X_train, X_val, y_train, y_val, \n",
    "                               epochs=2000, save_folder=save_folder)\n",
    "    plot_trained_model(trained_model, show=False, save=True, save_folder=save_folder)\n",
    "    \n",
    "    # Save the last model\n",
    "    model.save(os.path.join(save_folder, \"last_model.h5\"))\n",
    "    \n",
    "    print(f\"Models saved at: {save_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a7323-c5bf-4003-8fc7-b74e90921c7d",
   "metadata": {},
   "source": [
    "### Models saved at: model/Sun Jul 21 01:52:12 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4a8348-f166-4724-bd78-30e39180b5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.7.0",
   "language": "python",
   "name": "tensorflow-2.7.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
