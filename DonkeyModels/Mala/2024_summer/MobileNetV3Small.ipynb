{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43dcd366-5961-4b74-9ad0-45c67b8554ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count: 10645\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 06:54:29.399776: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-21 06:54:31.520861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78902 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:47:00.0, compute capability: 8.0\n",
      "2024-07-21 06:54:31.522647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78902 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:4e:00.0, compute capability: 8.0\n",
      "2024-07-21 06:54:31.524275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 78902 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:b7:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_small_224_1.0_float_no_top.h5\n",
      "6701056/6698480 [==============================] - 0s 0us/step\n",
      "6709248/6698480 [==============================] - 0s 0us/step\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 144, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rescaling (Rescaling)          (None, 144, 224, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " Conv (Conv2D)                  (None, 72, 112, 16)  432         ['rescaling[0][0]']              \n",
      "                                                                                                  \n",
      " Conv/BatchNorm (BatchNormaliza  (None, 72, 112, 16)  64         ['Conv[0][0]']                   \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 72, 112, 16)  0          ['Conv/BatchNorm[0][0]']         \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 72, 112, 16)  0           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 72, 112, 16)  0           ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 72, 112, 16)  0           ['Conv/BatchNorm[0][0]',         \n",
      "                                                                  'tf.math.multiply[0][0]']       \n",
      "                                                                                                  \n",
      " expanded_conv/depthwise/pad (Z  (None, 73, 113, 16)  0          ['multiply[0][0]']               \n",
      " eroPadding2D)                                                                                    \n",
      "                                                                                                  \n",
      " expanded_conv/depthwise (Depth  (None, 36, 56, 16)  144         ['expanded_conv/depthwise/pad[0][\n",
      " wiseConv2D)                                                     0]']                             \n",
      "                                                                                                  \n",
      " expanded_conv/depthwise/BatchN  (None, 36, 56, 16)  64          ['expanded_conv/depthwise[0][0]']\n",
      " orm (BatchNormalization)                                                                         \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 36, 56, 16)   0           ['expanded_conv/depthwise/BatchNo\n",
      "                                                                 rm[0][0]']                       \n",
      "                                                                                                  \n",
      " expanded_conv/squeeze_excite/A  (None, 1, 1, 16)    0           ['re_lu_1[0][0]']                \n",
      " vgPool (GlobalAveragePooling2D                                                                   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " expanded_conv/squeeze_excite/C  (None, 1, 1, 8)     136         ['expanded_conv/squeeze_excite/Av\n",
      " onv (Conv2D)                                                    gPool[0][0]']                    \n",
      "                                                                                                  \n",
      " expanded_conv/squeeze_excite/R  (None, 1, 1, 8)     0           ['expanded_conv/squeeze_excite/Co\n",
      " elu (ReLU)                                                      nv[0][0]']                       \n",
      "                                                                                                  \n",
      " expanded_conv/squeeze_excite/C  (None, 1, 1, 16)    144         ['expanded_conv/squeeze_excite/Re\n",
      " onv_1 (Conv2D)                                                  lu[0][0]']                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 1, 1, 16)    0           ['expanded_conv/squeeze_excite/Co\n",
      " mbda)                                                           nv_1[0][0]']                     \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 1, 1, 16)     0           ['tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 1, 1, 16)    0           ['re_lu_2[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " expanded_conv/squeeze_excite/M  (None, 36, 56, 16)  0           ['re_lu_1[0][0]',                \n",
      " ul (Multiply)                                                    'tf.math.multiply_1[0][0]']     \n",
      "                                                                                                  \n",
      " expanded_conv/project (Conv2D)  (None, 36, 56, 16)  256         ['expanded_conv/squeeze_excite/Mu\n",
      "                                                                 l[0][0]']                        \n",
      "                                                                                                  \n",
      " expanded_conv/project/BatchNor  (None, 36, 56, 16)  64          ['expanded_conv/project[0][0]']  \n",
      " m (BatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " expanded_conv_1/expand (Conv2D  (None, 36, 56, 72)  1152        ['expanded_conv/project/BatchNorm\n",
      " )                                                               [0][0]']                         \n",
      "                                                                                                  \n",
      " expanded_conv_1/expand/BatchNo  (None, 36, 56, 72)  288         ['expanded_conv_1/expand[0][0]'] \n",
      " rm (BatchNormalization)                                                                          \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 36, 56, 72)   0           ['expanded_conv_1/expand/BatchNor\n",
      "                                                                 m[0][0]']                        \n",
      "                                                                                                  \n",
      " expanded_conv_1/depthwise/pad   (None, 37, 57, 72)  0           ['re_lu_3[0][0]']                \n",
      " (ZeroPadding2D)                                                                                  \n",
      "                                                                                                  \n",
      " expanded_conv_1/depthwise (Dep  (None, 18, 28, 72)  648         ['expanded_conv_1/depthwise/pad[0\n",
      " thwiseConv2D)                                                   ][0]']                           \n",
      "                                                                                                  \n",
      " expanded_conv_1/depthwise/Batc  (None, 18, 28, 72)  288         ['expanded_conv_1/depthwise[0][0]\n",
      " hNorm (BatchNormalization)                                      ']                               \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 18, 28, 72)   0           ['expanded_conv_1/depthwise/Batch\n",
      "                                                                 Norm[0][0]']                     \n",
      "                                                                                                  \n",
      " expanded_conv_1/project (Conv2  (None, 18, 28, 24)  1728        ['re_lu_4[0][0]']                \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " expanded_conv_1/project/BatchN  (None, 18, 28, 24)  96          ['expanded_conv_1/project[0][0]']\n",
      " orm (BatchNormalization)                                                                         \n",
      "                                                                                                  \n",
      " expanded_conv_2/expand (Conv2D  (None, 18, 28, 88)  2112        ['expanded_conv_1/project/BatchNo\n",
      " )                                                               rm[0][0]']                       \n",
      "                                                                                                  \n",
      " expanded_conv_2/expand/BatchNo  (None, 18, 28, 88)  352         ['expanded_conv_2/expand[0][0]'] \n",
      " rm (BatchNormalization)                                                                          \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 18, 28, 88)   0           ['expanded_conv_2/expand/BatchNor\n",
      "                                                                 m[0][0]']                        \n",
      "                                                                                                  \n",
      " expanded_conv_2/depthwise (Dep  (None, 18, 28, 88)  792         ['re_lu_5[0][0]']                \n",
      " thwiseConv2D)                                                                                    \n",
      "                                                                                                  \n",
      " expanded_conv_2/depthwise/Batc  (None, 18, 28, 88)  352         ['expanded_conv_2/depthwise[0][0]\n",
      " hNorm (BatchNormalization)                                      ']                               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 18, 28, 88)   0           ['expanded_conv_2/depthwise/Batch\n",
      "                                                                 Norm[0][0]']                     \n",
      "                                                                                                  \n",
      " expanded_conv_2/project (Conv2  (None, 18, 28, 24)  2112        ['re_lu_6[0][0]']                \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " expanded_conv_2/project/BatchN  (None, 18, 28, 24)  96          ['expanded_conv_2/project[0][0]']\n",
      " orm (BatchNormalization)                                                                         \n",
      "                                                                                                  \n",
      " expanded_conv_2/Add (Add)      (None, 18, 28, 24)   0           ['expanded_conv_1/project/BatchNo\n",
      "                                                                 rm[0][0]',                       \n",
      "                                                                  'expanded_conv_2/project/BatchNo\n",
      "                                                                 rm[0][0]']                       \n",
      "                                                                                                  \n",
      " expanded_conv_3/expand (Conv2D  (None, 18, 28, 96)  2304        ['expanded_conv_2/Add[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " expanded_conv_3/expand/BatchNo  (None, 18, 28, 96)  384         ['expanded_conv_3/expand[0][0]'] \n",
      " rm (BatchNormalization)                                                                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 18, 28, 96)  0           ['expanded_conv_3/expand/BatchNor\n",
      " mbda)                                                           m[0][0]']                        \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 18, 28, 96)   0           ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLambda  (None, 18, 28, 96)  0           ['re_lu_7[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 18, 28, 96)   0           ['expanded_conv_3/expand/BatchNor\n",
      "                                                                 m[0][0]',                        \n",
      "                                                                  'tf.math.multiply_2[0][0]']     \n",
      "                                                                                                  \n",
      " expanded_conv_3/depthwise/pad   (None, 21, 31, 96)  0           ['multiply_1[0][0]']             \n",
      " (ZeroPadding2D)                                                                                  \n",
      "                                                                                                  \n",
      " expanded_conv_3/depthwise (Dep  (None, 9, 14, 96)   2400        ['expanded_conv_3/depthwise/pad[0\n",
      " thwiseConv2D)                                                   ][0]']                           \n",
      "                                                                                                  \n",
      " expanded_conv_3/depthwise/Batc  (None, 9, 14, 96)   384         ['expanded_conv_3/depthwise[0][0]\n",
      " hNorm (BatchNormalization)                                      ']                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 9, 14, 96)   0           ['expanded_conv_3/depthwise/Batch\n",
      " mbda)                                                           Norm[0][0]']                     \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)                 (None, 9, 14, 96)    0           ['tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLambda  (None, 9, 14, 96)   0           ['re_lu_8[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)          (None, 9, 14, 96)    0           ['expanded_conv_3/depthwise/Batch\n",
      "                                                                 Norm[0][0]',                     \n",
      "                                                                  'tf.math.multiply_3[0][0]']     \n",
      "                                                                                                  \n",
      " expanded_conv_3/squeeze_excite  (None, 1, 1, 96)    0           ['multiply_2[0][0]']             \n",
      " /AvgPool (GlobalAveragePooling                                                                   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " expanded_conv_3/squeeze_excite  (None, 1, 1, 24)    2328        ['expanded_conv_3/squeeze_excite/\n",
      " /Conv (Conv2D)                                                  AvgPool[0][0]']                  \n",
      "                                                                                                  \n",
      " expanded_conv_3/squeeze_excite  (None, 1, 1, 24)    0           ['expanded_conv_3/squeeze_excite/\n",
      " /Relu (ReLU)                                                    Conv[0][0]']                     \n",
      "                                                                                                  \n",
      " expanded_conv_3/squeeze_excite  (None, 1, 1, 96)    2400        ['expanded_conv_3/squeeze_excite/\n",
      " /Conv_1 (Conv2D)                                                Relu[0][0]']                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 1, 1, 96)    0           ['expanded_conv_3/squeeze_excite/\n",
      " mbda)                                                           Conv_1[0][0]']                   \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)                 (None, 1, 1, 96)     0           ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLambda  (None, 1, 1, 96)    0           ['re_lu_9[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " expanded_conv_3/squeeze_excite  (None, 9, 14, 96)   0           ['multiply_2[0][0]',             \n",
      " /Mul (Multiply)                                                  'tf.math.multiply_4[0][0]']     \n",
      "                                                                                                  \n",
      " expanded_conv_3/project (Conv2  (None, 9, 14, 40)   3840        ['expanded_conv_3/squeeze_excite/\n",
      " D)                                                              Mul[0][0]']                      \n",
      "                                                                                                  \n",
      " expanded_conv_3/project/BatchN  (None, 9, 14, 40)   160         ['expanded_conv_3/project[0][0]']\n",
      " orm (BatchNormalization)                                                                         \n",
      "                                                                                                  \n",
      " expanded_conv_4/expand (Conv2D  (None, 9, 14, 240)  9600        ['expanded_conv_3/project/BatchNo\n",
      " )                                                               rm[0][0]']                       \n",
      "                                                                                                  \n",
      " expanded_conv_4/expand/BatchNo  (None, 9, 14, 240)  960         ['expanded_conv_4/expand[0][0]'] \n",
      " rm (BatchNormalization)                                                                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 9, 14, 240)  0           ['expanded_conv_4/expand/BatchNor\n",
      " mbda)                                                           m[0][0]']                        \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)                (None, 9, 14, 240)   0           ['tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.multiply_5 (TFOpLambda  (None, 9, 14, 240)  0           ['re_lu_10[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)          (None, 9, 14, 240)   0           ['expanded_conv_4/expand/BatchNor\n",
      "                                                                 m[0][0]',                        \n",
      "                                                                  'tf.math.multiply_5[0][0]']     \n",
      "                                                                                                  \n",
      " expanded_conv_4/depthwise (Dep  (None, 9, 14, 240)  6000        ['multiply_3[0][0]']             \n",
      " thwiseConv2D)                                                                                    \n",
      "                                                                                                  \n",
      " expanded_conv_4/depthwise/Batc  (None, 9, 14, 240)  960         ['expanded_conv_4/depthwise[0][0]\n",
      " hNorm (BatchNormalization)                                      ']                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 9, 14, 240)  0           ['expanded_conv_4/depthwise/Batch\n",
      " mbda)                                                           Norm[0][0]']                     \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)                (None, 9, 14, 240)   0           ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLambda  (None, 9, 14, 240)  0           ['re_lu_11[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " multiply_4 (Multiply)          (None, 9, 14, 240)   0           ['expanded_conv_4/depthwise/Batch\n",
      "                                                                 Norm[0][0]',                     \n",
      "                                                                  'tf.math.multiply_6[0][0]']     \n",
      "                                                                                                  \n",
      " expanded_conv_4/squeeze_excite  (None, 1, 1, 240)   0           ['multiply_4[0][0]']             \n",
      " /AvgPool (GlobalAveragePooling                                                                   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " expanded_conv_4/squeeze_excite  (None, 1, 1, 64)    15424       ['expanded_conv_4/squeeze_excite/\n",
      " /Conv (Conv2D)                                                  AvgPool[0][0]']                  \n",
      "                                                                                                  \n",
      " expanded_conv_4/squeeze_excite  (None, 1, 1, 64)    0           ['expanded_conv_4/squeeze_excite/\n",
      " /Relu (ReLU)                                                    Conv[0][0]']                     \n",
      "                                                                                                  \n",
      " expanded_conv_4/squeeze_excite  (None, 1, 1, 240)   15600       ['expanded_conv_4/squeeze_excite/\n",
      " /Conv_1 (Conv2D)                                                Relu[0][0]']                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 1, 1, 240)   0           ['expanded_conv_4/squeeze_excite/\n",
      " mbda)                                                           Conv_1[0][0]']                   \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)                (None, 1, 1, 240)    0           ['tf.__operators__.add_7[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.multiply_7 (TFOpLambda  (None, 1, 1, 240)   0           ['re_lu_12[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " expanded_conv_4/squeeze_excite  (None, 9, 14, 240)  0           ['multiply_4[0][0]',             \n",
      " /Mul (Multiply)                                                  'tf.math.multiply_7[0][0]']     \n",
      "                                                                                                  \n",
      " expanded_conv_4/project (Conv2  (None, 9, 14, 40)   9600        ['expanded_conv_4/squeeze_excite/\n",
      " D)                                                              Mul[0][0]']                      \n",
      "                                                                                                  \n",
      " expanded_conv_4/project/BatchN  (None, 9, 14, 40)   160         ['expanded_conv_4/project[0][0]']\n",
      " orm (BatchNormalization)                                                                         \n",
      "                                                                                                  \n",
      " expanded_conv_4/Add (Add)      (None, 9, 14, 40)    0           ['expanded_conv_3/project/BatchNo\n",
      "                                                                 rm[0][0]',                       \n",
      "                                                                  'expanded_conv_4/project/BatchNo\n",
      "                                                                 rm[0][0]']                       \n",
      "                                                                                                  \n",
      " expanded_conv_5/expand (Conv2D  (None, 9, 14, 240)  9600        ['expanded_conv_4/Add[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " expanded_conv_5/expand/BatchNo  (None, 9, 14, 240)  960         ['expanded_conv_5/expand[0][0]'] \n",
      " rm (BatchNormalization)                                                                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TFOpLa  (None, 9, 14, 240)  0           ['expanded_conv_5/expand/BatchNor\n",
      " mbda)                                                           m[0][0]']                        \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)                (None, 9, 14, 240)   0           ['tf.__operators__.add_8[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.multiply_8 (TFOpLambda  (None, 9, 14, 240)  0           ['re_lu_13[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " multiply_5 (Multiply)          (None, 9, 14, 240)   0           ['expanded_conv_5/expand/BatchNor\n",
      "                                                                 m[0][0]',                        \n",
      "                                                                  'tf.math.multiply_8[0][0]']     \n",
      "                                                                                                  \n",
      " expanded_conv_5/depthwise (Dep  (None, 9, 14, 240)  6000        ['multiply_5[0][0]']             \n",
      " thwiseConv2D)                                                                                    \n",
      "                                                                                                  \n",
      " expanded_conv_5/depthwise/Batc  (None, 9, 14, 240)  960         ['expanded_conv_5/depthwise[0][0]\n",
      " hNorm (BatchNormalization)                                      ']                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  (None, 9, 14, 240)  0           ['expanded_conv_5/depthwise/Batch\n",
      " mbda)                                                           Norm[0][0]']                     \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)                (None, 9, 14, 240)   0           ['tf.__operators__.add_9[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.multiply_9 (TFOpLambda  (None, 9, 14, 240)  0           ['re_lu_14[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " multiply_6 (Multiply)          (None, 9, 14, 240)   0           ['expanded_conv_5/depthwise/Batch\n",
      "                                                                 Norm[0][0]',                     \n",
      "                                                                  'tf.math.multiply_9[0][0]']     \n",
      "                                                                                                  \n",
      " expanded_conv_5/squeeze_excite  (None, 1, 1, 240)   0           ['multiply_6[0][0]']             \n",
      " /AvgPool (GlobalAveragePooling                                                                   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " expanded_conv_5/squeeze_excite  (None, 1, 1, 64)    15424       ['expanded_conv_5/squeeze_excite/\n",
      " /Conv (Conv2D)                                                  AvgPool[0][0]']                  \n",
      "                                                                                                  \n",
      " expanded_conv_5/squeeze_excite  (None, 1, 1, 64)    0           ['expanded_conv_5/squeeze_excite/\n",
      " /Relu (ReLU)                                                    Conv[0][0]']                     \n",
      "                                                                                                  \n",
      " expanded_conv_5/squeeze_excite  (None, 1, 1, 240)   15600       ['expanded_conv_5/squeeze_excite/\n",
      " /Conv_1 (Conv2D)                                                Relu[0][0]']                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  (None, 1, 1, 240)   0           ['expanded_conv_5/squeeze_excite/\n",
      " ambda)                                                          Conv_1[0][0]']                   \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)                (None, 1, 1, 240)    0           ['tf.__operators__.add_10[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_10 (TFOpLambd  (None, 1, 1, 240)   0           ['re_lu_15[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " expanded_conv_5/squeeze_excite  (None, 9, 14, 240)  0           ['multiply_6[0][0]',             \n",
      " /Mul (Multiply)                                                  'tf.math.multiply_10[0][0]']    \n",
      "                                                                                                  \n",
      " expanded_conv_5/project (Conv2  (None, 9, 14, 40)   9600        ['expanded_conv_5/squeeze_excite/\n",
      " D)                                                              Mul[0][0]']                      \n",
      "                                                                                                  \n",
      " expanded_conv_5/project/BatchN  (None, 9, 14, 40)   160         ['expanded_conv_5/project[0][0]']\n",
      " orm (BatchNormalization)                                                                         \n",
      "                                                                                                  \n",
      " expanded_conv_5/Add (Add)      (None, 9, 14, 40)    0           ['expanded_conv_4/Add[0][0]',    \n",
      "                                                                  'expanded_conv_5/project/BatchNo\n",
      "                                                                 rm[0][0]']                       \n",
      "                                                                                                  \n",
      " expanded_conv_6/expand (Conv2D  (None, 9, 14, 120)  4800        ['expanded_conv_5/Add[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " expanded_conv_6/expand/BatchNo  (None, 9, 14, 120)  480         ['expanded_conv_6/expand[0][0]'] \n",
      " rm (BatchNormalization)                                                                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  (None, 9, 14, 120)  0           ['expanded_conv_6/expand/BatchNor\n",
      " ambda)                                                          m[0][0]']                        \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)                (None, 9, 14, 120)   0           ['tf.__operators__.add_11[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_11 (TFOpLambd  (None, 9, 14, 120)  0           ['re_lu_16[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " multiply_7 (Multiply)          (None, 9, 14, 120)   0           ['expanded_conv_6/expand/BatchNor\n",
      "                                                                 m[0][0]',                        \n",
      "                                                                  'tf.math.multiply_11[0][0]']    \n",
      "                                                                                                  \n",
      " expanded_conv_6/depthwise (Dep  (None, 9, 14, 120)  3000        ['multiply_7[0][0]']             \n",
      " thwiseConv2D)                                                                                    \n",
      "                                                                                                  \n",
      " expanded_conv_6/depthwise/Batc  (None, 9, 14, 120)  480         ['expanded_conv_6/depthwise[0][0]\n",
      " hNorm (BatchNormalization)                                      ']                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (TFOpL  (None, 9, 14, 120)  0           ['expanded_conv_6/depthwise/Batch\n",
      " ambda)                                                          Norm[0][0]']                     \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)                (None, 9, 14, 120)   0           ['tf.__operators__.add_12[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_12 (TFOpLambd  (None, 9, 14, 120)  0           ['re_lu_17[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " multiply_8 (Multiply)          (None, 9, 14, 120)   0           ['expanded_conv_6/depthwise/Batch\n",
      "                                                                 Norm[0][0]',                     \n",
      "                                                                  'tf.math.multiply_12[0][0]']    \n",
      "                                                                                                  \n",
      " expanded_conv_6/squeeze_excite  (None, 1, 1, 120)   0           ['multiply_8[0][0]']             \n",
      " /AvgPool (GlobalAveragePooling                                                                   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " expanded_conv_6/squeeze_excite  (None, 1, 1, 32)    3872        ['expanded_conv_6/squeeze_excite/\n",
      " /Conv (Conv2D)                                                  AvgPool[0][0]']                  \n",
      "                                                                                                  \n",
      " expanded_conv_6/squeeze_excite  (None, 1, 1, 32)    0           ['expanded_conv_6/squeeze_excite/\n",
      " /Relu (ReLU)                                                    Conv[0][0]']                     \n",
      "                                                                                                  \n",
      " expanded_conv_6/squeeze_excite  (None, 1, 1, 120)   3960        ['expanded_conv_6/squeeze_excite/\n",
      " /Conv_1 (Conv2D)                                                Relu[0][0]']                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (TFOpL  (None, 1, 1, 120)   0           ['expanded_conv_6/squeeze_excite/\n",
      " ambda)                                                          Conv_1[0][0]']                   \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)                (None, 1, 1, 120)    0           ['tf.__operators__.add_13[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_13 (TFOpLambd  (None, 1, 1, 120)   0           ['re_lu_18[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " expanded_conv_6/squeeze_excite  (None, 9, 14, 120)  0           ['multiply_8[0][0]',             \n",
      " /Mul (Multiply)                                                  'tf.math.multiply_13[0][0]']    \n",
      "                                                                                                  \n",
      " expanded_conv_6/project (Conv2  (None, 9, 14, 48)   5760        ['expanded_conv_6/squeeze_excite/\n",
      " D)                                                              Mul[0][0]']                      \n",
      "                                                                                                  \n",
      " expanded_conv_6/project/BatchN  (None, 9, 14, 48)   192         ['expanded_conv_6/project[0][0]']\n",
      " orm (BatchNormalization)                                                                         \n",
      "                                                                                                  \n",
      " expanded_conv_7/expand (Conv2D  (None, 9, 14, 144)  6912        ['expanded_conv_6/project/BatchNo\n",
      " )                                                               rm[0][0]']                       \n",
      "                                                                                                  \n",
      " expanded_conv_7/expand/BatchNo  (None, 9, 14, 144)  576         ['expanded_conv_7/expand[0][0]'] \n",
      " rm (BatchNormalization)                                                                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (TFOpL  (None, 9, 14, 144)  0           ['expanded_conv_7/expand/BatchNor\n",
      " ambda)                                                          m[0][0]']                        \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)                (None, 9, 14, 144)   0           ['tf.__operators__.add_14[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_14 (TFOpLambd  (None, 9, 14, 144)  0           ['re_lu_19[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " multiply_9 (Multiply)          (None, 9, 14, 144)   0           ['expanded_conv_7/expand/BatchNor\n",
      "                                                                 m[0][0]',                        \n",
      "                                                                  'tf.math.multiply_14[0][0]']    \n",
      "                                                                                                  \n",
      " expanded_conv_7/depthwise (Dep  (None, 9, 14, 144)  3600        ['multiply_9[0][0]']             \n",
      " thwiseConv2D)                                                                                    \n",
      "                                                                                                  \n",
      " expanded_conv_7/depthwise/Batc  (None, 9, 14, 144)  576         ['expanded_conv_7/depthwise[0][0]\n",
      " hNorm (BatchNormalization)                                      ']                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (TFOpL  (None, 9, 14, 144)  0           ['expanded_conv_7/depthwise/Batch\n",
      " ambda)                                                          Norm[0][0]']                     \n",
      "                                                                                                  \n",
      " re_lu_20 (ReLU)                (None, 9, 14, 144)   0           ['tf.__operators__.add_15[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_15 (TFOpLambd  (None, 9, 14, 144)  0           ['re_lu_20[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " multiply_10 (Multiply)         (None, 9, 14, 144)   0           ['expanded_conv_7/depthwise/Batch\n",
      "                                                                 Norm[0][0]',                     \n",
      "                                                                  'tf.math.multiply_15[0][0]']    \n",
      "                                                                                                  \n",
      " expanded_conv_7/squeeze_excite  (None, 1, 1, 144)   0           ['multiply_10[0][0]']            \n",
      " /AvgPool (GlobalAveragePooling                                                                   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " expanded_conv_7/squeeze_excite  (None, 1, 1, 40)    5800        ['expanded_conv_7/squeeze_excite/\n",
      " /Conv (Conv2D)                                                  AvgPool[0][0]']                  \n",
      "                                                                                                  \n",
      " expanded_conv_7/squeeze_excite  (None, 1, 1, 40)    0           ['expanded_conv_7/squeeze_excite/\n",
      " /Relu (ReLU)                                                    Conv[0][0]']                     \n",
      "                                                                                                  \n",
      " expanded_conv_7/squeeze_excite  (None, 1, 1, 144)   5904        ['expanded_conv_7/squeeze_excite/\n",
      " /Conv_1 (Conv2D)                                                Relu[0][0]']                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (TFOpL  (None, 1, 1, 144)   0           ['expanded_conv_7/squeeze_excite/\n",
      " ambda)                                                          Conv_1[0][0]']                   \n",
      "                                                                                                  \n",
      " re_lu_21 (ReLU)                (None, 1, 1, 144)    0           ['tf.__operators__.add_16[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_16 (TFOpLambd  (None, 1, 1, 144)   0           ['re_lu_21[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " expanded_conv_7/squeeze_excite  (None, 9, 14, 144)  0           ['multiply_10[0][0]',            \n",
      " /Mul (Multiply)                                                  'tf.math.multiply_16[0][0]']    \n",
      "                                                                                                  \n",
      " expanded_conv_7/project (Conv2  (None, 9, 14, 48)   6912        ['expanded_conv_7/squeeze_excite/\n",
      " D)                                                              Mul[0][0]']                      \n",
      "                                                                                                  \n",
      " expanded_conv_7/project/BatchN  (None, 9, 14, 48)   192         ['expanded_conv_7/project[0][0]']\n",
      " orm (BatchNormalization)                                                                         \n",
      "                                                                                                  \n",
      " expanded_conv_7/Add (Add)      (None, 9, 14, 48)    0           ['expanded_conv_6/project/BatchNo\n",
      "                                                                 rm[0][0]',                       \n",
      "                                                                  'expanded_conv_7/project/BatchNo\n",
      "                                                                 rm[0][0]']                       \n",
      "                                                                                                  \n",
      " expanded_conv_8/expand (Conv2D  (None, 9, 14, 288)  13824       ['expanded_conv_7/Add[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " expanded_conv_8/expand/BatchNo  (None, 9, 14, 288)  1152        ['expanded_conv_8/expand[0][0]'] \n",
      " rm (BatchNormalization)                                                                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (TFOpL  (None, 9, 14, 288)  0           ['expanded_conv_8/expand/BatchNor\n",
      " ambda)                                                          m[0][0]']                        \n",
      "                                                                                                  \n",
      " re_lu_22 (ReLU)                (None, 9, 14, 288)   0           ['tf.__operators__.add_17[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_17 (TFOpLambd  (None, 9, 14, 288)  0           ['re_lu_22[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " multiply_11 (Multiply)         (None, 9, 14, 288)   0           ['expanded_conv_8/expand/BatchNor\n",
      "                                                                 m[0][0]',                        \n",
      "                                                                  'tf.math.multiply_17[0][0]']    \n",
      "                                                                                                  \n",
      " expanded_conv_8/depthwise/pad   (None, 13, 17, 288)  0          ['multiply_11[0][0]']            \n",
      " (ZeroPadding2D)                                                                                  \n",
      "                                                                                                  \n",
      " expanded_conv_8/depthwise (Dep  (None, 5, 7, 288)   7200        ['expanded_conv_8/depthwise/pad[0\n",
      " thwiseConv2D)                                                   ][0]']                           \n",
      "                                                                                                  \n",
      " expanded_conv_8/depthwise/Batc  (None, 5, 7, 288)   1152        ['expanded_conv_8/depthwise[0][0]\n",
      " hNorm (BatchNormalization)                                      ']                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (TFOpL  (None, 5, 7, 288)   0           ['expanded_conv_8/depthwise/Batch\n",
      " ambda)                                                          Norm[0][0]']                     \n",
      "                                                                                                  \n",
      " re_lu_23 (ReLU)                (None, 5, 7, 288)    0           ['tf.__operators__.add_18[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_18 (TFOpLambd  (None, 5, 7, 288)   0           ['re_lu_23[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " multiply_12 (Multiply)         (None, 5, 7, 288)    0           ['expanded_conv_8/depthwise/Batch\n",
      "                                                                 Norm[0][0]',                     \n",
      "                                                                  'tf.math.multiply_18[0][0]']    \n",
      "                                                                                                  \n",
      " expanded_conv_8/squeeze_excite  (None, 1, 1, 288)   0           ['multiply_12[0][0]']            \n",
      " /AvgPool (GlobalAveragePooling                                                                   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " expanded_conv_8/squeeze_excite  (None, 1, 1, 72)    20808       ['expanded_conv_8/squeeze_excite/\n",
      " /Conv (Conv2D)                                                  AvgPool[0][0]']                  \n",
      "                                                                                                  \n",
      " expanded_conv_8/squeeze_excite  (None, 1, 1, 72)    0           ['expanded_conv_8/squeeze_excite/\n",
      " /Relu (ReLU)                                                    Conv[0][0]']                     \n",
      "                                                                                                  \n",
      " expanded_conv_8/squeeze_excite  (None, 1, 1, 288)   21024       ['expanded_conv_8/squeeze_excite/\n",
      " /Conv_1 (Conv2D)                                                Relu[0][0]']                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (TFOpL  (None, 1, 1, 288)   0           ['expanded_conv_8/squeeze_excite/\n",
      " ambda)                                                          Conv_1[0][0]']                   \n",
      "                                                                                                  \n",
      " re_lu_24 (ReLU)                (None, 1, 1, 288)    0           ['tf.__operators__.add_19[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_19 (TFOpLambd  (None, 1, 1, 288)   0           ['re_lu_24[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " expanded_conv_8/squeeze_excite  (None, 5, 7, 288)   0           ['multiply_12[0][0]',            \n",
      " /Mul (Multiply)                                                  'tf.math.multiply_19[0][0]']    \n",
      "                                                                                                  \n",
      " expanded_conv_8/project (Conv2  (None, 5, 7, 96)    27648       ['expanded_conv_8/squeeze_excite/\n",
      " D)                                                              Mul[0][0]']                      \n",
      "                                                                                                  \n",
      " expanded_conv_8/project/BatchN  (None, 5, 7, 96)    384         ['expanded_conv_8/project[0][0]']\n",
      " orm (BatchNormalization)                                                                         \n",
      "                                                                                                  \n",
      " expanded_conv_9/expand (Conv2D  (None, 5, 7, 576)   55296       ['expanded_conv_8/project/BatchNo\n",
      " )                                                               rm[0][0]']                       \n",
      "                                                                                                  \n",
      " expanded_conv_9/expand/BatchNo  (None, 5, 7, 576)   2304        ['expanded_conv_9/expand[0][0]'] \n",
      " rm (BatchNormalization)                                                                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (TFOpL  (None, 5, 7, 576)   0           ['expanded_conv_9/expand/BatchNor\n",
      " ambda)                                                          m[0][0]']                        \n",
      "                                                                                                  \n",
      " re_lu_25 (ReLU)                (None, 5, 7, 576)    0           ['tf.__operators__.add_20[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_20 (TFOpLambd  (None, 5, 7, 576)   0           ['re_lu_25[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " multiply_13 (Multiply)         (None, 5, 7, 576)    0           ['expanded_conv_9/expand/BatchNor\n",
      "                                                                 m[0][0]',                        \n",
      "                                                                  'tf.math.multiply_20[0][0]']    \n",
      "                                                                                                  \n",
      " expanded_conv_9/depthwise (Dep  (None, 5, 7, 576)   14400       ['multiply_13[0][0]']            \n",
      " thwiseConv2D)                                                                                    \n",
      "                                                                                                  \n",
      " expanded_conv_9/depthwise/Batc  (None, 5, 7, 576)   2304        ['expanded_conv_9/depthwise[0][0]\n",
      " hNorm (BatchNormalization)                                      ']                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (TFOpL  (None, 5, 7, 576)   0           ['expanded_conv_9/depthwise/Batch\n",
      " ambda)                                                          Norm[0][0]']                     \n",
      "                                                                                                  \n",
      " re_lu_26 (ReLU)                (None, 5, 7, 576)    0           ['tf.__operators__.add_21[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_21 (TFOpLambd  (None, 5, 7, 576)   0           ['re_lu_26[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " multiply_14 (Multiply)         (None, 5, 7, 576)    0           ['expanded_conv_9/depthwise/Batch\n",
      "                                                                 Norm[0][0]',                     \n",
      "                                                                  'tf.math.multiply_21[0][0]']    \n",
      "                                                                                                  \n",
      " expanded_conv_9/squeeze_excite  (None, 1, 1, 576)   0           ['multiply_14[0][0]']            \n",
      " /AvgPool (GlobalAveragePooling                                                                   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " expanded_conv_9/squeeze_excite  (None, 1, 1, 144)   83088       ['expanded_conv_9/squeeze_excite/\n",
      " /Conv (Conv2D)                                                  AvgPool[0][0]']                  \n",
      "                                                                                                  \n",
      " expanded_conv_9/squeeze_excite  (None, 1, 1, 144)   0           ['expanded_conv_9/squeeze_excite/\n",
      " /Relu (ReLU)                                                    Conv[0][0]']                     \n",
      "                                                                                                  \n",
      " expanded_conv_9/squeeze_excite  (None, 1, 1, 576)   83520       ['expanded_conv_9/squeeze_excite/\n",
      " /Conv_1 (Conv2D)                                                Relu[0][0]']                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (TFOpL  (None, 1, 1, 576)   0           ['expanded_conv_9/squeeze_excite/\n",
      " ambda)                                                          Conv_1[0][0]']                   \n",
      "                                                                                                  \n",
      " re_lu_27 (ReLU)                (None, 1, 1, 576)    0           ['tf.__operators__.add_22[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_22 (TFOpLambd  (None, 1, 1, 576)   0           ['re_lu_27[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " expanded_conv_9/squeeze_excite  (None, 5, 7, 576)   0           ['multiply_14[0][0]',            \n",
      " /Mul (Multiply)                                                  'tf.math.multiply_22[0][0]']    \n",
      "                                                                                                  \n",
      " expanded_conv_9/project (Conv2  (None, 5, 7, 96)    55296       ['expanded_conv_9/squeeze_excite/\n",
      " D)                                                              Mul[0][0]']                      \n",
      "                                                                                                  \n",
      " expanded_conv_9/project/BatchN  (None, 5, 7, 96)    384         ['expanded_conv_9/project[0][0]']\n",
      " orm (BatchNormalization)                                                                         \n",
      "                                                                                                  \n",
      " expanded_conv_9/Add (Add)      (None, 5, 7, 96)     0           ['expanded_conv_8/project/BatchNo\n",
      "                                                                 rm[0][0]',                       \n",
      "                                                                  'expanded_conv_9/project/BatchNo\n",
      "                                                                 rm[0][0]']                       \n",
      "                                                                                                  \n",
      " expanded_conv_10/expand (Conv2  (None, 5, 7, 576)   55296       ['expanded_conv_9/Add[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " expanded_conv_10/expand/BatchN  (None, 5, 7, 576)   2304        ['expanded_conv_10/expand[0][0]']\n",
      " orm (BatchNormalization)                                                                         \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (TFOpL  (None, 5, 7, 576)   0           ['expanded_conv_10/expand/BatchNo\n",
      " ambda)                                                          rm[0][0]']                       \n",
      "                                                                                                  \n",
      " re_lu_28 (ReLU)                (None, 5, 7, 576)    0           ['tf.__operators__.add_23[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_23 (TFOpLambd  (None, 5, 7, 576)   0           ['re_lu_28[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " multiply_15 (Multiply)         (None, 5, 7, 576)    0           ['expanded_conv_10/expand/BatchNo\n",
      "                                                                 rm[0][0]',                       \n",
      "                                                                  'tf.math.multiply_23[0][0]']    \n",
      "                                                                                                  \n",
      " expanded_conv_10/depthwise (De  (None, 5, 7, 576)   14400       ['multiply_15[0][0]']            \n",
      " pthwiseConv2D)                                                                                   \n",
      "                                                                                                  \n",
      " expanded_conv_10/depthwise/Bat  (None, 5, 7, 576)   2304        ['expanded_conv_10/depthwise[0][0\n",
      " chNorm (BatchNormalization)                                     ]']                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_24 (TFOpL  (None, 5, 7, 576)   0           ['expanded_conv_10/depthwise/Batc\n",
      " ambda)                                                          hNorm[0][0]']                    \n",
      "                                                                                                  \n",
      " re_lu_29 (ReLU)                (None, 5, 7, 576)    0           ['tf.__operators__.add_24[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_24 (TFOpLambd  (None, 5, 7, 576)   0           ['re_lu_29[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " multiply_16 (Multiply)         (None, 5, 7, 576)    0           ['expanded_conv_10/depthwise/Batc\n",
      "                                                                 hNorm[0][0]',                    \n",
      "                                                                  'tf.math.multiply_24[0][0]']    \n",
      "                                                                                                  \n",
      " expanded_conv_10/squeeze_excit  (None, 1, 1, 576)   0           ['multiply_16[0][0]']            \n",
      " e/AvgPool (GlobalAveragePoolin                                                                   \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " expanded_conv_10/squeeze_excit  (None, 1, 1, 144)   83088       ['expanded_conv_10/squeeze_excite\n",
      " e/Conv (Conv2D)                                                 /AvgPool[0][0]']                 \n",
      "                                                                                                  \n",
      " expanded_conv_10/squeeze_excit  (None, 1, 1, 144)   0           ['expanded_conv_10/squeeze_excite\n",
      " e/Relu (ReLU)                                                   /Conv[0][0]']                    \n",
      "                                                                                                  \n",
      " expanded_conv_10/squeeze_excit  (None, 1, 1, 576)   83520       ['expanded_conv_10/squeeze_excite\n",
      " e/Conv_1 (Conv2D)                                               /Relu[0][0]']                    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_25 (TFOpL  (None, 1, 1, 576)   0           ['expanded_conv_10/squeeze_excite\n",
      " ambda)                                                          /Conv_1[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu_30 (ReLU)                (None, 1, 1, 576)    0           ['tf.__operators__.add_25[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_25 (TFOpLambd  (None, 1, 1, 576)   0           ['re_lu_30[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " expanded_conv_10/squeeze_excit  (None, 5, 7, 576)   0           ['multiply_16[0][0]',            \n",
      " e/Mul (Multiply)                                                 'tf.math.multiply_25[0][0]']    \n",
      "                                                                                                  \n",
      " expanded_conv_10/project (Conv  (None, 5, 7, 96)    55296       ['expanded_conv_10/squeeze_excite\n",
      " 2D)                                                             /Mul[0][0]']                     \n",
      "                                                                                                  \n",
      " expanded_conv_10/project/Batch  (None, 5, 7, 96)    384         ['expanded_conv_10/project[0][0]'\n",
      " Norm (BatchNormalization)                                       ]                                \n",
      "                                                                                                  \n",
      " expanded_conv_10/Add (Add)     (None, 5, 7, 96)     0           ['expanded_conv_9/Add[0][0]',    \n",
      "                                                                  'expanded_conv_10/project/BatchN\n",
      "                                                                 orm[0][0]']                      \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)                (None, 5, 7, 576)    55296       ['expanded_conv_10/Add[0][0]']   \n",
      "                                                                                                  \n",
      " Conv_1/BatchNorm (BatchNormali  (None, 5, 7, 576)   2304        ['Conv_1[0][0]']                 \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_26 (TFOpL  (None, 5, 7, 576)   0           ['Conv_1/BatchNorm[0][0]']       \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " re_lu_31 (ReLU)                (None, 5, 7, 576)    0           ['tf.__operators__.add_26[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_26 (TFOpLambd  (None, 5, 7, 576)   0           ['re_lu_31[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " multiply_17 (Multiply)         (None, 5, 7, 576)    0           ['Conv_1/BatchNorm[0][0]',       \n",
      "                                                                  'tf.math.multiply_26[0][0]']    \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1, 1, 576)   0           ['multiply_17[0][0]']            \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " Conv_2 (Conv2D)                (None, 1, 1, 1024)   590848      ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_27 (TFOpL  (None, 1, 1, 1024)  0           ['Conv_2[0][0]']                 \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " re_lu_32 (ReLU)                (None, 1, 1, 1024)   0           ['tf.__operators__.add_27[0][0]']\n",
      "                                                                                                  \n",
      " tf.math.multiply_27 (TFOpLambd  (None, 1, 1, 1024)  0           ['re_lu_32[0][0]']               \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " multiply_18 (Multiply)         (None, 1, 1, 1024)   0           ['Conv_2[0][0]',                 \n",
      "                                                                  'tf.math.multiply_27[0][0]']    \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 1024)        0           ['multiply_18[0][0]']            \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         1049600     ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            1025        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,580,593\n",
      "Trainable params: 1,050,625\n",
      "Non-trainable params: 1,529,968\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 06:54:36.209571: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34/566 [>.............................] - ETA: 2s - loss: 0.6295 - mae: 0.6207 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 06:54:37.945651: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566/566 [==============================] - ETA: 0s - loss: 0.2945 - mae: 0.4216"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/tensorflow/2.7.0/lib/python3.9/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566/566 [==============================] - 8s 8ms/step - loss: 0.2945 - mae: 0.4216 - val_loss: 0.1662 - val_mae: 0.3131\n",
      "Epoch 2/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.1856 - mae: 0.3343 - val_loss: 0.1591 - val_mae: 0.3086\n",
      "Epoch 3/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.1718 - mae: 0.3211 - val_loss: 0.1559 - val_mae: 0.2928\n",
      "Epoch 4/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.1638 - mae: 0.3108 - val_loss: 0.1522 - val_mae: 0.2946\n",
      "Epoch 5/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.1584 - mae: 0.3065 - val_loss: 0.1473 - val_mae: 0.2874\n",
      "Epoch 6/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.1504 - mae: 0.2971 - val_loss: 0.1490 - val_mae: 0.2848\n",
      "Epoch 7/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.1487 - mae: 0.2952 - val_loss: 0.1578 - val_mae: 0.2992\n",
      "Epoch 8/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.1437 - mae: 0.2892 - val_loss: 0.1434 - val_mae: 0.2796\n",
      "Epoch 9/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.1420 - mae: 0.2874 - val_loss: 0.1418 - val_mae: 0.2808\n",
      "Epoch 10/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.1388 - mae: 0.2840 - val_loss: 0.1388 - val_mae: 0.2773\n",
      "Epoch 11/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.1353 - mae: 0.2805 - val_loss: 0.1421 - val_mae: 0.2784\n",
      "Epoch 12/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.1323 - mae: 0.2759 - val_loss: 0.1362 - val_mae: 0.2689\n",
      "Epoch 13/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.1296 - mae: 0.2728 - val_loss: 0.1356 - val_mae: 0.2729\n",
      "Epoch 14/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.1277 - mae: 0.2717 - val_loss: 0.1338 - val_mae: 0.2682\n",
      "Epoch 15/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.1235 - mae: 0.2674 - val_loss: 0.1374 - val_mae: 0.2699\n",
      "Epoch 16/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.1217 - mae: 0.2638 - val_loss: 0.1332 - val_mae: 0.2632\n",
      "Epoch 17/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.1189 - mae: 0.2602 - val_loss: 0.1418 - val_mae: 0.2831\n",
      "Epoch 18/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.1182 - mae: 0.2590 - val_loss: 0.1332 - val_mae: 0.2716\n",
      "Epoch 19/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.1158 - mae: 0.2567 - val_loss: 0.1330 - val_mae: 0.2680\n",
      "Epoch 20/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.1123 - mae: 0.2527 - val_loss: 0.1329 - val_mae: 0.2642\n",
      "Epoch 21/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.1102 - mae: 0.2505 - val_loss: 0.1309 - val_mae: 0.2659\n",
      "Epoch 22/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.1074 - mae: 0.2472 - val_loss: 0.1315 - val_mae: 0.2696\n",
      "Epoch 23/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.1047 - mae: 0.2441 - val_loss: 0.1321 - val_mae: 0.2624\n",
      "Epoch 24/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.1047 - mae: 0.2424 - val_loss: 0.1309 - val_mae: 0.2668\n",
      "Epoch 25/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.1029 - mae: 0.2421 - val_loss: 0.1374 - val_mae: 0.2735\n",
      "Epoch 26/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.1002 - mae: 0.2375 - val_loss: 0.1318 - val_mae: 0.2654\n",
      "Epoch 27/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0987 - mae: 0.2361 - val_loss: 0.1290 - val_mae: 0.2631\n",
      "Epoch 28/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0965 - mae: 0.2335 - val_loss: 0.1300 - val_mae: 0.2600\n",
      "Epoch 29/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0938 - mae: 0.2297 - val_loss: 0.1326 - val_mae: 0.2696\n",
      "Epoch 30/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0906 - mae: 0.2252 - val_loss: 0.1306 - val_mae: 0.2596\n",
      "Epoch 31/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0911 - mae: 0.2258 - val_loss: 0.1285 - val_mae: 0.2592\n",
      "Epoch 32/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0896 - mae: 0.2248 - val_loss: 0.1341 - val_mae: 0.2663\n",
      "Epoch 33/2000\n",
      "566/566 [==============================] - 6s 10ms/step - loss: 0.0887 - mae: 0.2236 - val_loss: 0.1265 - val_mae: 0.2564\n",
      "Epoch 34/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0863 - mae: 0.2214 - val_loss: 0.1308 - val_mae: 0.2616\n",
      "Epoch 35/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0868 - mae: 0.2207 - val_loss: 0.1279 - val_mae: 0.2591\n",
      "Epoch 36/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0828 - mae: 0.2156 - val_loss: 0.1287 - val_mae: 0.2631\n",
      "Epoch 37/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0825 - mae: 0.2172 - val_loss: 0.1298 - val_mae: 0.2635\n",
      "Epoch 38/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0795 - mae: 0.2117 - val_loss: 0.1329 - val_mae: 0.2679\n",
      "Epoch 39/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0797 - mae: 0.2116 - val_loss: 0.1304 - val_mae: 0.2624\n",
      "Epoch 40/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0776 - mae: 0.2097 - val_loss: 0.1296 - val_mae: 0.2621\n",
      "Epoch 41/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0777 - mae: 0.2090 - val_loss: 0.1279 - val_mae: 0.2616\n",
      "Epoch 42/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0765 - mae: 0.2072 - val_loss: 0.1413 - val_mae: 0.2749\n",
      "Epoch 43/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0760 - mae: 0.2068 - val_loss: 0.1333 - val_mae: 0.2631\n",
      "Epoch 44/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0729 - mae: 0.2018 - val_loss: 0.1293 - val_mae: 0.2583\n",
      "Epoch 45/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0732 - mae: 0.2021 - val_loss: 0.1281 - val_mae: 0.2569\n",
      "Epoch 46/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0700 - mae: 0.1982 - val_loss: 0.1293 - val_mae: 0.2594\n",
      "Epoch 47/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0702 - mae: 0.1980 - val_loss: 0.1263 - val_mae: 0.2552\n",
      "Epoch 48/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0686 - mae: 0.1958 - val_loss: 0.1283 - val_mae: 0.2554\n",
      "Epoch 49/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0678 - mae: 0.1948 - val_loss: 0.1283 - val_mae: 0.2536\n",
      "Epoch 50/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0663 - mae: 0.1941 - val_loss: 0.1275 - val_mae: 0.2562\n",
      "Epoch 51/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0652 - mae: 0.1918 - val_loss: 0.1352 - val_mae: 0.2660\n",
      "Epoch 52/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0641 - mae: 0.1907 - val_loss: 0.1274 - val_mae: 0.2585\n",
      "Epoch 53/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0630 - mae: 0.1886 - val_loss: 0.1304 - val_mae: 0.2586\n",
      "Epoch 54/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0631 - mae: 0.1891 - val_loss: 0.1324 - val_mae: 0.2617\n",
      "Epoch 55/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0625 - mae: 0.1871 - val_loss: 0.1307 - val_mae: 0.2628\n",
      "Epoch 56/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0623 - mae: 0.1874 - val_loss: 0.1272 - val_mae: 0.2576\n",
      "Epoch 57/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0616 - mae: 0.1859 - val_loss: 0.1281 - val_mae: 0.2555\n",
      "Epoch 58/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0604 - mae: 0.1849 - val_loss: 0.1284 - val_mae: 0.2599\n",
      "Epoch 59/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0597 - mae: 0.1834 - val_loss: 0.1278 - val_mae: 0.2563\n",
      "Epoch 60/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0583 - mae: 0.1822 - val_loss: 0.1332 - val_mae: 0.2614\n",
      "Epoch 61/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0579 - mae: 0.1815 - val_loss: 0.1277 - val_mae: 0.2561\n",
      "Epoch 62/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0556 - mae: 0.1766 - val_loss: 0.1268 - val_mae: 0.2545\n",
      "Epoch 63/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0547 - mae: 0.1765 - val_loss: 0.1281 - val_mae: 0.2581\n",
      "Epoch 64/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0564 - mae: 0.1775 - val_loss: 0.1279 - val_mae: 0.2577\n",
      "Epoch 65/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0547 - mae: 0.1749 - val_loss: 0.1284 - val_mae: 0.2571\n",
      "Epoch 66/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0540 - mae: 0.1749 - val_loss: 0.1312 - val_mae: 0.2648\n",
      "Epoch 67/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0535 - mae: 0.1732 - val_loss: 0.1328 - val_mae: 0.2595\n",
      "Epoch 68/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0534 - mae: 0.1737 - val_loss: 0.1301 - val_mae: 0.2588\n",
      "Epoch 69/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0513 - mae: 0.1700 - val_loss: 0.1314 - val_mae: 0.2592\n",
      "Epoch 70/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0533 - mae: 0.1731 - val_loss: 0.1337 - val_mae: 0.2611\n",
      "Epoch 71/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0512 - mae: 0.1696 - val_loss: 0.1276 - val_mae: 0.2543\n",
      "Epoch 72/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0502 - mae: 0.1679 - val_loss: 0.1309 - val_mae: 0.2591\n",
      "Epoch 73/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0510 - mae: 0.1697 - val_loss: 0.1319 - val_mae: 0.2626\n",
      "Epoch 74/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0503 - mae: 0.1690 - val_loss: 0.1268 - val_mae: 0.2547\n",
      "Epoch 75/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0504 - mae: 0.1678 - val_loss: 0.1279 - val_mae: 0.2568\n",
      "Epoch 76/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0501 - mae: 0.1677 - val_loss: 0.1272 - val_mae: 0.2571\n",
      "Epoch 77/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0483 - mae: 0.1650 - val_loss: 0.1301 - val_mae: 0.2603\n",
      "Epoch 78/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0468 - mae: 0.1620 - val_loss: 0.1289 - val_mae: 0.2590\n",
      "Epoch 79/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0482 - mae: 0.1641 - val_loss: 0.1294 - val_mae: 0.2618\n",
      "Epoch 80/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0477 - mae: 0.1637 - val_loss: 0.1305 - val_mae: 0.2573\n",
      "Epoch 81/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0464 - mae: 0.1623 - val_loss: 0.1304 - val_mae: 0.2589\n",
      "Epoch 82/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0465 - mae: 0.1616 - val_loss: 0.1351 - val_mae: 0.2643\n",
      "Epoch 83/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0469 - mae: 0.1618 - val_loss: 0.1313 - val_mae: 0.2553\n",
      "Epoch 84/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0456 - mae: 0.1597 - val_loss: 0.1307 - val_mae: 0.2594\n",
      "Epoch 85/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0449 - mae: 0.1586 - val_loss: 0.1267 - val_mae: 0.2532\n",
      "Epoch 86/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0460 - mae: 0.1611 - val_loss: 0.1325 - val_mae: 0.2618\n",
      "Epoch 87/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0466 - mae: 0.1618 - val_loss: 0.1280 - val_mae: 0.2559\n",
      "Epoch 88/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0437 - mae: 0.1568 - val_loss: 0.1290 - val_mae: 0.2549\n",
      "Epoch 89/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0440 - mae: 0.1576 - val_loss: 0.1277 - val_mae: 0.2536\n",
      "Epoch 90/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0428 - mae: 0.1544 - val_loss: 0.1289 - val_mae: 0.2576\n",
      "Epoch 91/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0427 - mae: 0.1549 - val_loss: 0.1306 - val_mae: 0.2587\n",
      "Epoch 92/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0418 - mae: 0.1529 - val_loss: 0.1271 - val_mae: 0.2515\n",
      "Epoch 93/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0434 - mae: 0.1558 - val_loss: 0.1297 - val_mae: 0.2574\n",
      "Epoch 94/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0436 - mae: 0.1561 - val_loss: 0.1298 - val_mae: 0.2586\n",
      "Epoch 95/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0410 - mae: 0.1505 - val_loss: 0.1294 - val_mae: 0.2577\n",
      "Epoch 96/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0414 - mae: 0.1515 - val_loss: 0.1299 - val_mae: 0.2583\n",
      "Epoch 97/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0419 - mae: 0.1528 - val_loss: 0.1276 - val_mae: 0.2537\n",
      "Epoch 98/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0398 - mae: 0.1495 - val_loss: 0.1258 - val_mae: 0.2526\n",
      "Epoch 99/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0406 - mae: 0.1508 - val_loss: 0.1311 - val_mae: 0.2551\n",
      "Epoch 100/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0401 - mae: 0.1501 - val_loss: 0.1279 - val_mae: 0.2570\n",
      "Epoch 101/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0404 - mae: 0.1501 - val_loss: 0.1296 - val_mae: 0.2532\n",
      "Epoch 102/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0397 - mae: 0.1484 - val_loss: 0.1274 - val_mae: 0.2531\n",
      "Epoch 103/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0401 - mae: 0.1499 - val_loss: 0.1270 - val_mae: 0.2524\n",
      "Epoch 104/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0388 - mae: 0.1470 - val_loss: 0.1263 - val_mae: 0.2515\n",
      "Epoch 105/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0409 - mae: 0.1506 - val_loss: 0.1308 - val_mae: 0.2577\n",
      "Epoch 106/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0381 - mae: 0.1461 - val_loss: 0.1275 - val_mae: 0.2556\n",
      "Epoch 107/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0383 - mae: 0.1466 - val_loss: 0.1288 - val_mae: 0.2543\n",
      "Epoch 108/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0399 - mae: 0.1492 - val_loss: 0.1291 - val_mae: 0.2538\n",
      "Epoch 109/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0391 - mae: 0.1473 - val_loss: 0.1271 - val_mae: 0.2504\n",
      "Epoch 110/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0393 - mae: 0.1480 - val_loss: 0.1259 - val_mae: 0.2545\n",
      "Epoch 111/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0387 - mae: 0.1471 - val_loss: 0.1278 - val_mae: 0.2540\n",
      "Epoch 112/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0374 - mae: 0.1446 - val_loss: 0.1275 - val_mae: 0.2524\n",
      "Epoch 113/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0376 - mae: 0.1451 - val_loss: 0.1257 - val_mae: 0.2488\n",
      "Epoch 114/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0370 - mae: 0.1427 - val_loss: 0.1288 - val_mae: 0.2544\n",
      "Epoch 115/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0378 - mae: 0.1444 - val_loss: 0.1289 - val_mae: 0.2491\n",
      "Epoch 116/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0378 - mae: 0.1450 - val_loss: 0.1250 - val_mae: 0.2493\n",
      "Epoch 117/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0364 - mae: 0.1422 - val_loss: 0.1275 - val_mae: 0.2520\n",
      "Epoch 118/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0366 - mae: 0.1426 - val_loss: 0.1305 - val_mae: 0.2600\n",
      "Epoch 119/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0376 - mae: 0.1456 - val_loss: 0.1276 - val_mae: 0.2517\n",
      "Epoch 120/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0350 - mae: 0.1402 - val_loss: 0.1285 - val_mae: 0.2537\n",
      "Epoch 121/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0361 - mae: 0.1418 - val_loss: 0.1277 - val_mae: 0.2507\n",
      "Epoch 122/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0374 - mae: 0.1435 - val_loss: 0.1289 - val_mae: 0.2578\n",
      "Epoch 123/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0351 - mae: 0.1403 - val_loss: 0.1289 - val_mae: 0.2544\n",
      "Epoch 124/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0376 - mae: 0.1448 - val_loss: 0.1287 - val_mae: 0.2523\n",
      "Epoch 125/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0366 - mae: 0.1421 - val_loss: 0.1259 - val_mae: 0.2469\n",
      "Epoch 126/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0345 - mae: 0.1385 - val_loss: 0.1249 - val_mae: 0.2484\n",
      "Epoch 127/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0356 - mae: 0.1405 - val_loss: 0.1285 - val_mae: 0.2536\n",
      "Epoch 128/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0357 - mae: 0.1398 - val_loss: 0.1287 - val_mae: 0.2510\n",
      "Epoch 129/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0347 - mae: 0.1390 - val_loss: 0.1249 - val_mae: 0.2469\n",
      "Epoch 130/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0340 - mae: 0.1367 - val_loss: 0.1286 - val_mae: 0.2533\n",
      "Epoch 131/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0338 - mae: 0.1377 - val_loss: 0.1258 - val_mae: 0.2483\n",
      "Epoch 132/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0351 - mae: 0.1393 - val_loss: 0.1260 - val_mae: 0.2477\n",
      "Epoch 133/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0352 - mae: 0.1384 - val_loss: 0.1292 - val_mae: 0.2515\n",
      "Epoch 134/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0348 - mae: 0.1390 - val_loss: 0.1266 - val_mae: 0.2492\n",
      "Epoch 135/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0338 - mae: 0.1363 - val_loss: 0.1287 - val_mae: 0.2529\n",
      "Epoch 136/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0317 - mae: 0.1330 - val_loss: 0.1268 - val_mae: 0.2494\n",
      "Epoch 137/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0330 - mae: 0.1345 - val_loss: 0.1279 - val_mae: 0.2526\n",
      "Epoch 138/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0343 - mae: 0.1375 - val_loss: 0.1273 - val_mae: 0.2539\n",
      "Epoch 139/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0329 - mae: 0.1345 - val_loss: 0.1293 - val_mae: 0.2526\n",
      "Epoch 140/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0326 - mae: 0.1337 - val_loss: 0.1291 - val_mae: 0.2524\n",
      "Epoch 141/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0340 - mae: 0.1362 - val_loss: 0.1306 - val_mae: 0.2552\n",
      "Epoch 142/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0325 - mae: 0.1344 - val_loss: 0.1296 - val_mae: 0.2558\n",
      "Epoch 143/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0321 - mae: 0.1331 - val_loss: 0.1280 - val_mae: 0.2497\n",
      "Epoch 144/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0331 - mae: 0.1341 - val_loss: 0.1277 - val_mae: 0.2536\n",
      "Epoch 145/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0322 - mae: 0.1335 - val_loss: 0.1271 - val_mae: 0.2505\n",
      "Epoch 146/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0325 - mae: 0.1341 - val_loss: 0.1267 - val_mae: 0.2489\n",
      "Epoch 147/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0330 - mae: 0.1346 - val_loss: 0.1272 - val_mae: 0.2521\n",
      "Epoch 148/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0330 - mae: 0.1340 - val_loss: 0.1273 - val_mae: 0.2506\n",
      "Epoch 149/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0325 - mae: 0.1333 - val_loss: 0.1298 - val_mae: 0.2514\n",
      "Epoch 150/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0323 - mae: 0.1333 - val_loss: 0.1286 - val_mae: 0.2522\n",
      "Epoch 151/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0325 - mae: 0.1332 - val_loss: 0.1288 - val_mae: 0.2521\n",
      "Epoch 152/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0337 - mae: 0.1351 - val_loss: 0.1274 - val_mae: 0.2490\n",
      "Epoch 153/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0314 - mae: 0.1308 - val_loss: 0.1300 - val_mae: 0.2500\n",
      "Epoch 154/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0315 - mae: 0.1310 - val_loss: 0.1262 - val_mae: 0.2500\n",
      "Epoch 155/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0311 - mae: 0.1301 - val_loss: 0.1279 - val_mae: 0.2497\n",
      "Epoch 156/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0306 - mae: 0.1295 - val_loss: 0.1291 - val_mae: 0.2520\n",
      "Epoch 157/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0302 - mae: 0.1292 - val_loss: 0.1251 - val_mae: 0.2457\n",
      "Epoch 158/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0318 - mae: 0.1319 - val_loss: 0.1271 - val_mae: 0.2497\n",
      "Epoch 159/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0320 - mae: 0.1320 - val_loss: 0.1274 - val_mae: 0.2509\n",
      "Epoch 160/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0310 - mae: 0.1304 - val_loss: 0.1282 - val_mae: 0.2486\n",
      "Epoch 161/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0308 - mae: 0.1305 - val_loss: 0.1267 - val_mae: 0.2486\n",
      "Epoch 162/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0310 - mae: 0.1305 - val_loss: 0.1266 - val_mae: 0.2471\n",
      "Epoch 163/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0307 - mae: 0.1293 - val_loss: 0.1286 - val_mae: 0.2500\n",
      "Epoch 164/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0305 - mae: 0.1287 - val_loss: 0.1273 - val_mae: 0.2511\n",
      "Epoch 165/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0312 - mae: 0.1303 - val_loss: 0.1262 - val_mae: 0.2472\n",
      "Epoch 166/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0304 - mae: 0.1293 - val_loss: 0.1297 - val_mae: 0.2522\n",
      "Epoch 167/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0300 - mae: 0.1275 - val_loss: 0.1257 - val_mae: 0.2477\n",
      "Epoch 168/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0306 - mae: 0.1292 - val_loss: 0.1267 - val_mae: 0.2481\n",
      "Epoch 169/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0306 - mae: 0.1292 - val_loss: 0.1286 - val_mae: 0.2497\n",
      "Epoch 170/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0297 - mae: 0.1267 - val_loss: 0.1258 - val_mae: 0.2463\n",
      "Epoch 171/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0319 - mae: 0.1313 - val_loss: 0.1273 - val_mae: 0.2510\n",
      "Epoch 172/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0287 - mae: 0.1256 - val_loss: 0.1280 - val_mae: 0.2479\n",
      "Epoch 173/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0290 - mae: 0.1256 - val_loss: 0.1280 - val_mae: 0.2496\n",
      "Epoch 174/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0300 - mae: 0.1278 - val_loss: 0.1269 - val_mae: 0.2507\n",
      "Epoch 175/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0296 - mae: 0.1267 - val_loss: 0.1268 - val_mae: 0.2512\n",
      "Epoch 176/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0304 - mae: 0.1280 - val_loss: 0.1252 - val_mae: 0.2470\n",
      "Epoch 177/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0300 - mae: 0.1269 - val_loss: 0.1273 - val_mae: 0.2510\n",
      "Epoch 178/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0302 - mae: 0.1281 - val_loss: 0.1284 - val_mae: 0.2494\n",
      "Epoch 179/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0286 - mae: 0.1246 - val_loss: 0.1262 - val_mae: 0.2470\n",
      "Epoch 180/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0303 - mae: 0.1281 - val_loss: 0.1258 - val_mae: 0.2474\n",
      "Epoch 181/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0289 - mae: 0.1258 - val_loss: 0.1282 - val_mae: 0.2515\n",
      "Epoch 182/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0290 - mae: 0.1245 - val_loss: 0.1277 - val_mae: 0.2467\n",
      "Epoch 183/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0287 - mae: 0.1247 - val_loss: 0.1258 - val_mae: 0.2489\n",
      "Epoch 184/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0287 - mae: 0.1253 - val_loss: 0.1277 - val_mae: 0.2484\n",
      "Epoch 185/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0292 - mae: 0.1262 - val_loss: 0.1250 - val_mae: 0.2470\n",
      "Epoch 186/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0296 - mae: 0.1273 - val_loss: 0.1259 - val_mae: 0.2478\n",
      "Epoch 187/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0295 - mae: 0.1262 - val_loss: 0.1275 - val_mae: 0.2487\n",
      "Epoch 188/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0282 - mae: 0.1236 - val_loss: 0.1290 - val_mae: 0.2505\n",
      "Epoch 189/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0290 - mae: 0.1251 - val_loss: 0.1269 - val_mae: 0.2469\n",
      "Epoch 190/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0289 - mae: 0.1253 - val_loss: 0.1271 - val_mae: 0.2480\n",
      "Epoch 191/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0277 - mae: 0.1229 - val_loss: 0.1280 - val_mae: 0.2455\n",
      "Epoch 192/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0289 - mae: 0.1248 - val_loss: 0.1287 - val_mae: 0.2490\n",
      "Epoch 193/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0288 - mae: 0.1250 - val_loss: 0.1287 - val_mae: 0.2487\n",
      "Epoch 194/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0289 - mae: 0.1247 - val_loss: 0.1284 - val_mae: 0.2493\n",
      "Epoch 195/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0289 - mae: 0.1249 - val_loss: 0.1283 - val_mae: 0.2493\n",
      "Epoch 196/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0280 - mae: 0.1217 - val_loss: 0.1264 - val_mae: 0.2445\n",
      "Epoch 197/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0284 - mae: 0.1230 - val_loss: 0.1285 - val_mae: 0.2509\n",
      "Epoch 198/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0284 - mae: 0.1237 - val_loss: 0.1274 - val_mae: 0.2452\n",
      "Epoch 199/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0277 - mae: 0.1216 - val_loss: 0.1278 - val_mae: 0.2471\n",
      "Epoch 200/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0277 - mae: 0.1220 - val_loss: 0.1264 - val_mae: 0.2437\n",
      "Epoch 201/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0280 - mae: 0.1227 - val_loss: 0.1257 - val_mae: 0.2450\n",
      "Epoch 202/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0285 - mae: 0.1240 - val_loss: 0.1291 - val_mae: 0.2509\n",
      "Epoch 203/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0265 - mae: 0.1196 - val_loss: 0.1274 - val_mae: 0.2463\n",
      "Epoch 204/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0268 - mae: 0.1197 - val_loss: 0.1251 - val_mae: 0.2437\n",
      "Epoch 205/2000\n",
      "566/566 [==============================] - 4s 7ms/step - loss: 0.0284 - mae: 0.1237 - val_loss: 0.1247 - val_mae: 0.2455\n",
      "Epoch 206/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0278 - mae: 0.1223 - val_loss: 0.1272 - val_mae: 0.2477\n",
      "Epoch 207/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0276 - mae: 0.1214 - val_loss: 0.1275 - val_mae: 0.2462\n",
      "Epoch 208/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0280 - mae: 0.1218 - val_loss: 0.1274 - val_mae: 0.2478\n",
      "Epoch 209/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0280 - mae: 0.1230 - val_loss: 0.1240 - val_mae: 0.2439\n",
      "Epoch 210/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0277 - mae: 0.1217 - val_loss: 0.1268 - val_mae: 0.2498\n",
      "Epoch 211/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0268 - mae: 0.1212 - val_loss: 0.1280 - val_mae: 0.2468\n",
      "Epoch 212/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0273 - mae: 0.1211 - val_loss: 0.1300 - val_mae: 0.2497\n",
      "Epoch 213/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0268 - mae: 0.1193 - val_loss: 0.1305 - val_mae: 0.2552\n",
      "Epoch 214/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0271 - mae: 0.1211 - val_loss: 0.1275 - val_mae: 0.2496\n",
      "Epoch 215/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0268 - mae: 0.1207 - val_loss: 0.1252 - val_mae: 0.2430\n",
      "Epoch 216/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0271 - mae: 0.1205 - val_loss: 0.1272 - val_mae: 0.2487\n",
      "Epoch 217/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0278 - mae: 0.1225 - val_loss: 0.1249 - val_mae: 0.2462\n",
      "Epoch 218/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0266 - mae: 0.1192 - val_loss: 0.1264 - val_mae: 0.2486\n",
      "Epoch 219/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0265 - mae: 0.1191 - val_loss: 0.1293 - val_mae: 0.2486\n",
      "Epoch 220/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0267 - mae: 0.1189 - val_loss: 0.1253 - val_mae: 0.2455\n",
      "Epoch 221/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0272 - mae: 0.1211 - val_loss: 0.1288 - val_mae: 0.2482\n",
      "Epoch 222/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0256 - mae: 0.1175 - val_loss: 0.1279 - val_mae: 0.2469\n",
      "Epoch 223/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0274 - mae: 0.1208 - val_loss: 0.1263 - val_mae: 0.2461\n",
      "Epoch 224/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0262 - mae: 0.1189 - val_loss: 0.1250 - val_mae: 0.2428\n",
      "Epoch 225/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0267 - mae: 0.1197 - val_loss: 0.1254 - val_mae: 0.2444\n",
      "Epoch 226/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0258 - mae: 0.1180 - val_loss: 0.1257 - val_mae: 0.2477\n",
      "Epoch 227/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0262 - mae: 0.1191 - val_loss: 0.1269 - val_mae: 0.2481\n",
      "Epoch 228/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0264 - mae: 0.1199 - val_loss: 0.1269 - val_mae: 0.2463\n",
      "Epoch 229/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0272 - mae: 0.1198 - val_loss: 0.1253 - val_mae: 0.2454\n",
      "Epoch 230/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0259 - mae: 0.1177 - val_loss: 0.1237 - val_mae: 0.2405\n",
      "Epoch 231/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0254 - mae: 0.1168 - val_loss: 0.1274 - val_mae: 0.2455\n",
      "Epoch 232/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0261 - mae: 0.1181 - val_loss: 0.1278 - val_mae: 0.2466\n",
      "Epoch 233/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0262 - mae: 0.1185 - val_loss: 0.1290 - val_mae: 0.2477\n",
      "Epoch 234/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0257 - mae: 0.1170 - val_loss: 0.1272 - val_mae: 0.2491\n",
      "Epoch 235/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0259 - mae: 0.1176 - val_loss: 0.1267 - val_mae: 0.2436\n",
      "Epoch 236/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0265 - mae: 0.1186 - val_loss: 0.1281 - val_mae: 0.2488\n",
      "Epoch 237/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0267 - mae: 0.1190 - val_loss: 0.1265 - val_mae: 0.2458\n",
      "Epoch 238/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0255 - mae: 0.1168 - val_loss: 0.1253 - val_mae: 0.2424\n",
      "Epoch 239/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0262 - mae: 0.1178 - val_loss: 0.1254 - val_mae: 0.2440\n",
      "Epoch 240/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0264 - mae: 0.1187 - val_loss: 0.1274 - val_mae: 0.2486\n",
      "Epoch 241/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0256 - mae: 0.1170 - val_loss: 0.1240 - val_mae: 0.2429\n",
      "Epoch 242/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0248 - mae: 0.1151 - val_loss: 0.1253 - val_mae: 0.2474\n",
      "Epoch 243/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0268 - mae: 0.1187 - val_loss: 0.1237 - val_mae: 0.2400\n",
      "Epoch 244/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0247 - mae: 0.1151 - val_loss: 0.1260 - val_mae: 0.2429\n",
      "Epoch 245/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0259 - mae: 0.1167 - val_loss: 0.1283 - val_mae: 0.2477\n",
      "Epoch 246/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0255 - mae: 0.1166 - val_loss: 0.1241 - val_mae: 0.2436\n",
      "Epoch 247/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0260 - mae: 0.1176 - val_loss: 0.1249 - val_mae: 0.2426\n",
      "Epoch 248/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0254 - mae: 0.1164 - val_loss: 0.1242 - val_mae: 0.2426\n",
      "Epoch 249/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0248 - mae: 0.1152 - val_loss: 0.1267 - val_mae: 0.2475\n",
      "Epoch 250/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0257 - mae: 0.1163 - val_loss: 0.1223 - val_mae: 0.2424\n",
      "Epoch 251/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0250 - mae: 0.1157 - val_loss: 0.1278 - val_mae: 0.2452\n",
      "Epoch 252/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0262 - mae: 0.1182 - val_loss: 0.1277 - val_mae: 0.2501\n",
      "Epoch 253/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0263 - mae: 0.1183 - val_loss: 0.1265 - val_mae: 0.2480\n",
      "Epoch 254/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0246 - mae: 0.1132 - val_loss: 0.1271 - val_mae: 0.2451\n",
      "Epoch 255/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0246 - mae: 0.1150 - val_loss: 0.1315 - val_mae: 0.2524\n",
      "Epoch 256/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0259 - mae: 0.1167 - val_loss: 0.1253 - val_mae: 0.2445\n",
      "Epoch 257/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0258 - mae: 0.1167 - val_loss: 0.1268 - val_mae: 0.2442\n",
      "Epoch 258/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0247 - mae: 0.1144 - val_loss: 0.1252 - val_mae: 0.2425\n",
      "Epoch 259/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0258 - mae: 0.1165 - val_loss: 0.1286 - val_mae: 0.2468\n",
      "Epoch 260/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0252 - mae: 0.1154 - val_loss: 0.1272 - val_mae: 0.2471\n",
      "Epoch 261/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0247 - mae: 0.1144 - val_loss: 0.1256 - val_mae: 0.2431\n",
      "Epoch 262/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0253 - mae: 0.1156 - val_loss: 0.1248 - val_mae: 0.2443\n",
      "Epoch 263/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0244 - mae: 0.1139 - val_loss: 0.1264 - val_mae: 0.2468\n",
      "Epoch 264/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0248 - mae: 0.1151 - val_loss: 0.1280 - val_mae: 0.2465\n",
      "Epoch 265/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0253 - mae: 0.1157 - val_loss: 0.1273 - val_mae: 0.2447\n",
      "Epoch 266/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0238 - mae: 0.1129 - val_loss: 0.1259 - val_mae: 0.2438\n",
      "Epoch 267/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0241 - mae: 0.1133 - val_loss: 0.1293 - val_mae: 0.2476\n",
      "Epoch 268/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0242 - mae: 0.1136 - val_loss: 0.1244 - val_mae: 0.2439\n",
      "Epoch 269/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0244 - mae: 0.1134 - val_loss: 0.1251 - val_mae: 0.2443\n",
      "Epoch 270/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0257 - mae: 0.1163 - val_loss: 0.1273 - val_mae: 0.2463\n",
      "Epoch 271/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0249 - mae: 0.1143 - val_loss: 0.1264 - val_mae: 0.2446\n",
      "Epoch 272/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0246 - mae: 0.1140 - val_loss: 0.1274 - val_mae: 0.2500\n",
      "Epoch 273/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0255 - mae: 0.1149 - val_loss: 0.1272 - val_mae: 0.2434\n",
      "Epoch 274/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0258 - mae: 0.1168 - val_loss: 0.1256 - val_mae: 0.2452\n",
      "Epoch 275/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0241 - mae: 0.1123 - val_loss: 0.1258 - val_mae: 0.2442\n",
      "Epoch 276/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0253 - mae: 0.1155 - val_loss: 0.1264 - val_mae: 0.2440\n",
      "Epoch 277/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0245 - mae: 0.1141 - val_loss: 0.1273 - val_mae: 0.2443\n",
      "Epoch 278/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0242 - mae: 0.1137 - val_loss: 0.1255 - val_mae: 0.2420\n",
      "Epoch 279/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0239 - mae: 0.1124 - val_loss: 0.1243 - val_mae: 0.2405\n",
      "Epoch 280/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0249 - mae: 0.1140 - val_loss: 0.1294 - val_mae: 0.2462\n",
      "Epoch 281/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0237 - mae: 0.1116 - val_loss: 0.1261 - val_mae: 0.2422\n",
      "Epoch 282/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0243 - mae: 0.1134 - val_loss: 0.1244 - val_mae: 0.2396\n",
      "Epoch 283/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0239 - mae: 0.1124 - val_loss: 0.1266 - val_mae: 0.2447\n",
      "Epoch 284/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0244 - mae: 0.1129 - val_loss: 0.1282 - val_mae: 0.2463\n",
      "Epoch 285/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0237 - mae: 0.1117 - val_loss: 0.1249 - val_mae: 0.2441\n",
      "Epoch 286/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0239 - mae: 0.1122 - val_loss: 0.1235 - val_mae: 0.2424\n",
      "Epoch 287/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0242 - mae: 0.1125 - val_loss: 0.1268 - val_mae: 0.2444\n",
      "Epoch 288/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0246 - mae: 0.1135 - val_loss: 0.1252 - val_mae: 0.2412\n",
      "Epoch 289/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0243 - mae: 0.1128 - val_loss: 0.1256 - val_mae: 0.2446\n",
      "Epoch 290/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0243 - mae: 0.1134 - val_loss: 0.1231 - val_mae: 0.2422\n",
      "Epoch 291/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0244 - mae: 0.1139 - val_loss: 0.1243 - val_mae: 0.2466\n",
      "Epoch 292/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0243 - mae: 0.1127 - val_loss: 0.1269 - val_mae: 0.2439\n",
      "Epoch 293/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0232 - mae: 0.1108 - val_loss: 0.1269 - val_mae: 0.2442\n",
      "Epoch 294/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0234 - mae: 0.1100 - val_loss: 0.1275 - val_mae: 0.2418\n",
      "Epoch 295/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0237 - mae: 0.1113 - val_loss: 0.1266 - val_mae: 0.2427\n",
      "Epoch 296/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0245 - mae: 0.1131 - val_loss: 0.1267 - val_mae: 0.2462\n",
      "Epoch 297/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0239 - mae: 0.1120 - val_loss: 0.1241 - val_mae: 0.2412\n",
      "Epoch 298/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0231 - mae: 0.1098 - val_loss: 0.1255 - val_mae: 0.2421\n",
      "Epoch 299/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0243 - mae: 0.1132 - val_loss: 0.1248 - val_mae: 0.2418\n",
      "Epoch 300/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0240 - mae: 0.1122 - val_loss: 0.1257 - val_mae: 0.2443\n",
      "Epoch 301/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0234 - mae: 0.1108 - val_loss: 0.1253 - val_mae: 0.2442\n",
      "Epoch 302/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0241 - mae: 0.1121 - val_loss: 0.1264 - val_mae: 0.2474\n",
      "Epoch 303/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0234 - mae: 0.1112 - val_loss: 0.1253 - val_mae: 0.2428\n",
      "Epoch 304/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0244 - mae: 0.1124 - val_loss: 0.1230 - val_mae: 0.2422\n",
      "Epoch 305/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0229 - mae: 0.1096 - val_loss: 0.1245 - val_mae: 0.2424\n",
      "Epoch 306/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0240 - mae: 0.1118 - val_loss: 0.1269 - val_mae: 0.2461\n",
      "Epoch 307/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0236 - mae: 0.1118 - val_loss: 0.1217 - val_mae: 0.2392\n",
      "Epoch 308/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0231 - mae: 0.1089 - val_loss: 0.1246 - val_mae: 0.2433\n",
      "Epoch 309/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0228 - mae: 0.1101 - val_loss: 0.1258 - val_mae: 0.2435\n",
      "Epoch 310/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0242 - mae: 0.1115 - val_loss: 0.1239 - val_mae: 0.2423\n",
      "Epoch 311/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0234 - mae: 0.1118 - val_loss: 0.1246 - val_mae: 0.2425\n",
      "Epoch 312/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0241 - mae: 0.1132 - val_loss: 0.1259 - val_mae: 0.2439\n",
      "Epoch 313/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0234 - mae: 0.1103 - val_loss: 0.1247 - val_mae: 0.2437\n",
      "Epoch 314/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0229 - mae: 0.1096 - val_loss: 0.1249 - val_mae: 0.2450\n",
      "Epoch 315/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0234 - mae: 0.1110 - val_loss: 0.1249 - val_mae: 0.2436\n",
      "Epoch 316/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0231 - mae: 0.1097 - val_loss: 0.1256 - val_mae: 0.2421\n",
      "Epoch 317/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0231 - mae: 0.1098 - val_loss: 0.1268 - val_mae: 0.2443\n",
      "Epoch 318/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0230 - mae: 0.1096 - val_loss: 0.1260 - val_mae: 0.2454\n",
      "Epoch 319/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0226 - mae: 0.1092 - val_loss: 0.1248 - val_mae: 0.2428\n",
      "Epoch 320/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0227 - mae: 0.1092 - val_loss: 0.1251 - val_mae: 0.2440\n",
      "Epoch 321/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0230 - mae: 0.1094 - val_loss: 0.1267 - val_mae: 0.2475\n",
      "Epoch 322/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0233 - mae: 0.1098 - val_loss: 0.1250 - val_mae: 0.2436\n",
      "Epoch 323/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0220 - mae: 0.1068 - val_loss: 0.1258 - val_mae: 0.2438\n",
      "Epoch 324/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0220 - mae: 0.1078 - val_loss: 0.1249 - val_mae: 0.2434\n",
      "Epoch 325/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0227 - mae: 0.1096 - val_loss: 0.1267 - val_mae: 0.2431\n",
      "Epoch 326/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0238 - mae: 0.1123 - val_loss: 0.1256 - val_mae: 0.2427\n",
      "Epoch 327/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0231 - mae: 0.1096 - val_loss: 0.1263 - val_mae: 0.2446\n",
      "Epoch 328/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0236 - mae: 0.1106 - val_loss: 0.1253 - val_mae: 0.2419\n",
      "Epoch 329/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0230 - mae: 0.1101 - val_loss: 0.1265 - val_mae: 0.2451\n",
      "Epoch 330/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0222 - mae: 0.1075 - val_loss: 0.1263 - val_mae: 0.2436\n",
      "Epoch 331/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0229 - mae: 0.1087 - val_loss: 0.1264 - val_mae: 0.2432\n",
      "Epoch 332/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0226 - mae: 0.1089 - val_loss: 0.1253 - val_mae: 0.2420\n",
      "Epoch 333/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0231 - mae: 0.1102 - val_loss: 0.1261 - val_mae: 0.2442\n",
      "Epoch 334/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0228 - mae: 0.1100 - val_loss: 0.1257 - val_mae: 0.2433\n",
      "Epoch 335/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0224 - mae: 0.1078 - val_loss: 0.1263 - val_mae: 0.2430\n",
      "Epoch 336/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0224 - mae: 0.1089 - val_loss: 0.1266 - val_mae: 0.2441\n",
      "Epoch 337/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0222 - mae: 0.1070 - val_loss: 0.1254 - val_mae: 0.2425\n",
      "Epoch 338/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0223 - mae: 0.1081 - val_loss: 0.1243 - val_mae: 0.2428\n",
      "Epoch 339/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0227 - mae: 0.1089 - val_loss: 0.1246 - val_mae: 0.2426\n",
      "Epoch 340/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0215 - mae: 0.1067 - val_loss: 0.1258 - val_mae: 0.2431\n",
      "Epoch 341/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0220 - mae: 0.1073 - val_loss: 0.1251 - val_mae: 0.2423\n",
      "Epoch 342/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0225 - mae: 0.1078 - val_loss: 0.1249 - val_mae: 0.2422\n",
      "Epoch 343/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0230 - mae: 0.1095 - val_loss: 0.1241 - val_mae: 0.2394\n",
      "Epoch 344/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0221 - mae: 0.1077 - val_loss: 0.1242 - val_mae: 0.2420\n",
      "Epoch 345/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0219 - mae: 0.1069 - val_loss: 0.1233 - val_mae: 0.2403\n",
      "Epoch 346/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0234 - mae: 0.1089 - val_loss: 0.1268 - val_mae: 0.2420\n",
      "Epoch 347/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0220 - mae: 0.1060 - val_loss: 0.1248 - val_mae: 0.2418\n",
      "Epoch 348/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0231 - mae: 0.1090 - val_loss: 0.1253 - val_mae: 0.2427\n",
      "Epoch 349/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0221 - mae: 0.1081 - val_loss: 0.1269 - val_mae: 0.2445\n",
      "Epoch 350/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0221 - mae: 0.1070 - val_loss: 0.1272 - val_mae: 0.2458\n",
      "Epoch 351/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0228 - mae: 0.1085 - val_loss: 0.1278 - val_mae: 0.2438\n",
      "Epoch 352/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0223 - mae: 0.1078 - val_loss: 0.1256 - val_mae: 0.2409\n",
      "Epoch 353/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0220 - mae: 0.1069 - val_loss: 0.1254 - val_mae: 0.2450\n",
      "Epoch 354/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0217 - mae: 0.1060 - val_loss: 0.1274 - val_mae: 0.2452\n",
      "Epoch 355/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0228 - mae: 0.1088 - val_loss: 0.1244 - val_mae: 0.2421\n",
      "Epoch 356/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0221 - mae: 0.1071 - val_loss: 0.1267 - val_mae: 0.2451\n",
      "Epoch 357/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0221 - mae: 0.1072 - val_loss: 0.1272 - val_mae: 0.2416\n",
      "Epoch 358/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0214 - mae: 0.1058 - val_loss: 0.1251 - val_mae: 0.2429\n",
      "Epoch 359/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0225 - mae: 0.1080 - val_loss: 0.1252 - val_mae: 0.2436\n",
      "Epoch 360/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0226 - mae: 0.1082 - val_loss: 0.1254 - val_mae: 0.2427\n",
      "Epoch 361/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0223 - mae: 0.1078 - val_loss: 0.1253 - val_mae: 0.2436\n",
      "Epoch 362/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0226 - mae: 0.1083 - val_loss: 0.1234 - val_mae: 0.2409\n",
      "Epoch 363/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0221 - mae: 0.1068 - val_loss: 0.1258 - val_mae: 0.2421\n",
      "Epoch 364/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0219 - mae: 0.1065 - val_loss: 0.1242 - val_mae: 0.2411\n",
      "Epoch 365/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0218 - mae: 0.1060 - val_loss: 0.1245 - val_mae: 0.2430\n",
      "Epoch 366/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0213 - mae: 0.1054 - val_loss: 0.1261 - val_mae: 0.2443\n",
      "Epoch 367/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0221 - mae: 0.1072 - val_loss: 0.1279 - val_mae: 0.2459\n",
      "Epoch 368/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0221 - mae: 0.1074 - val_loss: 0.1283 - val_mae: 0.2450\n",
      "Epoch 369/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0220 - mae: 0.1067 - val_loss: 0.1274 - val_mae: 0.2460\n",
      "Epoch 370/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0212 - mae: 0.1048 - val_loss: 0.1272 - val_mae: 0.2448\n",
      "Epoch 371/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0217 - mae: 0.1057 - val_loss: 0.1268 - val_mae: 0.2433\n",
      "Epoch 372/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0218 - mae: 0.1063 - val_loss: 0.1265 - val_mae: 0.2421\n",
      "Epoch 373/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0216 - mae: 0.1057 - val_loss: 0.1249 - val_mae: 0.2407\n",
      "Epoch 374/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0216 - mae: 0.1055 - val_loss: 0.1247 - val_mae: 0.2434\n",
      "Epoch 375/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0216 - mae: 0.1061 - val_loss: 0.1278 - val_mae: 0.2449\n",
      "Epoch 376/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0209 - mae: 0.1041 - val_loss: 0.1229 - val_mae: 0.2382\n",
      "Epoch 377/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0216 - mae: 0.1057 - val_loss: 0.1245 - val_mae: 0.2416\n",
      "Epoch 378/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0214 - mae: 0.1041 - val_loss: 0.1283 - val_mae: 0.2448\n",
      "Epoch 379/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0218 - mae: 0.1064 - val_loss: 0.1276 - val_mae: 0.2448\n",
      "Epoch 380/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0216 - mae: 0.1055 - val_loss: 0.1249 - val_mae: 0.2414\n",
      "Epoch 381/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0224 - mae: 0.1080 - val_loss: 0.1262 - val_mae: 0.2432\n",
      "Epoch 382/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0220 - mae: 0.1066 - val_loss: 0.1270 - val_mae: 0.2426\n",
      "Epoch 383/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0219 - mae: 0.1061 - val_loss: 0.1250 - val_mae: 0.2416\n",
      "Epoch 384/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0209 - mae: 0.1042 - val_loss: 0.1275 - val_mae: 0.2422\n",
      "Epoch 385/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0214 - mae: 0.1049 - val_loss: 0.1261 - val_mae: 0.2430\n",
      "Epoch 386/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0217 - mae: 0.1055 - val_loss: 0.1259 - val_mae: 0.2429\n",
      "Epoch 387/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0214 - mae: 0.1047 - val_loss: 0.1282 - val_mae: 0.2460\n",
      "Epoch 388/2000\n",
      "566/566 [==============================] - 3s 5ms/step - loss: 0.0216 - mae: 0.1052 - val_loss: 0.1243 - val_mae: 0.2408\n",
      "Epoch 389/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0213 - mae: 0.1048 - val_loss: 0.1234 - val_mae: 0.2394\n",
      "Epoch 390/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0208 - mae: 0.1040 - val_loss: 0.1248 - val_mae: 0.2384\n",
      "Epoch 391/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0222 - mae: 0.1065 - val_loss: 0.1257 - val_mae: 0.2418\n",
      "Epoch 392/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0219 - mae: 0.1059 - val_loss: 0.1251 - val_mae: 0.2419\n",
      "Epoch 393/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0215 - mae: 0.1059 - val_loss: 0.1255 - val_mae: 0.2432\n",
      "Epoch 394/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0216 - mae: 0.1054 - val_loss: 0.1269 - val_mae: 0.2459\n",
      "Epoch 395/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0217 - mae: 0.1059 - val_loss: 0.1248 - val_mae: 0.2436\n",
      "Epoch 396/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0221 - mae: 0.1060 - val_loss: 0.1261 - val_mae: 0.2440\n",
      "Epoch 397/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0210 - mae: 0.1041 - val_loss: 0.1258 - val_mae: 0.2443\n",
      "Epoch 398/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0220 - mae: 0.1064 - val_loss: 0.1256 - val_mae: 0.2411\n",
      "Epoch 399/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0209 - mae: 0.1041 - val_loss: 0.1249 - val_mae: 0.2402\n",
      "Epoch 400/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0212 - mae: 0.1046 - val_loss: 0.1260 - val_mae: 0.2455\n",
      "Epoch 401/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0207 - mae: 0.1035 - val_loss: 0.1254 - val_mae: 0.2434\n",
      "Epoch 402/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0218 - mae: 0.1062 - val_loss: 0.1269 - val_mae: 0.2462\n",
      "Epoch 403/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0204 - mae: 0.1027 - val_loss: 0.1257 - val_mae: 0.2430\n",
      "Epoch 404/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0214 - mae: 0.1051 - val_loss: 0.1257 - val_mae: 0.2416\n",
      "Epoch 405/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0212 - mae: 0.1044 - val_loss: 0.1256 - val_mae: 0.2424\n",
      "Epoch 406/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0211 - mae: 0.1043 - val_loss: 0.1257 - val_mae: 0.2425\n",
      "Epoch 407/2000\n",
      "566/566 [==============================] - 3s 6ms/step - loss: 0.0212 - mae: 0.1046 - val_loss: 0.1262 - val_mae: 0.2438\n",
      "Models saved at: model/Sun Jul 21 06:54:03 2024\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "def load_catalogs(folder: str):\n",
    "    _img_name = []\n",
    "    _angle = []\n",
    "    _throttle = []\n",
    "\n",
    "    for _file in sorted(glob.glob(f\"{folder}/*.catalog\"),\n",
    "                        key=lambda x: [\n",
    "                            int(c) if c.isdigit()\n",
    "                            else c for c in re.split(r'(\\d+)', x)]):\n",
    "        with open(_file) as f:\n",
    "            for _line in f:\n",
    "                _img_name.append(_line.split()[7][1:-2])\n",
    "                _angle.append(float(_line.split()[9][0:-1]))\n",
    "                _throttle.append(float(_line.split()[13][0:-1]))\n",
    "\n",
    "    print(f'Image count: {len(_img_name)}')\n",
    "    return _img_name, _angle, _throttle\n",
    "\n",
    "\n",
    "def load_images(_img_name: list, folder: str):\n",
    "    _image = []\n",
    "    for i in range(len(_img_name)):\n",
    "        _img = cv2.imread(os.path.join(f\"{folder}/images\", _img_name[i]))\n",
    "        assert _img.shape == (224, 224, 3),\\\n",
    "            \"img %s has shape %r\" % (_img_name[i], _img.shape)\n",
    "        _image.append(_img)\n",
    "    return _image\n",
    "\n",
    "\n",
    "def data_preprocessing(_throttle, _angle, _image):\n",
    "    _throttle = np.array(_throttle)\n",
    "    _steering = np.array(_angle)\n",
    "    _train_img = np.array(_image)\n",
    "    _label = _steering\n",
    "    _cut_height = 80\n",
    "    _train_img_cut_orig = _train_img[:, _cut_height:224, :]\n",
    "    # _train_img_cut_gray = np.dot(_train_img_cut_orig[..., :3],\n",
    "    #                              [0.299, 0.587, 0.114])\n",
    "    _train_img_cut_gray = _train_img_cut_orig\n",
    "    return _train_img_cut_orig, _train_img_cut_gray, _label\n",
    "\n",
    "\n",
    "def train_split(_train_img_cut_orig, _train_img_cut_gray, _label):\n",
    "    _X_train, _X_val, _y_train, _y_val = train_test_split(\n",
    "        _train_img_cut_gray, _label,\n",
    "        test_size=0.15, random_state=42)\n",
    "    return _X_train, _X_val, _y_train, _y_val\n",
    "\n",
    "\n",
    "def build_fine_tuned_mobilenetv2_model(input_shape):\n",
    "    base_model = tf.keras.applications.MobileNetV3Small(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    _x = base_model.output\n",
    "    _x = GlobalAveragePooling2D()(_x)\n",
    "    _x = Dense(1024, activation='relu')(_x)\n",
    "    _x = Dropout(0.5)(_x)\n",
    "    _outputs = Dense(1, activation='linear')(_x)\n",
    "\n",
    "    _model = Model(inputs=base_model.input, outputs=_outputs)\n",
    "    return _model\n",
    "\n",
    "\n",
    "def train_start(_model, _X_train, _X_val, _y_train, _y_val, \n",
    "                epochs: int=100, batch_size: int=16, patience: int=100, save_folder: str=''):\n",
    "    _optimizer = tf.optimizers.Adam(learning_rate=0.0001,\n",
    "                                    beta_1=0.9, beta_2=0.999)\n",
    "    _model.compile(optimizer=_optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
    "    _model.summary()\n",
    "    \n",
    "    # Add EarlyStopping callback\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                  patience=patience, \n",
    "                                                  restore_best_weights=True)\n",
    "    \n",
    "    # Add ModelCheckpoint callback to save the best model\n",
    "    best_model_path = os.path.join(save_folder, \"best_model.h5\")\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_path, \n",
    "                                                          monitor='val_loss', \n",
    "                                                          save_best_only=True)\n",
    "    \n",
    "    _trained_model = _model.fit(_X_train, _y_train,\n",
    "                                epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(_X_val, _y_val),\n",
    "                                callbacks=[early_stop, model_checkpoint])\n",
    "    return _trained_model\n",
    "\n",
    "\n",
    "def plot_trained_model(_trained_model, \n",
    "                       show: bool=False,\n",
    "                       save: bool=True,\n",
    "                       save_folder: str=''):\n",
    "    \n",
    "    history = _trained_model.history\n",
    "\n",
    "    plt.plot(history['loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(save_folder, f'Loss.png'),\n",
    "                    bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    plt.plot(history['mae'], label='Train MAE')\n",
    "    plt.plot(history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(save_folder, f'MAE.png'),\n",
    "                    bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"../data/data_0202\"\n",
    "    save_folder = f\"model/{time.ctime(time.time())}\"\n",
    "    # create save path\n",
    "    if not os.path.isdir(save_folder):\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    img_name, angle, throttle = load_catalogs(data_folder)\n",
    "    image = load_images(img_name, data_folder)\n",
    "    image = np.array(image)\n",
    "    train_img_cut_orig, train_img_cut_gray, label = data_preprocessing(\n",
    "        throttle, angle, image)\n",
    "    X_train, X_val, y_train, y_val = train_split(\n",
    "        train_img_cut_orig, train_img_cut_gray, label)\n",
    "\n",
    "    # Update input shape for MobileNetV2\n",
    "    model = build_fine_tuned_mobilenetv2_model(input_shape=(144, 224, 3))\n",
    "    trained_model = train_start(model, X_train, X_val, y_train, y_val, \n",
    "                               epochs=2000, save_folder=save_folder)\n",
    "    plot_trained_model(trained_model, show=False, save=True, save_folder=save_folder)\n",
    "    \n",
    "    # Save the last model\n",
    "    model.save(os.path.join(save_folder, \"last_model.h5\"))\n",
    "    \n",
    "    print(f\"Models saved at: {save_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29052ebc-1a23-4a51-a8f7-3063eb661cc0",
   "metadata": {},
   "source": [
    "### Models saved at: model/Sun Jul 21 06:54:03 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d5638-7e83-4021-b8a7-ec408fdcf645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.7.0",
   "language": "python",
   "name": "tensorflow-2.7.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
