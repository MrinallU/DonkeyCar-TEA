{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc56131d-94ec-4711-a887-ca4717d06156",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count: 10645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 05:04:11.697681: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-21 05:04:14.339307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78902 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:47:00.0, compute capability: 8.0\n",
      "2024-07-21 05:04:14.341053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78902 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:4e:00.0, compute capability: 8.0\n",
      "2024-07-21 05:04:14.342698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 78902 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:b7:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "29089792/29084464 [==============================] - 0s 0us/step\n",
      "29097984/29084464 [==============================] - 0s 0us/step\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 144, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " zero_padding2d (ZeroPadding2D)  (None, 150, 230, 3)  0          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1/conv (Conv2D)            (None, 72, 112, 64)  9408        ['zero_padding2d[0][0]']         \n",
      "                                                                                                  \n",
      " conv1/bn (BatchNormalization)  (None, 72, 112, 64)  256         ['conv1/conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv1/relu (Activation)        (None, 72, 112, 64)  0           ['conv1/bn[0][0]']               \n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadding2  (None, 74, 114, 64)  0          ['conv1/relu[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " pool1 (MaxPooling2D)           (None, 36, 56, 64)   0           ['zero_padding2d_1[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 36, 56, 64)  256         ['pool1[0][0]']                  \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_0_relu (Activatio  (None, 36, 56, 64)  0           ['conv2_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 36, 56, 128)  8192        ['conv2_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 36, 56, 128)  512        ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 36, 56, 128)  0          ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 36, 56, 32)   36864       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_concat (Concatena  (None, 36, 56, 96)  0           ['pool1[0][0]',                  \n",
      " te)                                                              'conv2_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_0_bn (BatchNormal  (None, 36, 56, 96)  384         ['conv2_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_0_relu (Activatio  (None, 36, 56, 96)  0           ['conv2_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 36, 56, 128)  12288       ['conv2_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 36, 56, 128)  512        ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 36, 56, 128)  0          ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 36, 56, 32)   36864       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_concat (Concatena  (None, 36, 56, 128)  0          ['conv2_block1_concat[0][0]',    \n",
      " te)                                                              'conv2_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_0_bn (BatchNormal  (None, 36, 56, 128)  512        ['conv2_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_0_relu (Activatio  (None, 36, 56, 128)  0          ['conv2_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 36, 56, 128)  16384       ['conv2_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 36, 56, 128)  512        ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 36, 56, 128)  0          ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 36, 56, 32)   36864       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_concat (Concatena  (None, 36, 56, 160)  0          ['conv2_block2_concat[0][0]',    \n",
      " te)                                                              'conv2_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_0_bn (BatchNormal  (None, 36, 56, 160)  640        ['conv2_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_0_relu (Activatio  (None, 36, 56, 160)  0          ['conv2_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_1_conv (Conv2D)   (None, 36, 56, 128)  20480       ['conv2_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_1_bn (BatchNormal  (None, 36, 56, 128)  512        ['conv2_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block4_1_relu (Activatio  (None, 36, 56, 128)  0          ['conv2_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block4_2_conv (Conv2D)   (None, 36, 56, 32)   36864       ['conv2_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block4_concat (Concatena  (None, 36, 56, 192)  0          ['conv2_block3_concat[0][0]',    \n",
      " te)                                                              'conv2_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_0_bn (BatchNormal  (None, 36, 56, 192)  768        ['conv2_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_0_relu (Activatio  (None, 36, 56, 192)  0          ['conv2_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_1_conv (Conv2D)   (None, 36, 56, 128)  24576       ['conv2_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_1_bn (BatchNormal  (None, 36, 56, 128)  512        ['conv2_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block5_1_relu (Activatio  (None, 36, 56, 128)  0          ['conv2_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block5_2_conv (Conv2D)   (None, 36, 56, 32)   36864       ['conv2_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block5_concat (Concatena  (None, 36, 56, 224)  0          ['conv2_block4_concat[0][0]',    \n",
      " te)                                                              'conv2_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_0_bn (BatchNormal  (None, 36, 56, 224)  896        ['conv2_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_0_relu (Activatio  (None, 36, 56, 224)  0          ['conv2_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_1_conv (Conv2D)   (None, 36, 56, 128)  28672       ['conv2_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_1_bn (BatchNormal  (None, 36, 56, 128)  512        ['conv2_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block6_1_relu (Activatio  (None, 36, 56, 128)  0          ['conv2_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block6_2_conv (Conv2D)   (None, 36, 56, 32)   36864       ['conv2_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block6_concat (Concatena  (None, 36, 56, 256)  0          ['conv2_block5_concat[0][0]',    \n",
      " te)                                                              'conv2_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_bn (BatchNormalization)  (None, 36, 56, 256)  1024        ['conv2_block6_concat[0][0]']    \n",
      "                                                                                                  \n",
      " pool2_relu (Activation)        (None, 36, 56, 256)  0           ['pool2_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool2_conv (Conv2D)            (None, 36, 56, 128)  32768       ['pool2_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool2_pool (AveragePooling2D)  (None, 18, 28, 128)  0           ['pool2_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 18, 28, 128)  512        ['pool2_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_0_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 18, 28, 128)  16384       ['conv3_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 18, 28, 32)   36864       ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_concat (Concatena  (None, 18, 28, 160)  0          ['pool2_pool[0][0]',             \n",
      " te)                                                              'conv3_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_0_bn (BatchNormal  (None, 18, 28, 160)  640        ['conv3_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_0_relu (Activatio  (None, 18, 28, 160)  0          ['conv3_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 18, 28, 128)  20480       ['conv3_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 18, 28, 32)   36864       ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_concat (Concatena  (None, 18, 28, 192)  0          ['conv3_block1_concat[0][0]',    \n",
      " te)                                                              'conv3_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_0_bn (BatchNormal  (None, 18, 28, 192)  768        ['conv3_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_0_relu (Activatio  (None, 18, 28, 192)  0          ['conv3_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 18, 28, 128)  24576       ['conv3_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 18, 28, 32)   36864       ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_concat (Concatena  (None, 18, 28, 224)  0          ['conv3_block2_concat[0][0]',    \n",
      " te)                                                              'conv3_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_0_bn (BatchNormal  (None, 18, 28, 224)  896        ['conv3_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_0_relu (Activatio  (None, 18, 28, 224)  0          ['conv3_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 18, 28, 128)  28672       ['conv3_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 18, 28, 32)   36864       ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_concat (Concatena  (None, 18, 28, 256)  0          ['conv3_block3_concat[0][0]',    \n",
      " te)                                                              'conv3_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_0_bn (BatchNormal  (None, 18, 28, 256)  1024       ['conv3_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_0_relu (Activatio  (None, 18, 28, 256)  0          ['conv3_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_1_conv (Conv2D)   (None, 18, 28, 128)  32768       ['conv3_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block5_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_2_conv (Conv2D)   (None, 18, 28, 32)   36864       ['conv3_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block5_concat (Concatena  (None, 18, 28, 288)  0          ['conv3_block4_concat[0][0]',    \n",
      " te)                                                              'conv3_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_0_bn (BatchNormal  (None, 18, 28, 288)  1152       ['conv3_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_0_relu (Activatio  (None, 18, 28, 288)  0          ['conv3_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_1_conv (Conv2D)   (None, 18, 28, 128)  36864       ['conv3_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block6_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_2_conv (Conv2D)   (None, 18, 28, 32)   36864       ['conv3_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block6_concat (Concatena  (None, 18, 28, 320)  0          ['conv3_block5_concat[0][0]',    \n",
      " te)                                                              'conv3_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_0_bn (BatchNormal  (None, 18, 28, 320)  1280       ['conv3_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_0_relu (Activatio  (None, 18, 28, 320)  0          ['conv3_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_1_conv (Conv2D)   (None, 18, 28, 128)  40960       ['conv3_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block7_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_2_conv (Conv2D)   (None, 18, 28, 32)   36864       ['conv3_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block7_concat (Concatena  (None, 18, 28, 352)  0          ['conv3_block6_concat[0][0]',    \n",
      " te)                                                              'conv3_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_0_bn (BatchNormal  (None, 18, 28, 352)  1408       ['conv3_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_0_relu (Activatio  (None, 18, 28, 352)  0          ['conv3_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_1_conv (Conv2D)   (None, 18, 28, 128)  45056       ['conv3_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block8_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_2_conv (Conv2D)   (None, 18, 28, 32)   36864       ['conv3_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block8_concat (Concatena  (None, 18, 28, 384)  0          ['conv3_block7_concat[0][0]',    \n",
      " te)                                                              'conv3_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_0_bn (BatchNormal  (None, 18, 28, 384)  1536       ['conv3_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_0_relu (Activatio  (None, 18, 28, 384)  0          ['conv3_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_1_conv (Conv2D)   (None, 18, 28, 128)  49152       ['conv3_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_1_bn (BatchNormal  (None, 18, 28, 128)  512        ['conv3_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block9_1_relu (Activatio  (None, 18, 28, 128)  0          ['conv3_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block9_2_conv (Conv2D)   (None, 18, 28, 32)   36864       ['conv3_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block9_concat (Concatena  (None, 18, 28, 416)  0          ['conv3_block8_concat[0][0]',    \n",
      " te)                                                              'conv3_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block10_0_bn (BatchNorma  (None, 18, 28, 416)  1664       ['conv3_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_0_relu (Activati  (None, 18, 28, 416)  0          ['conv3_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_1_conv (Conv2D)  (None, 18, 28, 128)  53248       ['conv3_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_1_bn (BatchNorma  (None, 18, 28, 128)  512        ['conv3_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block10_1_relu (Activati  (None, 18, 28, 128)  0          ['conv3_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block10_2_conv (Conv2D)  (None, 18, 28, 32)   36864       ['conv3_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block10_concat (Concaten  (None, 18, 28, 448)  0          ['conv3_block9_concat[0][0]',    \n",
      " ate)                                                             'conv3_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_0_bn (BatchNorma  (None, 18, 28, 448)  1792       ['conv3_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_0_relu (Activati  (None, 18, 28, 448)  0          ['conv3_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_1_conv (Conv2D)  (None, 18, 28, 128)  57344       ['conv3_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_1_bn (BatchNorma  (None, 18, 28, 128)  512        ['conv3_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block11_1_relu (Activati  (None, 18, 28, 128)  0          ['conv3_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block11_2_conv (Conv2D)  (None, 18, 28, 32)   36864       ['conv3_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block11_concat (Concaten  (None, 18, 28, 480)  0          ['conv3_block10_concat[0][0]',   \n",
      " ate)                                                             'conv3_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_0_bn (BatchNorma  (None, 18, 28, 480)  1920       ['conv3_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_0_relu (Activati  (None, 18, 28, 480)  0          ['conv3_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_1_conv (Conv2D)  (None, 18, 28, 128)  61440       ['conv3_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_1_bn (BatchNorma  (None, 18, 28, 128)  512        ['conv3_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv3_block12_1_relu (Activati  (None, 18, 28, 128)  0          ['conv3_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block12_2_conv (Conv2D)  (None, 18, 28, 32)   36864       ['conv3_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block12_concat (Concaten  (None, 18, 28, 512)  0          ['conv3_block11_concat[0][0]',   \n",
      " ate)                                                             'conv3_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_bn (BatchNormalization)  (None, 18, 28, 512)  2048        ['conv3_block12_concat[0][0]']   \n",
      "                                                                                                  \n",
      " pool3_relu (Activation)        (None, 18, 28, 512)  0           ['pool3_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool3_conv (Conv2D)            (None, 18, 28, 256)  131072      ['pool3_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool3_pool (AveragePooling2D)  (None, 9, 14, 256)   0           ['pool3_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 9, 14, 256)  1024        ['pool3_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_0_relu (Activatio  (None, 9, 14, 256)  0           ['conv4_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 9, 14, 128)   32768       ['conv4_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 9, 14, 128)  512         ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 9, 14, 128)  0           ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 9, 14, 32)    36864       ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_concat (Concatena  (None, 9, 14, 288)  0           ['pool3_pool[0][0]',             \n",
      " te)                                                              'conv4_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_0_bn (BatchNormal  (None, 9, 14, 288)  1152        ['conv4_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_0_relu (Activatio  (None, 9, 14, 288)  0           ['conv4_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 9, 14, 128)   36864       ['conv4_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 9, 14, 128)  512         ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 9, 14, 128)  0           ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 9, 14, 32)    36864       ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_concat (Concatena  (None, 9, 14, 320)  0           ['conv4_block1_concat[0][0]',    \n",
      " te)                                                              'conv4_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_0_bn (BatchNormal  (None, 9, 14, 320)  1280        ['conv4_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_0_relu (Activatio  (None, 9, 14, 320)  0           ['conv4_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 9, 14, 128)   40960       ['conv4_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 9, 14, 128)  512         ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 9, 14, 128)  0           ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 9, 14, 32)    36864       ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_concat (Concatena  (None, 9, 14, 352)  0           ['conv4_block2_concat[0][0]',    \n",
      " te)                                                              'conv4_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_0_bn (BatchNormal  (None, 9, 14, 352)  1408        ['conv4_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_0_relu (Activatio  (None, 9, 14, 352)  0           ['conv4_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 9, 14, 128)   45056       ['conv4_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 9, 14, 128)  512         ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 9, 14, 128)  0           ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 9, 14, 32)    36864       ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_concat (Concatena  (None, 9, 14, 384)  0           ['conv4_block3_concat[0][0]',    \n",
      " te)                                                              'conv4_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_0_bn (BatchNormal  (None, 9, 14, 384)  1536        ['conv4_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_0_relu (Activatio  (None, 9, 14, 384)  0           ['conv4_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 9, 14, 128)   49152       ['conv4_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 9, 14, 128)  512         ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 9, 14, 128)  0           ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 9, 14, 32)    36864       ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_concat (Concatena  (None, 9, 14, 416)  0           ['conv4_block4_concat[0][0]',    \n",
      " te)                                                              'conv4_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_0_bn (BatchNormal  (None, 9, 14, 416)  1664        ['conv4_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_0_relu (Activatio  (None, 9, 14, 416)  0           ['conv4_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 9, 14, 128)   53248       ['conv4_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 9, 14, 128)  512         ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 9, 14, 128)  0           ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 9, 14, 32)    36864       ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_concat (Concatena  (None, 9, 14, 448)  0           ['conv4_block5_concat[0][0]',    \n",
      " te)                                                              'conv4_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_0_bn (BatchNormal  (None, 9, 14, 448)  1792        ['conv4_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_0_relu (Activatio  (None, 9, 14, 448)  0           ['conv4_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_1_conv (Conv2D)   (None, 9, 14, 128)   57344       ['conv4_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_1_bn (BatchNormal  (None, 9, 14, 128)  512         ['conv4_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block7_1_relu (Activatio  (None, 9, 14, 128)  0           ['conv4_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_2_conv (Conv2D)   (None, 9, 14, 32)    36864       ['conv4_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block7_concat (Concatena  (None, 9, 14, 480)  0           ['conv4_block6_concat[0][0]',    \n",
      " te)                                                              'conv4_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_0_bn (BatchNormal  (None, 9, 14, 480)  1920        ['conv4_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_0_relu (Activatio  (None, 9, 14, 480)  0           ['conv4_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_1_conv (Conv2D)   (None, 9, 14, 128)   61440       ['conv4_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_1_bn (BatchNormal  (None, 9, 14, 128)  512         ['conv4_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block8_1_relu (Activatio  (None, 9, 14, 128)  0           ['conv4_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_2_conv (Conv2D)   (None, 9, 14, 32)    36864       ['conv4_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block8_concat (Concatena  (None, 9, 14, 512)  0           ['conv4_block7_concat[0][0]',    \n",
      " te)                                                              'conv4_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_0_bn (BatchNormal  (None, 9, 14, 512)  2048        ['conv4_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_0_relu (Activatio  (None, 9, 14, 512)  0           ['conv4_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_1_conv (Conv2D)   (None, 9, 14, 128)   65536       ['conv4_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_1_bn (BatchNormal  (None, 9, 14, 128)  512         ['conv4_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block9_1_relu (Activatio  (None, 9, 14, 128)  0           ['conv4_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_2_conv (Conv2D)   (None, 9, 14, 32)    36864       ['conv4_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block9_concat (Concatena  (None, 9, 14, 544)  0           ['conv4_block8_concat[0][0]',    \n",
      " te)                                                              'conv4_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block10_0_bn (BatchNorma  (None, 9, 14, 544)  2176        ['conv4_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_0_relu (Activati  (None, 9, 14, 544)  0           ['conv4_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_1_conv (Conv2D)  (None, 9, 14, 128)   69632       ['conv4_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_1_bn (BatchNorma  (None, 9, 14, 128)  512         ['conv4_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block10_1_relu (Activati  (None, 9, 14, 128)  0           ['conv4_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_2_conv (Conv2D)  (None, 9, 14, 32)    36864       ['conv4_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block10_concat (Concaten  (None, 9, 14, 576)  0           ['conv4_block9_concat[0][0]',    \n",
      " ate)                                                             'conv4_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_0_bn (BatchNorma  (None, 9, 14, 576)  2304        ['conv4_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_0_relu (Activati  (None, 9, 14, 576)  0           ['conv4_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_1_conv (Conv2D)  (None, 9, 14, 128)   73728       ['conv4_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_1_bn (BatchNorma  (None, 9, 14, 128)  512         ['conv4_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block11_1_relu (Activati  (None, 9, 14, 128)  0           ['conv4_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_2_conv (Conv2D)  (None, 9, 14, 32)    36864       ['conv4_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block11_concat (Concaten  (None, 9, 14, 608)  0           ['conv4_block10_concat[0][0]',   \n",
      " ate)                                                             'conv4_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_0_bn (BatchNorma  (None, 9, 14, 608)  2432        ['conv4_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_0_relu (Activati  (None, 9, 14, 608)  0           ['conv4_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_1_conv (Conv2D)  (None, 9, 14, 128)   77824       ['conv4_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_1_bn (BatchNorma  (None, 9, 14, 128)  512         ['conv4_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block12_1_relu (Activati  (None, 9, 14, 128)  0           ['conv4_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_2_conv (Conv2D)  (None, 9, 14, 32)    36864       ['conv4_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block12_concat (Concaten  (None, 9, 14, 640)  0           ['conv4_block11_concat[0][0]',   \n",
      " ate)                                                             'conv4_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_0_bn (BatchNorma  (None, 9, 14, 640)  2560        ['conv4_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_0_relu (Activati  (None, 9, 14, 640)  0           ['conv4_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_1_conv (Conv2D)  (None, 9, 14, 128)   81920       ['conv4_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_1_bn (BatchNorma  (None, 9, 14, 128)  512         ['conv4_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block13_1_relu (Activati  (None, 9, 14, 128)  0           ['conv4_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_2_conv (Conv2D)  (None, 9, 14, 32)    36864       ['conv4_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block13_concat (Concaten  (None, 9, 14, 672)  0           ['conv4_block12_concat[0][0]',   \n",
      " ate)                                                             'conv4_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_0_bn (BatchNorma  (None, 9, 14, 672)  2688        ['conv4_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_0_relu (Activati  (None, 9, 14, 672)  0           ['conv4_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_1_conv (Conv2D)  (None, 9, 14, 128)   86016       ['conv4_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_1_bn (BatchNorma  (None, 9, 14, 128)  512         ['conv4_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block14_1_relu (Activati  (None, 9, 14, 128)  0           ['conv4_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_2_conv (Conv2D)  (None, 9, 14, 32)    36864       ['conv4_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block14_concat (Concaten  (None, 9, 14, 704)  0           ['conv4_block13_concat[0][0]',   \n",
      " ate)                                                             'conv4_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_0_bn (BatchNorma  (None, 9, 14, 704)  2816        ['conv4_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_0_relu (Activati  (None, 9, 14, 704)  0           ['conv4_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_1_conv (Conv2D)  (None, 9, 14, 128)   90112       ['conv4_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_1_bn (BatchNorma  (None, 9, 14, 128)  512         ['conv4_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block15_1_relu (Activati  (None, 9, 14, 128)  0           ['conv4_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_2_conv (Conv2D)  (None, 9, 14, 32)    36864       ['conv4_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block15_concat (Concaten  (None, 9, 14, 736)  0           ['conv4_block14_concat[0][0]',   \n",
      " ate)                                                             'conv4_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_0_bn (BatchNorma  (None, 9, 14, 736)  2944        ['conv4_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_0_relu (Activati  (None, 9, 14, 736)  0           ['conv4_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_1_conv (Conv2D)  (None, 9, 14, 128)   94208       ['conv4_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_1_bn (BatchNorma  (None, 9, 14, 128)  512         ['conv4_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block16_1_relu (Activati  (None, 9, 14, 128)  0           ['conv4_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_2_conv (Conv2D)  (None, 9, 14, 32)    36864       ['conv4_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block16_concat (Concaten  (None, 9, 14, 768)  0           ['conv4_block15_concat[0][0]',   \n",
      " ate)                                                             'conv4_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_0_bn (BatchNorma  (None, 9, 14, 768)  3072        ['conv4_block16_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_0_relu (Activati  (None, 9, 14, 768)  0           ['conv4_block17_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_1_conv (Conv2D)  (None, 9, 14, 128)   98304       ['conv4_block17_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_1_bn (BatchNorma  (None, 9, 14, 128)  512         ['conv4_block17_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block17_1_relu (Activati  (None, 9, 14, 128)  0           ['conv4_block17_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_2_conv (Conv2D)  (None, 9, 14, 32)    36864       ['conv4_block17_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block17_concat (Concaten  (None, 9, 14, 800)  0           ['conv4_block16_concat[0][0]',   \n",
      " ate)                                                             'conv4_block17_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_0_bn (BatchNorma  (None, 9, 14, 800)  3200        ['conv4_block17_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_0_relu (Activati  (None, 9, 14, 800)  0           ['conv4_block18_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_1_conv (Conv2D)  (None, 9, 14, 128)   102400      ['conv4_block18_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_1_bn (BatchNorma  (None, 9, 14, 128)  512         ['conv4_block18_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block18_1_relu (Activati  (None, 9, 14, 128)  0           ['conv4_block18_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_2_conv (Conv2D)  (None, 9, 14, 32)    36864       ['conv4_block18_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block18_concat (Concaten  (None, 9, 14, 832)  0           ['conv4_block17_concat[0][0]',   \n",
      " ate)                                                             'conv4_block18_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_0_bn (BatchNorma  (None, 9, 14, 832)  3328        ['conv4_block18_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_0_relu (Activati  (None, 9, 14, 832)  0           ['conv4_block19_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_1_conv (Conv2D)  (None, 9, 14, 128)   106496      ['conv4_block19_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_1_bn (BatchNorma  (None, 9, 14, 128)  512         ['conv4_block19_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block19_1_relu (Activati  (None, 9, 14, 128)  0           ['conv4_block19_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_2_conv (Conv2D)  (None, 9, 14, 32)    36864       ['conv4_block19_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block19_concat (Concaten  (None, 9, 14, 864)  0           ['conv4_block18_concat[0][0]',   \n",
      " ate)                                                             'conv4_block19_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_0_bn (BatchNorma  (None, 9, 14, 864)  3456        ['conv4_block19_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_0_relu (Activati  (None, 9, 14, 864)  0           ['conv4_block20_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_1_conv (Conv2D)  (None, 9, 14, 128)   110592      ['conv4_block20_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_1_bn (BatchNorma  (None, 9, 14, 128)  512         ['conv4_block20_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block20_1_relu (Activati  (None, 9, 14, 128)  0           ['conv4_block20_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_2_conv (Conv2D)  (None, 9, 14, 32)    36864       ['conv4_block20_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block20_concat (Concaten  (None, 9, 14, 896)  0           ['conv4_block19_concat[0][0]',   \n",
      " ate)                                                             'conv4_block20_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_0_bn (BatchNorma  (None, 9, 14, 896)  3584        ['conv4_block20_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_0_relu (Activati  (None, 9, 14, 896)  0           ['conv4_block21_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_1_conv (Conv2D)  (None, 9, 14, 128)   114688      ['conv4_block21_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_1_bn (BatchNorma  (None, 9, 14, 128)  512         ['conv4_block21_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block21_1_relu (Activati  (None, 9, 14, 128)  0           ['conv4_block21_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_2_conv (Conv2D)  (None, 9, 14, 32)    36864       ['conv4_block21_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block21_concat (Concaten  (None, 9, 14, 928)  0           ['conv4_block20_concat[0][0]',   \n",
      " ate)                                                             'conv4_block21_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_0_bn (BatchNorma  (None, 9, 14, 928)  3712        ['conv4_block21_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_0_relu (Activati  (None, 9, 14, 928)  0           ['conv4_block22_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_1_conv (Conv2D)  (None, 9, 14, 128)   118784      ['conv4_block22_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_1_bn (BatchNorma  (None, 9, 14, 128)  512         ['conv4_block22_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block22_1_relu (Activati  (None, 9, 14, 128)  0           ['conv4_block22_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_2_conv (Conv2D)  (None, 9, 14, 32)    36864       ['conv4_block22_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block22_concat (Concaten  (None, 9, 14, 960)  0           ['conv4_block21_concat[0][0]',   \n",
      " ate)                                                             'conv4_block22_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_0_bn (BatchNorma  (None, 9, 14, 960)  3840        ['conv4_block22_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_0_relu (Activati  (None, 9, 14, 960)  0           ['conv4_block23_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_1_conv (Conv2D)  (None, 9, 14, 128)   122880      ['conv4_block23_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_1_bn (BatchNorma  (None, 9, 14, 128)  512         ['conv4_block23_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block23_1_relu (Activati  (None, 9, 14, 128)  0           ['conv4_block23_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_2_conv (Conv2D)  (None, 9, 14, 32)    36864       ['conv4_block23_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block23_concat (Concaten  (None, 9, 14, 992)  0           ['conv4_block22_concat[0][0]',   \n",
      " ate)                                                             'conv4_block23_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_0_bn (BatchNorma  (None, 9, 14, 992)  3968        ['conv4_block23_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_0_relu (Activati  (None, 9, 14, 992)  0           ['conv4_block24_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_1_conv (Conv2D)  (None, 9, 14, 128)   126976      ['conv4_block24_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_1_bn (BatchNorma  (None, 9, 14, 128)  512         ['conv4_block24_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv4_block24_1_relu (Activati  (None, 9, 14, 128)  0           ['conv4_block24_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_2_conv (Conv2D)  (None, 9, 14, 32)    36864       ['conv4_block24_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block24_concat (Concaten  (None, 9, 14, 1024)  0          ['conv4_block23_concat[0][0]',   \n",
      " ate)                                                             'conv4_block24_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " pool4_bn (BatchNormalization)  (None, 9, 14, 1024)  4096        ['conv4_block24_concat[0][0]']   \n",
      "                                                                                                  \n",
      " pool4_relu (Activation)        (None, 9, 14, 1024)  0           ['pool4_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool4_conv (Conv2D)            (None, 9, 14, 512)   524288      ['pool4_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool4_pool (AveragePooling2D)  (None, 4, 7, 512)    0           ['pool4_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 4, 7, 512)   2048        ['pool4_pool[0][0]']             \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_0_relu (Activatio  (None, 4, 7, 512)   0           ['conv5_block1_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 4, 7, 128)    65536       ['conv5_block1_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 4, 7, 128)   512         ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 4, 7, 128)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 4, 7, 32)     36864       ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_concat (Concatena  (None, 4, 7, 544)   0           ['pool4_pool[0][0]',             \n",
      " te)                                                              'conv5_block1_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_0_bn (BatchNormal  (None, 4, 7, 544)   2176        ['conv5_block1_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_0_relu (Activatio  (None, 4, 7, 544)   0           ['conv5_block2_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 4, 7, 128)    69632       ['conv5_block2_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 4, 7, 128)   512         ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 4, 7, 128)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 4, 7, 32)     36864       ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_concat (Concatena  (None, 4, 7, 576)   0           ['conv5_block1_concat[0][0]',    \n",
      " te)                                                              'conv5_block2_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_0_bn (BatchNormal  (None, 4, 7, 576)   2304        ['conv5_block2_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_0_relu (Activatio  (None, 4, 7, 576)   0           ['conv5_block3_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 4, 7, 128)    73728       ['conv5_block3_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 4, 7, 128)   512         ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 4, 7, 128)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 4, 7, 32)     36864       ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_concat (Concatena  (None, 4, 7, 608)   0           ['conv5_block2_concat[0][0]',    \n",
      " te)                                                              'conv5_block3_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_0_bn (BatchNormal  (None, 4, 7, 608)   2432        ['conv5_block3_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_0_relu (Activatio  (None, 4, 7, 608)   0           ['conv5_block4_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_1_conv (Conv2D)   (None, 4, 7, 128)    77824       ['conv5_block4_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_1_bn (BatchNormal  (None, 4, 7, 128)   512         ['conv5_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block4_1_relu (Activatio  (None, 4, 7, 128)   0           ['conv5_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block4_2_conv (Conv2D)   (None, 4, 7, 32)     36864       ['conv5_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block4_concat (Concatena  (None, 4, 7, 640)   0           ['conv5_block3_concat[0][0]',    \n",
      " te)                                                              'conv5_block4_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_0_bn (BatchNormal  (None, 4, 7, 640)   2560        ['conv5_block4_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_0_relu (Activatio  (None, 4, 7, 640)   0           ['conv5_block5_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_1_conv (Conv2D)   (None, 4, 7, 128)    81920       ['conv5_block5_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_1_bn (BatchNormal  (None, 4, 7, 128)   512         ['conv5_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block5_1_relu (Activatio  (None, 4, 7, 128)   0           ['conv5_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block5_2_conv (Conv2D)   (None, 4, 7, 32)     36864       ['conv5_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block5_concat (Concatena  (None, 4, 7, 672)   0           ['conv5_block4_concat[0][0]',    \n",
      " te)                                                              'conv5_block5_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_0_bn (BatchNormal  (None, 4, 7, 672)   2688        ['conv5_block5_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_0_relu (Activatio  (None, 4, 7, 672)   0           ['conv5_block6_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_1_conv (Conv2D)   (None, 4, 7, 128)    86016       ['conv5_block6_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_1_bn (BatchNormal  (None, 4, 7, 128)   512         ['conv5_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block6_1_relu (Activatio  (None, 4, 7, 128)   0           ['conv5_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block6_2_conv (Conv2D)   (None, 4, 7, 32)     36864       ['conv5_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block6_concat (Concatena  (None, 4, 7, 704)   0           ['conv5_block5_concat[0][0]',    \n",
      " te)                                                              'conv5_block6_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_0_bn (BatchNormal  (None, 4, 7, 704)   2816        ['conv5_block6_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_0_relu (Activatio  (None, 4, 7, 704)   0           ['conv5_block7_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_1_conv (Conv2D)   (None, 4, 7, 128)    90112       ['conv5_block7_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_1_bn (BatchNormal  (None, 4, 7, 128)   512         ['conv5_block7_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block7_1_relu (Activatio  (None, 4, 7, 128)   0           ['conv5_block7_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block7_2_conv (Conv2D)   (None, 4, 7, 32)     36864       ['conv5_block7_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block7_concat (Concatena  (None, 4, 7, 736)   0           ['conv5_block6_concat[0][0]',    \n",
      " te)                                                              'conv5_block7_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_0_bn (BatchNormal  (None, 4, 7, 736)   2944        ['conv5_block7_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_0_relu (Activatio  (None, 4, 7, 736)   0           ['conv5_block8_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_1_conv (Conv2D)   (None, 4, 7, 128)    94208       ['conv5_block8_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_1_bn (BatchNormal  (None, 4, 7, 128)   512         ['conv5_block8_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block8_1_relu (Activatio  (None, 4, 7, 128)   0           ['conv5_block8_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block8_2_conv (Conv2D)   (None, 4, 7, 32)     36864       ['conv5_block8_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block8_concat (Concatena  (None, 4, 7, 768)   0           ['conv5_block7_concat[0][0]',    \n",
      " te)                                                              'conv5_block8_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_0_bn (BatchNormal  (None, 4, 7, 768)   3072        ['conv5_block8_concat[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_0_relu (Activatio  (None, 4, 7, 768)   0           ['conv5_block9_0_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_1_conv (Conv2D)   (None, 4, 7, 128)    98304       ['conv5_block9_0_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_1_bn (BatchNormal  (None, 4, 7, 128)   512         ['conv5_block9_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block9_1_relu (Activatio  (None, 4, 7, 128)   0           ['conv5_block9_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block9_2_conv (Conv2D)   (None, 4, 7, 32)     36864       ['conv5_block9_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block9_concat (Concatena  (None, 4, 7, 800)   0           ['conv5_block8_concat[0][0]',    \n",
      " te)                                                              'conv5_block9_2_conv[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block10_0_bn (BatchNorma  (None, 4, 7, 800)   3200        ['conv5_block9_concat[0][0]']    \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_0_relu (Activati  (None, 4, 7, 800)   0           ['conv5_block10_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_1_conv (Conv2D)  (None, 4, 7, 128)    102400      ['conv5_block10_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_1_bn (BatchNorma  (None, 4, 7, 128)   512         ['conv5_block10_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block10_1_relu (Activati  (None, 4, 7, 128)   0           ['conv5_block10_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block10_2_conv (Conv2D)  (None, 4, 7, 32)     36864       ['conv5_block10_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block10_concat (Concaten  (None, 4, 7, 832)   0           ['conv5_block9_concat[0][0]',    \n",
      " ate)                                                             'conv5_block10_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_0_bn (BatchNorma  (None, 4, 7, 832)   3328        ['conv5_block10_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_0_relu (Activati  (None, 4, 7, 832)   0           ['conv5_block11_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_1_conv (Conv2D)  (None, 4, 7, 128)    106496      ['conv5_block11_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_1_bn (BatchNorma  (None, 4, 7, 128)   512         ['conv5_block11_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block11_1_relu (Activati  (None, 4, 7, 128)   0           ['conv5_block11_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block11_2_conv (Conv2D)  (None, 4, 7, 32)     36864       ['conv5_block11_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block11_concat (Concaten  (None, 4, 7, 864)   0           ['conv5_block10_concat[0][0]',   \n",
      " ate)                                                             'conv5_block11_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_0_bn (BatchNorma  (None, 4, 7, 864)   3456        ['conv5_block11_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_0_relu (Activati  (None, 4, 7, 864)   0           ['conv5_block12_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_1_conv (Conv2D)  (None, 4, 7, 128)    110592      ['conv5_block12_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_1_bn (BatchNorma  (None, 4, 7, 128)   512         ['conv5_block12_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block12_1_relu (Activati  (None, 4, 7, 128)   0           ['conv5_block12_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block12_2_conv (Conv2D)  (None, 4, 7, 32)     36864       ['conv5_block12_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block12_concat (Concaten  (None, 4, 7, 896)   0           ['conv5_block11_concat[0][0]',   \n",
      " ate)                                                             'conv5_block12_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_0_bn (BatchNorma  (None, 4, 7, 896)   3584        ['conv5_block12_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_0_relu (Activati  (None, 4, 7, 896)   0           ['conv5_block13_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_1_conv (Conv2D)  (None, 4, 7, 128)    114688      ['conv5_block13_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_1_bn (BatchNorma  (None, 4, 7, 128)   512         ['conv5_block13_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block13_1_relu (Activati  (None, 4, 7, 128)   0           ['conv5_block13_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block13_2_conv (Conv2D)  (None, 4, 7, 32)     36864       ['conv5_block13_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block13_concat (Concaten  (None, 4, 7, 928)   0           ['conv5_block12_concat[0][0]',   \n",
      " ate)                                                             'conv5_block13_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_0_bn (BatchNorma  (None, 4, 7, 928)   3712        ['conv5_block13_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_0_relu (Activati  (None, 4, 7, 928)   0           ['conv5_block14_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_1_conv (Conv2D)  (None, 4, 7, 128)    118784      ['conv5_block14_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_1_bn (BatchNorma  (None, 4, 7, 128)   512         ['conv5_block14_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block14_1_relu (Activati  (None, 4, 7, 128)   0           ['conv5_block14_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block14_2_conv (Conv2D)  (None, 4, 7, 32)     36864       ['conv5_block14_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block14_concat (Concaten  (None, 4, 7, 960)   0           ['conv5_block13_concat[0][0]',   \n",
      " ate)                                                             'conv5_block14_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_0_bn (BatchNorma  (None, 4, 7, 960)   3840        ['conv5_block14_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_0_relu (Activati  (None, 4, 7, 960)   0           ['conv5_block15_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_1_conv (Conv2D)  (None, 4, 7, 128)    122880      ['conv5_block15_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_1_bn (BatchNorma  (None, 4, 7, 128)   512         ['conv5_block15_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block15_1_relu (Activati  (None, 4, 7, 128)   0           ['conv5_block15_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block15_2_conv (Conv2D)  (None, 4, 7, 32)     36864       ['conv5_block15_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block15_concat (Concaten  (None, 4, 7, 992)   0           ['conv5_block14_concat[0][0]',   \n",
      " ate)                                                             'conv5_block15_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_0_bn (BatchNorma  (None, 4, 7, 992)   3968        ['conv5_block15_concat[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_0_relu (Activati  (None, 4, 7, 992)   0           ['conv5_block16_0_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_1_conv (Conv2D)  (None, 4, 7, 128)    126976      ['conv5_block16_0_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_1_bn (BatchNorma  (None, 4, 7, 128)   512         ['conv5_block16_1_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " conv5_block16_1_relu (Activati  (None, 4, 7, 128)   0           ['conv5_block16_1_bn[0][0]']     \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block16_2_conv (Conv2D)  (None, 4, 7, 32)     36864       ['conv5_block16_1_relu[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block16_concat (Concaten  (None, 4, 7, 1024)  0           ['conv5_block15_concat[0][0]',   \n",
      " ate)                                                             'conv5_block16_2_conv[0][0]']   \n",
      "                                                                                                  \n",
      " bn (BatchNormalization)        (None, 4, 7, 1024)   4096        ['conv5_block16_concat[0][0]']   \n",
      "                                                                                                  \n",
      " relu (Activation)              (None, 4, 7, 1024)   0           ['bn[0][0]']                     \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1024)        0           ['relu[0][0]']                   \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         1049600     ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            1025        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,088,129\n",
      "Trainable params: 1,050,625\n",
      "Non-trainable params: 7,037,504\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 05:04:21.646287: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16/566 [..............................] - ETA: 6s - loss: 35.7134 - mae: 4.7782"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 05:04:23.825487: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566/566 [==============================] - 16s 18ms/step - loss: 7.4348 - mae: 1.8744 - val_loss: 0.2517 - val_mae: 0.3963\n",
      "Epoch 2/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.4515 - mae: 0.5226 - val_loss: 0.1932 - val_mae: 0.3479\n",
      "Epoch 3/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.2506 - mae: 0.3974 - val_loss: 0.1858 - val_mae: 0.3299\n",
      "Epoch 4/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.2164 - mae: 0.3682 - val_loss: 0.1862 - val_mae: 0.3363\n",
      "Epoch 5/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.2043 - mae: 0.3560 - val_loss: 0.1725 - val_mae: 0.3145\n",
      "Epoch 6/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1949 - mae: 0.3462 - val_loss: 0.1723 - val_mae: 0.3138\n",
      "Epoch 7/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1889 - mae: 0.3412 - val_loss: 0.1730 - val_mae: 0.3223\n",
      "Epoch 8/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1860 - mae: 0.3384 - val_loss: 0.1755 - val_mae: 0.3196\n",
      "Epoch 9/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1823 - mae: 0.3333 - val_loss: 0.1690 - val_mae: 0.3121\n",
      "Epoch 10/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1820 - mae: 0.3317 - val_loss: 0.1666 - val_mae: 0.3087\n",
      "Epoch 11/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1805 - mae: 0.3310 - val_loss: 0.1650 - val_mae: 0.3038\n",
      "Epoch 12/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1780 - mae: 0.3279 - val_loss: 0.1620 - val_mae: 0.3052\n",
      "Epoch 13/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1740 - mae: 0.3230 - val_loss: 0.1655 - val_mae: 0.3127\n",
      "Epoch 14/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1754 - mae: 0.3254 - val_loss: 0.1647 - val_mae: 0.3097\n",
      "Epoch 15/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1753 - mae: 0.3259 - val_loss: 0.1577 - val_mae: 0.2973\n",
      "Epoch 16/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1704 - mae: 0.3205 - val_loss: 0.1553 - val_mae: 0.2955\n",
      "Epoch 17/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1716 - mae: 0.3206 - val_loss: 0.1623 - val_mae: 0.3081\n",
      "Epoch 18/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1690 - mae: 0.3195 - val_loss: 0.1529 - val_mae: 0.2953\n",
      "Epoch 19/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1730 - mae: 0.3217 - val_loss: 0.1533 - val_mae: 0.2957\n",
      "Epoch 20/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1716 - mae: 0.3200 - val_loss: 0.1551 - val_mae: 0.2969\n",
      "Epoch 21/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1664 - mae: 0.3136 - val_loss: 0.1555 - val_mae: 0.3013\n",
      "Epoch 22/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1650 - mae: 0.3130 - val_loss: 0.1602 - val_mae: 0.3035\n",
      "Epoch 23/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1638 - mae: 0.3117 - val_loss: 0.1515 - val_mae: 0.2933\n",
      "Epoch 24/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1649 - mae: 0.3129 - val_loss: 0.1495 - val_mae: 0.2856\n",
      "Epoch 25/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1622 - mae: 0.3092 - val_loss: 0.1492 - val_mae: 0.2902\n",
      "Epoch 26/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1612 - mae: 0.3083 - val_loss: 0.1503 - val_mae: 0.2951\n",
      "Epoch 27/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1618 - mae: 0.3091 - val_loss: 0.1498 - val_mae: 0.2837\n",
      "Epoch 28/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1626 - mae: 0.3097 - val_loss: 0.1474 - val_mae: 0.2834\n",
      "Epoch 29/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1611 - mae: 0.3082 - val_loss: 0.1544 - val_mae: 0.2926\n",
      "Epoch 30/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1589 - mae: 0.3061 - val_loss: 0.1500 - val_mae: 0.2860\n",
      "Epoch 31/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1604 - mae: 0.3069 - val_loss: 0.1508 - val_mae: 0.2924\n",
      "Epoch 32/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1584 - mae: 0.3059 - val_loss: 0.1507 - val_mae: 0.2898\n",
      "Epoch 33/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1586 - mae: 0.3049 - val_loss: 0.1482 - val_mae: 0.2836\n",
      "Epoch 34/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1594 - mae: 0.3051 - val_loss: 0.1455 - val_mae: 0.2818\n",
      "Epoch 35/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1561 - mae: 0.3024 - val_loss: 0.1494 - val_mae: 0.2924\n",
      "Epoch 36/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1553 - mae: 0.3011 - val_loss: 0.1492 - val_mae: 0.2853\n",
      "Epoch 37/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1573 - mae: 0.3048 - val_loss: 0.1441 - val_mae: 0.2784\n",
      "Epoch 38/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1568 - mae: 0.3032 - val_loss: 0.1445 - val_mae: 0.2796\n",
      "Epoch 39/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1550 - mae: 0.3005 - val_loss: 0.1459 - val_mae: 0.2839\n",
      "Epoch 40/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1550 - mae: 0.3000 - val_loss: 0.1468 - val_mae: 0.2813\n",
      "Epoch 41/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1512 - mae: 0.2977 - val_loss: 0.1472 - val_mae: 0.2839\n",
      "Epoch 42/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1534 - mae: 0.3002 - val_loss: 0.1434 - val_mae: 0.2823\n",
      "Epoch 43/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1532 - mae: 0.2983 - val_loss: 0.1458 - val_mae: 0.2833\n",
      "Epoch 44/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1502 - mae: 0.2968 - val_loss: 0.1435 - val_mae: 0.2834\n",
      "Epoch 45/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1518 - mae: 0.2970 - val_loss: 0.1458 - val_mae: 0.2847\n",
      "Epoch 46/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1515 - mae: 0.2966 - val_loss: 0.1462 - val_mae: 0.2822\n",
      "Epoch 47/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1502 - mae: 0.2956 - val_loss: 0.1524 - val_mae: 0.2864\n",
      "Epoch 48/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1488 - mae: 0.2938 - val_loss: 0.1433 - val_mae: 0.2801\n",
      "Epoch 49/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1506 - mae: 0.2965 - val_loss: 0.1449 - val_mae: 0.2820\n",
      "Epoch 50/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1491 - mae: 0.2961 - val_loss: 0.1425 - val_mae: 0.2777\n",
      "Epoch 51/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1481 - mae: 0.2917 - val_loss: 0.1451 - val_mae: 0.2775\n",
      "Epoch 52/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1504 - mae: 0.2959 - val_loss: 0.1445 - val_mae: 0.2807\n",
      "Epoch 53/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1513 - mae: 0.2965 - val_loss: 0.1448 - val_mae: 0.2812\n",
      "Epoch 54/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1476 - mae: 0.2921 - val_loss: 0.1397 - val_mae: 0.2738\n",
      "Epoch 55/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1489 - mae: 0.2939 - val_loss: 0.1430 - val_mae: 0.2790\n",
      "Epoch 56/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1463 - mae: 0.2923 - val_loss: 0.1458 - val_mae: 0.2865\n",
      "Epoch 57/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1462 - mae: 0.2892 - val_loss: 0.1446 - val_mae: 0.2845\n",
      "Epoch 58/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1482 - mae: 0.2919 - val_loss: 0.1448 - val_mae: 0.2824\n",
      "Epoch 59/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1453 - mae: 0.2891 - val_loss: 0.1419 - val_mae: 0.2738\n",
      "Epoch 60/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1479 - mae: 0.2923 - val_loss: 0.1432 - val_mae: 0.2758\n",
      "Epoch 61/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1466 - mae: 0.2920 - val_loss: 0.1434 - val_mae: 0.2793\n",
      "Epoch 62/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1454 - mae: 0.2902 - val_loss: 0.1408 - val_mae: 0.2781\n",
      "Epoch 63/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1453 - mae: 0.2887 - val_loss: 0.1431 - val_mae: 0.2803\n",
      "Epoch 64/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1454 - mae: 0.2889 - val_loss: 0.1481 - val_mae: 0.2877\n",
      "Epoch 65/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1438 - mae: 0.2877 - val_loss: 0.1399 - val_mae: 0.2724\n",
      "Epoch 66/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1453 - mae: 0.2896 - val_loss: 0.1447 - val_mae: 0.2739\n",
      "Epoch 67/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1444 - mae: 0.2880 - val_loss: 0.1445 - val_mae: 0.2868\n",
      "Epoch 68/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1432 - mae: 0.2876 - val_loss: 0.1411 - val_mae: 0.2753\n",
      "Epoch 69/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1443 - mae: 0.2874 - val_loss: 0.1401 - val_mae: 0.2714\n",
      "Epoch 70/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1427 - mae: 0.2857 - val_loss: 0.1424 - val_mae: 0.2822\n",
      "Epoch 71/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1429 - mae: 0.2864 - val_loss: 0.1441 - val_mae: 0.2786\n",
      "Epoch 72/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1420 - mae: 0.2844 - val_loss: 0.1576 - val_mae: 0.3002\n",
      "Epoch 73/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1430 - mae: 0.2862 - val_loss: 0.1382 - val_mae: 0.2687\n",
      "Epoch 74/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1419 - mae: 0.2850 - val_loss: 0.1419 - val_mae: 0.2804\n",
      "Epoch 75/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1402 - mae: 0.2822 - val_loss: 0.1413 - val_mae: 0.2794\n",
      "Epoch 76/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1419 - mae: 0.2841 - val_loss: 0.1393 - val_mae: 0.2736\n",
      "Epoch 77/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1436 - mae: 0.2873 - val_loss: 0.1380 - val_mae: 0.2750\n",
      "Epoch 78/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1400 - mae: 0.2837 - val_loss: 0.1416 - val_mae: 0.2813\n",
      "Epoch 79/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1404 - mae: 0.2829 - val_loss: 0.1458 - val_mae: 0.2801\n",
      "Epoch 80/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1407 - mae: 0.2835 - val_loss: 0.1447 - val_mae: 0.2799\n",
      "Epoch 81/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1385 - mae: 0.2816 - val_loss: 0.1465 - val_mae: 0.2822\n",
      "Epoch 82/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1397 - mae: 0.2836 - val_loss: 0.1463 - val_mae: 0.2888\n",
      "Epoch 83/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1387 - mae: 0.2819 - val_loss: 0.1400 - val_mae: 0.2749\n",
      "Epoch 84/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1394 - mae: 0.2822 - val_loss: 0.1409 - val_mae: 0.2781\n",
      "Epoch 85/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1390 - mae: 0.2827 - val_loss: 0.1366 - val_mae: 0.2665\n",
      "Epoch 86/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1380 - mae: 0.2800 - val_loss: 0.1403 - val_mae: 0.2728\n",
      "Epoch 87/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1395 - mae: 0.2829 - val_loss: 0.1393 - val_mae: 0.2709\n",
      "Epoch 88/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1388 - mae: 0.2813 - val_loss: 0.1416 - val_mae: 0.2798\n",
      "Epoch 89/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1371 - mae: 0.2812 - val_loss: 0.1399 - val_mae: 0.2765\n",
      "Epoch 90/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1379 - mae: 0.2804 - val_loss: 0.1493 - val_mae: 0.2835\n",
      "Epoch 91/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1372 - mae: 0.2800 - val_loss: 0.1411 - val_mae: 0.2751\n",
      "Epoch 92/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1365 - mae: 0.2784 - val_loss: 0.1371 - val_mae: 0.2752\n",
      "Epoch 93/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1361 - mae: 0.2786 - val_loss: 0.1342 - val_mae: 0.2660\n",
      "Epoch 94/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1370 - mae: 0.2796 - val_loss: 0.1398 - val_mae: 0.2735\n",
      "Epoch 95/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1357 - mae: 0.2770 - val_loss: 0.1366 - val_mae: 0.2700\n",
      "Epoch 96/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1356 - mae: 0.2777 - val_loss: 0.1397 - val_mae: 0.2750\n",
      "Epoch 97/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1377 - mae: 0.2808 - val_loss: 0.1386 - val_mae: 0.2750\n",
      "Epoch 98/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1345 - mae: 0.2778 - val_loss: 0.1360 - val_mae: 0.2708\n",
      "Epoch 99/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1355 - mae: 0.2797 - val_loss: 0.1358 - val_mae: 0.2679\n",
      "Epoch 100/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1347 - mae: 0.2762 - val_loss: 0.1405 - val_mae: 0.2797\n",
      "Epoch 101/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1344 - mae: 0.2767 - val_loss: 0.1364 - val_mae: 0.2678\n",
      "Epoch 102/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1334 - mae: 0.2775 - val_loss: 0.1387 - val_mae: 0.2721\n",
      "Epoch 103/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1348 - mae: 0.2779 - val_loss: 0.1428 - val_mae: 0.2782\n",
      "Epoch 104/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1326 - mae: 0.2766 - val_loss: 0.1400 - val_mae: 0.2696\n",
      "Epoch 105/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1349 - mae: 0.2755 - val_loss: 0.1361 - val_mae: 0.2715\n",
      "Epoch 106/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1318 - mae: 0.2739 - val_loss: 0.1396 - val_mae: 0.2725\n",
      "Epoch 107/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1314 - mae: 0.2736 - val_loss: 0.1424 - val_mae: 0.2787\n",
      "Epoch 108/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1350 - mae: 0.2771 - val_loss: 0.1347 - val_mae: 0.2663\n",
      "Epoch 109/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1337 - mae: 0.2763 - val_loss: 0.1370 - val_mae: 0.2699\n",
      "Epoch 110/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1332 - mae: 0.2747 - val_loss: 0.1379 - val_mae: 0.2707\n",
      "Epoch 111/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1326 - mae: 0.2745 - val_loss: 0.1363 - val_mae: 0.2712\n",
      "Epoch 112/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1344 - mae: 0.2757 - val_loss: 0.1468 - val_mae: 0.2840\n",
      "Epoch 113/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1333 - mae: 0.2752 - val_loss: 0.1338 - val_mae: 0.2621\n",
      "Epoch 114/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1317 - mae: 0.2713 - val_loss: 0.1412 - val_mae: 0.2770\n",
      "Epoch 115/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1315 - mae: 0.2733 - val_loss: 0.1359 - val_mae: 0.2675\n",
      "Epoch 116/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1309 - mae: 0.2715 - val_loss: 0.1421 - val_mae: 0.2774\n",
      "Epoch 117/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1306 - mae: 0.2728 - val_loss: 0.1342 - val_mae: 0.2687\n",
      "Epoch 118/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1313 - mae: 0.2714 - val_loss: 0.1390 - val_mae: 0.2719\n",
      "Epoch 119/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1323 - mae: 0.2734 - val_loss: 0.1399 - val_mae: 0.2765\n",
      "Epoch 120/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1308 - mae: 0.2717 - val_loss: 0.1367 - val_mae: 0.2654\n",
      "Epoch 121/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1297 - mae: 0.2707 - val_loss: 0.1339 - val_mae: 0.2675\n",
      "Epoch 122/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1289 - mae: 0.2701 - val_loss: 0.1351 - val_mae: 0.2706\n",
      "Epoch 123/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1303 - mae: 0.2722 - val_loss: 0.1352 - val_mae: 0.2711\n",
      "Epoch 124/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1317 - mae: 0.2725 - val_loss: 0.1396 - val_mae: 0.2736\n",
      "Epoch 125/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1305 - mae: 0.2714 - val_loss: 0.1354 - val_mae: 0.2714\n",
      "Epoch 126/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1317 - mae: 0.2732 - val_loss: 0.1338 - val_mae: 0.2715\n",
      "Epoch 127/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1295 - mae: 0.2704 - val_loss: 0.1405 - val_mae: 0.2791\n",
      "Epoch 128/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1294 - mae: 0.2698 - val_loss: 0.1365 - val_mae: 0.2703\n",
      "Epoch 129/2000\n",
      "566/566 [==============================] - 8s 15ms/step - loss: 0.1306 - mae: 0.2724 - val_loss: 0.1335 - val_mae: 0.2641\n",
      "Epoch 130/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1271 - mae: 0.2682 - val_loss: 0.1398 - val_mae: 0.2719\n",
      "Epoch 131/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1296 - mae: 0.2718 - val_loss: 0.1363 - val_mae: 0.2674\n",
      "Epoch 132/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1281 - mae: 0.2698 - val_loss: 0.1376 - val_mae: 0.2733\n",
      "Epoch 133/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1270 - mae: 0.2686 - val_loss: 0.1349 - val_mae: 0.2681\n",
      "Epoch 134/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1269 - mae: 0.2679 - val_loss: 0.1403 - val_mae: 0.2758\n",
      "Epoch 135/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1281 - mae: 0.2694 - val_loss: 0.1319 - val_mae: 0.2664\n",
      "Epoch 136/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1280 - mae: 0.2690 - val_loss: 0.1361 - val_mae: 0.2703\n",
      "Epoch 137/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1264 - mae: 0.2662 - val_loss: 0.1331 - val_mae: 0.2676\n",
      "Epoch 138/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1297 - mae: 0.2716 - val_loss: 0.1338 - val_mae: 0.2704\n",
      "Epoch 139/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1258 - mae: 0.2661 - val_loss: 0.1356 - val_mae: 0.2692\n",
      "Epoch 140/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1259 - mae: 0.2663 - val_loss: 0.1365 - val_mae: 0.2687\n",
      "Epoch 141/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1278 - mae: 0.2694 - val_loss: 0.1402 - val_mae: 0.2759\n",
      "Epoch 142/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1257 - mae: 0.2660 - val_loss: 0.1356 - val_mae: 0.2677\n",
      "Epoch 143/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1258 - mae: 0.2672 - val_loss: 0.1373 - val_mae: 0.2721\n",
      "Epoch 144/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1264 - mae: 0.2679 - val_loss: 0.1332 - val_mae: 0.2649\n",
      "Epoch 145/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1257 - mae: 0.2668 - val_loss: 0.1322 - val_mae: 0.2677\n",
      "Epoch 146/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1254 - mae: 0.2676 - val_loss: 0.1354 - val_mae: 0.2714\n",
      "Epoch 147/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1252 - mae: 0.2660 - val_loss: 0.1346 - val_mae: 0.2727\n",
      "Epoch 148/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1244 - mae: 0.2642 - val_loss: 0.1382 - val_mae: 0.2730\n",
      "Epoch 149/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1256 - mae: 0.2659 - val_loss: 0.1372 - val_mae: 0.2698\n",
      "Epoch 150/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1266 - mae: 0.2662 - val_loss: 0.1370 - val_mae: 0.2665\n",
      "Epoch 151/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1260 - mae: 0.2664 - val_loss: 0.1344 - val_mae: 0.2656\n",
      "Epoch 152/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1257 - mae: 0.2658 - val_loss: 0.1433 - val_mae: 0.2867\n",
      "Epoch 153/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1257 - mae: 0.2674 - val_loss: 0.1349 - val_mae: 0.2633\n",
      "Epoch 154/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1237 - mae: 0.2636 - val_loss: 0.1355 - val_mae: 0.2736\n",
      "Epoch 155/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1245 - mae: 0.2657 - val_loss: 0.1374 - val_mae: 0.2681\n",
      "Epoch 156/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1261 - mae: 0.2666 - val_loss: 0.1328 - val_mae: 0.2684\n",
      "Epoch 157/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1221 - mae: 0.2616 - val_loss: 0.1345 - val_mae: 0.2689\n",
      "Epoch 158/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1215 - mae: 0.2608 - val_loss: 0.1362 - val_mae: 0.2672\n",
      "Epoch 159/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1237 - mae: 0.2647 - val_loss: 0.1408 - val_mae: 0.2754\n",
      "Epoch 160/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1239 - mae: 0.2649 - val_loss: 0.1329 - val_mae: 0.2682\n",
      "Epoch 161/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1232 - mae: 0.2635 - val_loss: 0.1341 - val_mae: 0.2653\n",
      "Epoch 162/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1232 - mae: 0.2653 - val_loss: 0.1358 - val_mae: 0.2690\n",
      "Epoch 163/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1206 - mae: 0.2604 - val_loss: 0.1431 - val_mae: 0.2818\n",
      "Epoch 164/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1208 - mae: 0.2608 - val_loss: 0.1339 - val_mae: 0.2713\n",
      "Epoch 165/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1225 - mae: 0.2621 - val_loss: 0.1380 - val_mae: 0.2760\n",
      "Epoch 166/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1236 - mae: 0.2641 - val_loss: 0.1357 - val_mae: 0.2728\n",
      "Epoch 167/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1212 - mae: 0.2625 - val_loss: 0.1353 - val_mae: 0.2692\n",
      "Epoch 168/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1222 - mae: 0.2611 - val_loss: 0.1325 - val_mae: 0.2650\n",
      "Epoch 169/2000\n",
      "566/566 [==============================] - 8s 15ms/step - loss: 0.1211 - mae: 0.2613 - val_loss: 0.1296 - val_mae: 0.2641\n",
      "Epoch 170/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1200 - mae: 0.2604 - val_loss: 0.1353 - val_mae: 0.2704\n",
      "Epoch 171/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1223 - mae: 0.2635 - val_loss: 0.1364 - val_mae: 0.2709\n",
      "Epoch 172/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1236 - mae: 0.2633 - val_loss: 0.1320 - val_mae: 0.2653\n",
      "Epoch 173/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1195 - mae: 0.2586 - val_loss: 0.1346 - val_mae: 0.2679\n",
      "Epoch 174/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1213 - mae: 0.2607 - val_loss: 0.1332 - val_mae: 0.2720\n",
      "Epoch 175/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1216 - mae: 0.2609 - val_loss: 0.1360 - val_mae: 0.2672\n",
      "Epoch 176/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1197 - mae: 0.2603 - val_loss: 0.1351 - val_mae: 0.2747\n",
      "Epoch 177/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1195 - mae: 0.2584 - val_loss: 0.1364 - val_mae: 0.2727\n",
      "Epoch 178/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1197 - mae: 0.2598 - val_loss: 0.1328 - val_mae: 0.2653\n",
      "Epoch 179/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1220 - mae: 0.2614 - val_loss: 0.1323 - val_mae: 0.2622\n",
      "Epoch 180/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1204 - mae: 0.2604 - val_loss: 0.1311 - val_mae: 0.2607\n",
      "Epoch 181/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1189 - mae: 0.2575 - val_loss: 0.1335 - val_mae: 0.2672\n",
      "Epoch 182/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1209 - mae: 0.2607 - val_loss: 0.1324 - val_mae: 0.2644\n",
      "Epoch 183/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1198 - mae: 0.2603 - val_loss: 0.1325 - val_mae: 0.2665\n",
      "Epoch 184/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1176 - mae: 0.2572 - val_loss: 0.1336 - val_mae: 0.2700\n",
      "Epoch 185/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1202 - mae: 0.2592 - val_loss: 0.1340 - val_mae: 0.2683\n",
      "Epoch 186/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1183 - mae: 0.2584 - val_loss: 0.1341 - val_mae: 0.2667\n",
      "Epoch 187/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1187 - mae: 0.2592 - val_loss: 0.1367 - val_mae: 0.2683\n",
      "Epoch 188/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1183 - mae: 0.2573 - val_loss: 0.1325 - val_mae: 0.2667\n",
      "Epoch 189/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1183 - mae: 0.2563 - val_loss: 0.1300 - val_mae: 0.2616\n",
      "Epoch 190/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1164 - mae: 0.2556 - val_loss: 0.1298 - val_mae: 0.2610\n",
      "Epoch 191/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1156 - mae: 0.2531 - val_loss: 0.1303 - val_mae: 0.2611\n",
      "Epoch 192/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1192 - mae: 0.2577 - val_loss: 0.1307 - val_mae: 0.2652\n",
      "Epoch 193/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1167 - mae: 0.2548 - val_loss: 0.1330 - val_mae: 0.2648\n",
      "Epoch 194/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1185 - mae: 0.2594 - val_loss: 0.1323 - val_mae: 0.2645\n",
      "Epoch 195/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1181 - mae: 0.2592 - val_loss: 0.1367 - val_mae: 0.2725\n",
      "Epoch 196/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1170 - mae: 0.2556 - val_loss: 0.1377 - val_mae: 0.2717\n",
      "Epoch 197/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1168 - mae: 0.2571 - val_loss: 0.1347 - val_mae: 0.2648\n",
      "Epoch 198/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1183 - mae: 0.2576 - val_loss: 0.1386 - val_mae: 0.2693\n",
      "Epoch 199/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1165 - mae: 0.2558 - val_loss: 0.1319 - val_mae: 0.2690\n",
      "Epoch 200/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1168 - mae: 0.2549 - val_loss: 0.1330 - val_mae: 0.2688\n",
      "Epoch 201/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1169 - mae: 0.2556 - val_loss: 0.1352 - val_mae: 0.2672\n",
      "Epoch 202/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1156 - mae: 0.2537 - val_loss: 0.1324 - val_mae: 0.2632\n",
      "Epoch 203/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1163 - mae: 0.2559 - val_loss: 0.1315 - val_mae: 0.2681\n",
      "Epoch 204/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1151 - mae: 0.2542 - val_loss: 0.1304 - val_mae: 0.2641\n",
      "Epoch 205/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1162 - mae: 0.2551 - val_loss: 0.1303 - val_mae: 0.2623\n",
      "Epoch 206/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1170 - mae: 0.2554 - val_loss: 0.1368 - val_mae: 0.2710\n",
      "Epoch 207/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1146 - mae: 0.2533 - val_loss: 0.1298 - val_mae: 0.2646\n",
      "Epoch 208/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1162 - mae: 0.2554 - val_loss: 0.1321 - val_mae: 0.2653\n",
      "Epoch 209/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1155 - mae: 0.2549 - val_loss: 0.1341 - val_mae: 0.2665\n",
      "Epoch 210/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1140 - mae: 0.2528 - val_loss: 0.1324 - val_mae: 0.2650\n",
      "Epoch 211/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1147 - mae: 0.2524 - val_loss: 0.1333 - val_mae: 0.2675\n",
      "Epoch 212/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1175 - mae: 0.2553 - val_loss: 0.1358 - val_mae: 0.2688\n",
      "Epoch 213/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1149 - mae: 0.2544 - val_loss: 0.1343 - val_mae: 0.2653\n",
      "Epoch 214/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1127 - mae: 0.2500 - val_loss: 0.1356 - val_mae: 0.2684\n",
      "Epoch 215/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1140 - mae: 0.2526 - val_loss: 0.1349 - val_mae: 0.2666\n",
      "Epoch 216/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1151 - mae: 0.2542 - val_loss: 0.1354 - val_mae: 0.2724\n",
      "Epoch 217/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1143 - mae: 0.2530 - val_loss: 0.1352 - val_mae: 0.2707\n",
      "Epoch 218/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1124 - mae: 0.2504 - val_loss: 0.1325 - val_mae: 0.2665\n",
      "Epoch 219/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1148 - mae: 0.2548 - val_loss: 0.1308 - val_mae: 0.2613\n",
      "Epoch 220/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1145 - mae: 0.2532 - val_loss: 0.1315 - val_mae: 0.2622\n",
      "Epoch 221/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1143 - mae: 0.2522 - val_loss: 0.1385 - val_mae: 0.2728\n",
      "Epoch 222/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1135 - mae: 0.2531 - val_loss: 0.1310 - val_mae: 0.2663\n",
      "Epoch 223/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1132 - mae: 0.2520 - val_loss: 0.1315 - val_mae: 0.2662\n",
      "Epoch 224/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1163 - mae: 0.2548 - val_loss: 0.1349 - val_mae: 0.2655\n",
      "Epoch 225/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1130 - mae: 0.2504 - val_loss: 0.1323 - val_mae: 0.2653\n",
      "Epoch 226/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1125 - mae: 0.2507 - val_loss: 0.1309 - val_mae: 0.2621\n",
      "Epoch 227/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1131 - mae: 0.2508 - val_loss: 0.1322 - val_mae: 0.2649\n",
      "Epoch 228/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1134 - mae: 0.2516 - val_loss: 0.1301 - val_mae: 0.2620\n",
      "Epoch 229/2000\n",
      "566/566 [==============================] - 9s 15ms/step - loss: 0.1146 - mae: 0.2522 - val_loss: 0.1293 - val_mae: 0.2596\n",
      "Epoch 230/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1135 - mae: 0.2511 - val_loss: 0.1313 - val_mae: 0.2643\n",
      "Epoch 231/2000\n",
      "566/566 [==============================] - 8s 15ms/step - loss: 0.1123 - mae: 0.2500 - val_loss: 0.1279 - val_mae: 0.2588\n",
      "Epoch 232/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1126 - mae: 0.2496 - val_loss: 0.1316 - val_mae: 0.2648\n",
      "Epoch 233/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1135 - mae: 0.2510 - val_loss: 0.1318 - val_mae: 0.2662\n",
      "Epoch 234/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1134 - mae: 0.2516 - val_loss: 0.1320 - val_mae: 0.2640\n",
      "Epoch 235/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1136 - mae: 0.2511 - val_loss: 0.1345 - val_mae: 0.2660\n",
      "Epoch 236/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1121 - mae: 0.2504 - val_loss: 0.1281 - val_mae: 0.2592\n",
      "Epoch 237/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1123 - mae: 0.2495 - val_loss: 0.1289 - val_mae: 0.2627\n",
      "Epoch 238/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1116 - mae: 0.2503 - val_loss: 0.1324 - val_mae: 0.2657\n",
      "Epoch 239/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1104 - mae: 0.2481 - val_loss: 0.1316 - val_mae: 0.2619\n",
      "Epoch 240/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1124 - mae: 0.2501 - val_loss: 0.1298 - val_mae: 0.2635\n",
      "Epoch 241/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1123 - mae: 0.2487 - val_loss: 0.1333 - val_mae: 0.2681\n",
      "Epoch 242/2000\n",
      "566/566 [==============================] - 8s 15ms/step - loss: 0.1105 - mae: 0.2480 - val_loss: 0.1274 - val_mae: 0.2596\n",
      "Epoch 243/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1096 - mae: 0.2470 - val_loss: 0.1348 - val_mae: 0.2699\n",
      "Epoch 244/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1102 - mae: 0.2478 - val_loss: 0.1287 - val_mae: 0.2608\n",
      "Epoch 245/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1122 - mae: 0.2503 - val_loss: 0.1337 - val_mae: 0.2644\n",
      "Epoch 246/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1117 - mae: 0.2511 - val_loss: 0.1309 - val_mae: 0.2646\n",
      "Epoch 247/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1118 - mae: 0.2497 - val_loss: 0.1301 - val_mae: 0.2656\n",
      "Epoch 248/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1117 - mae: 0.2503 - val_loss: 0.1361 - val_mae: 0.2695\n",
      "Epoch 249/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1101 - mae: 0.2479 - val_loss: 0.1318 - val_mae: 0.2653\n",
      "Epoch 250/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1088 - mae: 0.2463 - val_loss: 0.1297 - val_mae: 0.2653\n",
      "Epoch 251/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1095 - mae: 0.2481 - val_loss: 0.1342 - val_mae: 0.2652\n",
      "Epoch 252/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1094 - mae: 0.2463 - val_loss: 0.1302 - val_mae: 0.2594\n",
      "Epoch 253/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1101 - mae: 0.2485 - val_loss: 0.1301 - val_mae: 0.2601\n",
      "Epoch 254/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1120 - mae: 0.2483 - val_loss: 0.1287 - val_mae: 0.2614\n",
      "Epoch 255/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1104 - mae: 0.2469 - val_loss: 0.1277 - val_mae: 0.2601\n",
      "Epoch 256/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1097 - mae: 0.2466 - val_loss: 0.1343 - val_mae: 0.2694\n",
      "Epoch 257/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1115 - mae: 0.2489 - val_loss: 0.1379 - val_mae: 0.2672\n",
      "Epoch 258/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1087 - mae: 0.2458 - val_loss: 0.1316 - val_mae: 0.2657\n",
      "Epoch 259/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1092 - mae: 0.2465 - val_loss: 0.1319 - val_mae: 0.2632\n",
      "Epoch 260/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1082 - mae: 0.2458 - val_loss: 0.1309 - val_mae: 0.2622\n",
      "Epoch 261/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1109 - mae: 0.2491 - val_loss: 0.1317 - val_mae: 0.2635\n",
      "Epoch 262/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1088 - mae: 0.2471 - val_loss: 0.1309 - val_mae: 0.2668\n",
      "Epoch 263/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1076 - mae: 0.2450 - val_loss: 0.1331 - val_mae: 0.2698\n",
      "Epoch 264/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1074 - mae: 0.2452 - val_loss: 0.1305 - val_mae: 0.2667\n",
      "Epoch 265/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1074 - mae: 0.2456 - val_loss: 0.1281 - val_mae: 0.2616\n",
      "Epoch 266/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1067 - mae: 0.2443 - val_loss: 0.1302 - val_mae: 0.2640\n",
      "Epoch 267/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1089 - mae: 0.2474 - val_loss: 0.1303 - val_mae: 0.2593\n",
      "Epoch 268/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1104 - mae: 0.2477 - val_loss: 0.1325 - val_mae: 0.2680\n",
      "Epoch 269/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1083 - mae: 0.2455 - val_loss: 0.1288 - val_mae: 0.2638\n",
      "Epoch 270/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1075 - mae: 0.2451 - val_loss: 0.1279 - val_mae: 0.2598\n",
      "Epoch 271/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1081 - mae: 0.2442 - val_loss: 0.1290 - val_mae: 0.2639\n",
      "Epoch 272/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1095 - mae: 0.2458 - val_loss: 0.1310 - val_mae: 0.2652\n",
      "Epoch 273/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1097 - mae: 0.2471 - val_loss: 0.1316 - val_mae: 0.2625\n",
      "Epoch 274/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1076 - mae: 0.2437 - val_loss: 0.1316 - val_mae: 0.2638\n",
      "Epoch 275/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1072 - mae: 0.2441 - val_loss: 0.1366 - val_mae: 0.2708\n",
      "Epoch 276/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1092 - mae: 0.2458 - val_loss: 0.1280 - val_mae: 0.2584\n",
      "Epoch 277/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1078 - mae: 0.2437 - val_loss: 0.1298 - val_mae: 0.2594\n",
      "Epoch 278/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1073 - mae: 0.2446 - val_loss: 0.1321 - val_mae: 0.2603\n",
      "Epoch 279/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1068 - mae: 0.2421 - val_loss: 0.1338 - val_mae: 0.2632\n",
      "Epoch 280/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1053 - mae: 0.2406 - val_loss: 0.1289 - val_mae: 0.2612\n",
      "Epoch 281/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1082 - mae: 0.2448 - val_loss: 0.1300 - val_mae: 0.2646\n",
      "Epoch 282/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1066 - mae: 0.2426 - val_loss: 0.1299 - val_mae: 0.2657\n",
      "Epoch 283/2000\n",
      "566/566 [==============================] - 8s 15ms/step - loss: 0.1065 - mae: 0.2428 - val_loss: 0.1273 - val_mae: 0.2597\n",
      "Epoch 284/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1040 - mae: 0.2395 - val_loss: 0.1289 - val_mae: 0.2586\n",
      "Epoch 285/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1046 - mae: 0.2408 - val_loss: 0.1308 - val_mae: 0.2651\n",
      "Epoch 286/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1071 - mae: 0.2439 - val_loss: 0.1278 - val_mae: 0.2590\n",
      "Epoch 287/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1053 - mae: 0.2428 - val_loss: 0.1283 - val_mae: 0.2579\n",
      "Epoch 288/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1046 - mae: 0.2413 - val_loss: 0.1318 - val_mae: 0.2646\n",
      "Epoch 289/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1066 - mae: 0.2424 - val_loss: 0.1321 - val_mae: 0.2709\n",
      "Epoch 290/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1069 - mae: 0.2428 - val_loss: 0.1278 - val_mae: 0.2638\n",
      "Epoch 291/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1053 - mae: 0.2418 - val_loss: 0.1334 - val_mae: 0.2631\n",
      "Epoch 292/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1055 - mae: 0.2417 - val_loss: 0.1282 - val_mae: 0.2583\n",
      "Epoch 293/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1054 - mae: 0.2424 - val_loss: 0.1278 - val_mae: 0.2630\n",
      "Epoch 294/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1062 - mae: 0.2430 - val_loss: 0.1370 - val_mae: 0.2678\n",
      "Epoch 295/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1064 - mae: 0.2424 - val_loss: 0.1317 - val_mae: 0.2640\n",
      "Epoch 296/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1055 - mae: 0.2427 - val_loss: 0.1311 - val_mae: 0.2612\n",
      "Epoch 297/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1065 - mae: 0.2428 - val_loss: 0.1335 - val_mae: 0.2669\n",
      "Epoch 298/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1064 - mae: 0.2429 - val_loss: 0.1362 - val_mae: 0.2691\n",
      "Epoch 299/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1051 - mae: 0.2420 - val_loss: 0.1292 - val_mae: 0.2620\n",
      "Epoch 300/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1043 - mae: 0.2405 - val_loss: 0.1320 - val_mae: 0.2667\n",
      "Epoch 301/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1058 - mae: 0.2423 - val_loss: 0.1334 - val_mae: 0.2650\n",
      "Epoch 302/2000\n",
      "566/566 [==============================] - 8s 15ms/step - loss: 0.1051 - mae: 0.2412 - val_loss: 0.1271 - val_mae: 0.2563\n",
      "Epoch 303/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1049 - mae: 0.2412 - val_loss: 0.1351 - val_mae: 0.2712\n",
      "Epoch 304/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1055 - mae: 0.2427 - val_loss: 0.1302 - val_mae: 0.2642\n",
      "Epoch 305/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1049 - mae: 0.2401 - val_loss: 0.1302 - val_mae: 0.2610\n",
      "Epoch 306/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1049 - mae: 0.2423 - val_loss: 0.1280 - val_mae: 0.2587\n",
      "Epoch 307/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1062 - mae: 0.2414 - val_loss: 0.1294 - val_mae: 0.2623\n",
      "Epoch 308/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1048 - mae: 0.2404 - val_loss: 0.1304 - val_mae: 0.2592\n",
      "Epoch 309/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1029 - mae: 0.2388 - val_loss: 0.1323 - val_mae: 0.2686\n",
      "Epoch 310/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1033 - mae: 0.2392 - val_loss: 0.1325 - val_mae: 0.2666\n",
      "Epoch 311/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1052 - mae: 0.2426 - val_loss: 0.1315 - val_mae: 0.2659\n",
      "Epoch 312/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1030 - mae: 0.2386 - val_loss: 0.1313 - val_mae: 0.2696\n",
      "Epoch 313/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1048 - mae: 0.2390 - val_loss: 0.1317 - val_mae: 0.2634\n",
      "Epoch 314/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1045 - mae: 0.2410 - val_loss: 0.1300 - val_mae: 0.2607\n",
      "Epoch 315/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1049 - mae: 0.2408 - val_loss: 0.1277 - val_mae: 0.2591\n",
      "Epoch 316/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1024 - mae: 0.2372 - val_loss: 0.1293 - val_mae: 0.2617\n",
      "Epoch 317/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1038 - mae: 0.2385 - val_loss: 0.1273 - val_mae: 0.2608\n",
      "Epoch 318/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1050 - mae: 0.2405 - val_loss: 0.1269 - val_mae: 0.2561\n",
      "Epoch 319/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1041 - mae: 0.2392 - val_loss: 0.1293 - val_mae: 0.2569\n",
      "Epoch 320/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1025 - mae: 0.2375 - val_loss: 0.1283 - val_mae: 0.2570\n",
      "Epoch 321/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.1043 - mae: 0.2396 - val_loss: 0.1321 - val_mae: 0.2656\n",
      "Epoch 322/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1039 - mae: 0.2391 - val_loss: 0.1318 - val_mae: 0.2591\n",
      "Epoch 323/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1021 - mae: 0.2364 - val_loss: 0.1294 - val_mae: 0.2608\n",
      "Epoch 324/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1024 - mae: 0.2378 - val_loss: 0.1275 - val_mae: 0.2565\n",
      "Epoch 325/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1036 - mae: 0.2382 - val_loss: 0.1295 - val_mae: 0.2574\n",
      "Epoch 326/2000\n",
      "566/566 [==============================] - 8s 15ms/step - loss: 0.1020 - mae: 0.2375 - val_loss: 0.1267 - val_mae: 0.2559\n",
      "Epoch 327/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1013 - mae: 0.2360 - val_loss: 0.1380 - val_mae: 0.2743\n",
      "Epoch 328/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1022 - mae: 0.2379 - val_loss: 0.1291 - val_mae: 0.2611\n",
      "Epoch 329/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1030 - mae: 0.2396 - val_loss: 0.1284 - val_mae: 0.2569\n",
      "Epoch 330/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1019 - mae: 0.2379 - val_loss: 0.1301 - val_mae: 0.2625\n",
      "Epoch 331/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1008 - mae: 0.2361 - val_loss: 0.1290 - val_mae: 0.2607\n",
      "Epoch 332/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1029 - mae: 0.2382 - val_loss: 0.1294 - val_mae: 0.2600\n",
      "Epoch 333/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1010 - mae: 0.2371 - val_loss: 0.1311 - val_mae: 0.2635\n",
      "Epoch 334/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1013 - mae: 0.2373 - val_loss: 0.1280 - val_mae: 0.2609\n",
      "Epoch 335/2000\n",
      "566/566 [==============================] - 8s 15ms/step - loss: 0.1014 - mae: 0.2375 - val_loss: 0.1259 - val_mae: 0.2585\n",
      "Epoch 336/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1007 - mae: 0.2350 - val_loss: 0.1351 - val_mae: 0.2698\n",
      "Epoch 337/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1008 - mae: 0.2355 - val_loss: 0.1288 - val_mae: 0.2599\n",
      "Epoch 338/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1010 - mae: 0.2360 - val_loss: 0.1325 - val_mae: 0.2634\n",
      "Epoch 339/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1036 - mae: 0.2392 - val_loss: 0.1271 - val_mae: 0.2562\n",
      "Epoch 340/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1013 - mae: 0.2363 - val_loss: 0.1277 - val_mae: 0.2572\n",
      "Epoch 341/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1023 - mae: 0.2381 - val_loss: 0.1274 - val_mae: 0.2579\n",
      "Epoch 342/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1017 - mae: 0.2381 - val_loss: 0.1304 - val_mae: 0.2635\n",
      "Epoch 343/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1023 - mae: 0.2382 - val_loss: 0.1321 - val_mae: 0.2633\n",
      "Epoch 344/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.1016 - mae: 0.2355 - val_loss: 0.1274 - val_mae: 0.2570\n",
      "Epoch 345/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0989 - mae: 0.2335 - val_loss: 0.1333 - val_mae: 0.2680\n",
      "Epoch 346/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0983 - mae: 0.2332 - val_loss: 0.1260 - val_mae: 0.2588\n",
      "Epoch 347/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1015 - mae: 0.2375 - val_loss: 0.1312 - val_mae: 0.2608\n",
      "Epoch 348/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1006 - mae: 0.2351 - val_loss: 0.1332 - val_mae: 0.2651\n",
      "Epoch 349/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0998 - mae: 0.2331 - val_loss: 0.1299 - val_mae: 0.2677\n",
      "Epoch 350/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0994 - mae: 0.2339 - val_loss: 0.1325 - val_mae: 0.2660\n",
      "Epoch 351/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0996 - mae: 0.2336 - val_loss: 0.1288 - val_mae: 0.2577\n",
      "Epoch 352/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1007 - mae: 0.2362 - val_loss: 0.1267 - val_mae: 0.2591\n",
      "Epoch 353/2000\n",
      "566/566 [==============================] - 8s 15ms/step - loss: 0.0980 - mae: 0.2326 - val_loss: 0.1248 - val_mae: 0.2566\n",
      "Epoch 354/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1000 - mae: 0.2338 - val_loss: 0.1257 - val_mae: 0.2571\n",
      "Epoch 355/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0998 - mae: 0.2344 - val_loss: 0.1270 - val_mae: 0.2565\n",
      "Epoch 356/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1005 - mae: 0.2355 - val_loss: 0.1289 - val_mae: 0.2606\n",
      "Epoch 357/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1013 - mae: 0.2367 - val_loss: 0.1293 - val_mae: 0.2590\n",
      "Epoch 358/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0971 - mae: 0.2321 - val_loss: 0.1290 - val_mae: 0.2616\n",
      "Epoch 359/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0997 - mae: 0.2355 - val_loss: 0.1292 - val_mae: 0.2591\n",
      "Epoch 360/2000\n",
      "566/566 [==============================] - 8s 15ms/step - loss: 0.0984 - mae: 0.2327 - val_loss: 0.1239 - val_mae: 0.2562\n",
      "Epoch 361/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0976 - mae: 0.2313 - val_loss: 0.1299 - val_mae: 0.2615\n",
      "Epoch 362/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0981 - mae: 0.2323 - val_loss: 0.1255 - val_mae: 0.2546\n",
      "Epoch 363/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0989 - mae: 0.2333 - val_loss: 0.1283 - val_mae: 0.2581\n",
      "Epoch 364/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0976 - mae: 0.2309 - val_loss: 0.1297 - val_mae: 0.2650\n",
      "Epoch 365/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0981 - mae: 0.2320 - val_loss: 0.1279 - val_mae: 0.2612\n",
      "Epoch 366/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.1017 - mae: 0.2364 - val_loss: 0.1302 - val_mae: 0.2639\n",
      "Epoch 367/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0978 - mae: 0.2322 - val_loss: 0.1298 - val_mae: 0.2635\n",
      "Epoch 368/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0986 - mae: 0.2339 - val_loss: 0.1259 - val_mae: 0.2548\n",
      "Epoch 369/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0975 - mae: 0.2307 - val_loss: 0.1299 - val_mae: 0.2592\n",
      "Epoch 370/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0979 - mae: 0.2319 - val_loss: 0.1280 - val_mae: 0.2606\n",
      "Epoch 371/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0969 - mae: 0.2310 - val_loss: 0.1308 - val_mae: 0.2604\n",
      "Epoch 372/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0974 - mae: 0.2307 - val_loss: 0.1277 - val_mae: 0.2591\n",
      "Epoch 373/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0990 - mae: 0.2334 - val_loss: 0.1275 - val_mae: 0.2591\n",
      "Epoch 374/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0954 - mae: 0.2292 - val_loss: 0.1291 - val_mae: 0.2597\n",
      "Epoch 375/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0962 - mae: 0.2302 - val_loss: 0.1268 - val_mae: 0.2560\n",
      "Epoch 376/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0978 - mae: 0.2326 - val_loss: 0.1272 - val_mae: 0.2550\n",
      "Epoch 377/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0970 - mae: 0.2307 - val_loss: 0.1300 - val_mae: 0.2632\n",
      "Epoch 378/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0971 - mae: 0.2301 - val_loss: 0.1366 - val_mae: 0.2695\n",
      "Epoch 379/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0962 - mae: 0.2305 - val_loss: 0.1282 - val_mae: 0.2606\n",
      "Epoch 380/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0954 - mae: 0.2299 - val_loss: 0.1288 - val_mae: 0.2595\n",
      "Epoch 381/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0973 - mae: 0.2303 - val_loss: 0.1332 - val_mae: 0.2670\n",
      "Epoch 382/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0969 - mae: 0.2308 - val_loss: 0.1315 - val_mae: 0.2606\n",
      "Epoch 383/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0982 - mae: 0.2319 - val_loss: 0.1321 - val_mae: 0.2698\n",
      "Epoch 384/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0967 - mae: 0.2309 - val_loss: 0.1313 - val_mae: 0.2639\n",
      "Epoch 385/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0942 - mae: 0.2262 - val_loss: 0.1276 - val_mae: 0.2599\n",
      "Epoch 386/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0946 - mae: 0.2285 - val_loss: 0.1287 - val_mae: 0.2574\n",
      "Epoch 387/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0971 - mae: 0.2295 - val_loss: 0.1264 - val_mae: 0.2563\n",
      "Epoch 388/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0964 - mae: 0.2309 - val_loss: 0.1281 - val_mae: 0.2612\n",
      "Epoch 389/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0964 - mae: 0.2299 - val_loss: 0.1256 - val_mae: 0.2572\n",
      "Epoch 390/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0968 - mae: 0.2296 - val_loss: 0.1303 - val_mae: 0.2585\n",
      "Epoch 391/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0964 - mae: 0.2287 - val_loss: 0.1301 - val_mae: 0.2619\n",
      "Epoch 392/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0960 - mae: 0.2285 - val_loss: 0.1328 - val_mae: 0.2682\n",
      "Epoch 393/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0968 - mae: 0.2303 - val_loss: 0.1345 - val_mae: 0.2633\n",
      "Epoch 394/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0984 - mae: 0.2327 - val_loss: 0.1286 - val_mae: 0.2606\n",
      "Epoch 395/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0963 - mae: 0.2302 - val_loss: 0.1264 - val_mae: 0.2573\n",
      "Epoch 396/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0943 - mae: 0.2283 - val_loss: 0.1258 - val_mae: 0.2573\n",
      "Epoch 397/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0946 - mae: 0.2279 - val_loss: 0.1280 - val_mae: 0.2582\n",
      "Epoch 398/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0943 - mae: 0.2281 - val_loss: 0.1288 - val_mae: 0.2603\n",
      "Epoch 399/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0961 - mae: 0.2288 - val_loss: 0.1293 - val_mae: 0.2620\n",
      "Epoch 400/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0951 - mae: 0.2279 - val_loss: 0.1277 - val_mae: 0.2595\n",
      "Epoch 401/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0955 - mae: 0.2297 - val_loss: 0.1308 - val_mae: 0.2666\n",
      "Epoch 402/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0969 - mae: 0.2307 - val_loss: 0.1266 - val_mae: 0.2598\n",
      "Epoch 403/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0934 - mae: 0.2261 - val_loss: 0.1290 - val_mae: 0.2628\n",
      "Epoch 404/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0935 - mae: 0.2264 - val_loss: 0.1247 - val_mae: 0.2548\n",
      "Epoch 405/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0931 - mae: 0.2257 - val_loss: 0.1271 - val_mae: 0.2551\n",
      "Epoch 406/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0978 - mae: 0.2312 - val_loss: 0.1279 - val_mae: 0.2586\n",
      "Epoch 407/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0958 - mae: 0.2294 - val_loss: 0.1358 - val_mae: 0.2697\n",
      "Epoch 408/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0972 - mae: 0.2306 - val_loss: 0.1270 - val_mae: 0.2622\n",
      "Epoch 409/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0947 - mae: 0.2282 - val_loss: 0.1300 - val_mae: 0.2617\n",
      "Epoch 410/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0935 - mae: 0.2276 - val_loss: 0.1303 - val_mae: 0.2624\n",
      "Epoch 411/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0938 - mae: 0.2282 - val_loss: 0.1253 - val_mae: 0.2588\n",
      "Epoch 412/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0941 - mae: 0.2277 - val_loss: 0.1275 - val_mae: 0.2573\n",
      "Epoch 413/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0943 - mae: 0.2275 - val_loss: 0.1320 - val_mae: 0.2671\n",
      "Epoch 414/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0951 - mae: 0.2278 - val_loss: 0.1359 - val_mae: 0.2716\n",
      "Epoch 415/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0960 - mae: 0.2298 - val_loss: 0.1261 - val_mae: 0.2565\n",
      "Epoch 416/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0940 - mae: 0.2283 - val_loss: 0.1284 - val_mae: 0.2608\n",
      "Epoch 417/2000\n",
      "566/566 [==============================] - 9s 16ms/step - loss: 0.0936 - mae: 0.2272 - val_loss: 0.1232 - val_mae: 0.2529\n",
      "Epoch 418/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0945 - mae: 0.2277 - val_loss: 0.1284 - val_mae: 0.2575\n",
      "Epoch 419/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0932 - mae: 0.2260 - val_loss: 0.1335 - val_mae: 0.2633\n",
      "Epoch 420/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0936 - mae: 0.2273 - val_loss: 0.1269 - val_mae: 0.2600\n",
      "Epoch 421/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0927 - mae: 0.2252 - val_loss: 0.1340 - val_mae: 0.2623\n",
      "Epoch 422/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0955 - mae: 0.2288 - val_loss: 0.1270 - val_mae: 0.2577\n",
      "Epoch 423/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0942 - mae: 0.2269 - val_loss: 0.1261 - val_mae: 0.2529\n",
      "Epoch 424/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0916 - mae: 0.2237 - val_loss: 0.1243 - val_mae: 0.2547\n",
      "Epoch 425/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0949 - mae: 0.2277 - val_loss: 0.1271 - val_mae: 0.2616\n",
      "Epoch 426/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0921 - mae: 0.2253 - val_loss: 0.1292 - val_mae: 0.2592\n",
      "Epoch 427/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0927 - mae: 0.2260 - val_loss: 0.1301 - val_mae: 0.2647\n",
      "Epoch 428/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0921 - mae: 0.2246 - val_loss: 0.1284 - val_mae: 0.2598\n",
      "Epoch 429/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0943 - mae: 0.2275 - val_loss: 0.1273 - val_mae: 0.2549\n",
      "Epoch 430/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0951 - mae: 0.2289 - val_loss: 0.1270 - val_mae: 0.2549\n",
      "Epoch 431/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0924 - mae: 0.2245 - val_loss: 0.1242 - val_mae: 0.2543\n",
      "Epoch 432/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0926 - mae: 0.2260 - val_loss: 0.1395 - val_mae: 0.2776\n",
      "Epoch 433/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0921 - mae: 0.2260 - val_loss: 0.1272 - val_mae: 0.2602\n",
      "Epoch 434/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0931 - mae: 0.2247 - val_loss: 0.1287 - val_mae: 0.2605\n",
      "Epoch 435/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0931 - mae: 0.2262 - val_loss: 0.1303 - val_mae: 0.2636\n",
      "Epoch 436/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0925 - mae: 0.2256 - val_loss: 0.1323 - val_mae: 0.2719\n",
      "Epoch 437/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0909 - mae: 0.2223 - val_loss: 0.1239 - val_mae: 0.2531\n",
      "Epoch 438/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0927 - mae: 0.2247 - val_loss: 0.1296 - val_mae: 0.2609\n",
      "Epoch 439/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0936 - mae: 0.2275 - val_loss: 0.1248 - val_mae: 0.2523\n",
      "Epoch 440/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0936 - mae: 0.2258 - val_loss: 0.1279 - val_mae: 0.2578\n",
      "Epoch 441/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0939 - mae: 0.2263 - val_loss: 0.1259 - val_mae: 0.2571\n",
      "Epoch 442/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0922 - mae: 0.2240 - val_loss: 0.1345 - val_mae: 0.2666\n",
      "Epoch 443/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0906 - mae: 0.2249 - val_loss: 0.1268 - val_mae: 0.2589\n",
      "Epoch 444/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0910 - mae: 0.2234 - val_loss: 0.1277 - val_mae: 0.2610\n",
      "Epoch 445/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0914 - mae: 0.2246 - val_loss: 0.1282 - val_mae: 0.2578\n",
      "Epoch 446/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0923 - mae: 0.2254 - val_loss: 0.1326 - val_mae: 0.2641\n",
      "Epoch 447/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0892 - mae: 0.2223 - val_loss: 0.1300 - val_mae: 0.2617\n",
      "Epoch 448/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0922 - mae: 0.2246 - val_loss: 0.1266 - val_mae: 0.2591\n",
      "Epoch 449/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0949 - mae: 0.2275 - val_loss: 0.1292 - val_mae: 0.2620\n",
      "Epoch 450/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0915 - mae: 0.2239 - val_loss: 0.1316 - val_mae: 0.2623\n",
      "Epoch 451/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0917 - mae: 0.2249 - val_loss: 0.1240 - val_mae: 0.2544\n",
      "Epoch 452/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0925 - mae: 0.2254 - val_loss: 0.1270 - val_mae: 0.2555\n",
      "Epoch 453/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0922 - mae: 0.2241 - val_loss: 0.1301 - val_mae: 0.2604\n",
      "Epoch 454/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0907 - mae: 0.2219 - val_loss: 0.1258 - val_mae: 0.2579\n",
      "Epoch 455/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0925 - mae: 0.2258 - val_loss: 0.1318 - val_mae: 0.2636\n",
      "Epoch 456/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0936 - mae: 0.2265 - val_loss: 0.1297 - val_mae: 0.2616\n",
      "Epoch 457/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0907 - mae: 0.2230 - val_loss: 0.1305 - val_mae: 0.2598\n",
      "Epoch 458/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0906 - mae: 0.2224 - val_loss: 0.1250 - val_mae: 0.2541\n",
      "Epoch 459/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0926 - mae: 0.2245 - val_loss: 0.1275 - val_mae: 0.2581\n",
      "Epoch 460/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0918 - mae: 0.2233 - val_loss: 0.1307 - val_mae: 0.2634\n",
      "Epoch 461/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0919 - mae: 0.2250 - val_loss: 0.1310 - val_mae: 0.2627\n",
      "Epoch 462/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0920 - mae: 0.2259 - val_loss: 0.1348 - val_mae: 0.2696\n",
      "Epoch 463/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0894 - mae: 0.2209 - val_loss: 0.1273 - val_mae: 0.2565\n",
      "Epoch 464/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0870 - mae: 0.2185 - val_loss: 0.1302 - val_mae: 0.2637\n",
      "Epoch 465/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0892 - mae: 0.2224 - val_loss: 0.1266 - val_mae: 0.2586\n",
      "Epoch 466/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0906 - mae: 0.2233 - val_loss: 0.1313 - val_mae: 0.2620\n",
      "Epoch 467/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0897 - mae: 0.2219 - val_loss: 0.1262 - val_mae: 0.2537\n",
      "Epoch 468/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0920 - mae: 0.2244 - val_loss: 0.1247 - val_mae: 0.2555\n",
      "Epoch 469/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0909 - mae: 0.2229 - val_loss: 0.1305 - val_mae: 0.2624\n",
      "Epoch 470/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0911 - mae: 0.2230 - val_loss: 0.1254 - val_mae: 0.2549\n",
      "Epoch 471/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0902 - mae: 0.2210 - val_loss: 0.1333 - val_mae: 0.2602\n",
      "Epoch 472/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0928 - mae: 0.2258 - val_loss: 0.1261 - val_mae: 0.2518\n",
      "Epoch 473/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0923 - mae: 0.2253 - val_loss: 0.1254 - val_mae: 0.2558\n",
      "Epoch 474/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0898 - mae: 0.2218 - val_loss: 0.1245 - val_mae: 0.2550\n",
      "Epoch 475/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0914 - mae: 0.2234 - val_loss: 0.1262 - val_mae: 0.2608\n",
      "Epoch 476/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0877 - mae: 0.2198 - val_loss: 0.1342 - val_mae: 0.2703\n",
      "Epoch 477/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.0905 - mae: 0.2230 - val_loss: 0.1270 - val_mae: 0.2587\n",
      "Epoch 478/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0917 - mae: 0.2239 - val_loss: 0.1299 - val_mae: 0.2661\n",
      "Epoch 479/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0878 - mae: 0.2197 - val_loss: 0.1276 - val_mae: 0.2577\n",
      "Epoch 480/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0887 - mae: 0.2197 - val_loss: 0.1275 - val_mae: 0.2583\n",
      "Epoch 481/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0921 - mae: 0.2238 - val_loss: 0.1291 - val_mae: 0.2579\n",
      "Epoch 482/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0881 - mae: 0.2187 - val_loss: 0.1287 - val_mae: 0.2645\n",
      "Epoch 483/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0907 - mae: 0.2239 - val_loss: 0.1264 - val_mae: 0.2572\n",
      "Epoch 484/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0893 - mae: 0.2210 - val_loss: 0.1243 - val_mae: 0.2522\n",
      "Epoch 485/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0897 - mae: 0.2212 - val_loss: 0.1283 - val_mae: 0.2584\n",
      "Epoch 486/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0908 - mae: 0.2229 - val_loss: 0.1282 - val_mae: 0.2615\n",
      "Epoch 487/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0892 - mae: 0.2211 - val_loss: 0.1241 - val_mae: 0.2526\n",
      "Epoch 488/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0909 - mae: 0.2221 - val_loss: 0.1319 - val_mae: 0.2642\n",
      "Epoch 489/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0888 - mae: 0.2201 - val_loss: 0.1278 - val_mae: 0.2576\n",
      "Epoch 490/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0878 - mae: 0.2188 - val_loss: 0.1263 - val_mae: 0.2535\n",
      "Epoch 491/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.0888 - mae: 0.2208 - val_loss: 0.1318 - val_mae: 0.2623\n",
      "Epoch 492/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0882 - mae: 0.2199 - val_loss: 0.1277 - val_mae: 0.2566\n",
      "Epoch 493/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0857 - mae: 0.2172 - val_loss: 0.1247 - val_mae: 0.2567\n",
      "Epoch 494/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0895 - mae: 0.2213 - val_loss: 0.1276 - val_mae: 0.2580\n",
      "Epoch 495/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0876 - mae: 0.2177 - val_loss: 0.1288 - val_mae: 0.2594\n",
      "Epoch 496/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0911 - mae: 0.2229 - val_loss: 0.1256 - val_mae: 0.2586\n",
      "Epoch 497/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0872 - mae: 0.2182 - val_loss: 0.1263 - val_mae: 0.2531\n",
      "Epoch 498/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0879 - mae: 0.2185 - val_loss: 0.1332 - val_mae: 0.2622\n",
      "Epoch 499/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0895 - mae: 0.2197 - val_loss: 0.1279 - val_mae: 0.2595\n",
      "Epoch 500/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0911 - mae: 0.2232 - val_loss: 0.1323 - val_mae: 0.2613\n",
      "Epoch 501/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0889 - mae: 0.2193 - val_loss: 0.1338 - val_mae: 0.2667\n",
      "Epoch 502/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0883 - mae: 0.2200 - val_loss: 0.1329 - val_mae: 0.2629\n",
      "Epoch 503/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0877 - mae: 0.2188 - val_loss: 0.1294 - val_mae: 0.2633\n",
      "Epoch 504/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0882 - mae: 0.2181 - val_loss: 0.1291 - val_mae: 0.2572\n",
      "Epoch 505/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0868 - mae: 0.2172 - val_loss: 0.1288 - val_mae: 0.2570\n",
      "Epoch 506/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0899 - mae: 0.2210 - val_loss: 0.1277 - val_mae: 0.2602\n",
      "Epoch 507/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0888 - mae: 0.2200 - val_loss: 0.1297 - val_mae: 0.2579\n",
      "Epoch 508/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0867 - mae: 0.2177 - val_loss: 0.1321 - val_mae: 0.2631\n",
      "Epoch 509/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0875 - mae: 0.2176 - val_loss: 0.1277 - val_mae: 0.2588\n",
      "Epoch 510/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0880 - mae: 0.2178 - val_loss: 0.1247 - val_mae: 0.2544\n",
      "Epoch 511/2000\n",
      "566/566 [==============================] - 8s 15ms/step - loss: 0.0882 - mae: 0.2185 - val_loss: 0.1223 - val_mae: 0.2537\n",
      "Epoch 512/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0865 - mae: 0.2165 - val_loss: 0.1268 - val_mae: 0.2564\n",
      "Epoch 513/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0866 - mae: 0.2173 - val_loss: 0.1263 - val_mae: 0.2527\n",
      "Epoch 514/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0885 - mae: 0.2196 - val_loss: 0.1239 - val_mae: 0.2525\n",
      "Epoch 515/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0864 - mae: 0.2161 - val_loss: 0.1282 - val_mae: 0.2583\n",
      "Epoch 516/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0872 - mae: 0.2171 - val_loss: 0.1267 - val_mae: 0.2534\n",
      "Epoch 517/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0876 - mae: 0.2185 - val_loss: 0.1265 - val_mae: 0.2547\n",
      "Epoch 518/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0861 - mae: 0.2170 - val_loss: 0.1273 - val_mae: 0.2571\n",
      "Epoch 519/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0878 - mae: 0.2194 - val_loss: 0.1249 - val_mae: 0.2542\n",
      "Epoch 520/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0882 - mae: 0.2194 - val_loss: 0.1267 - val_mae: 0.2544\n",
      "Epoch 521/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0874 - mae: 0.2182 - val_loss: 0.1307 - val_mae: 0.2634\n",
      "Epoch 522/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0869 - mae: 0.2179 - val_loss: 0.1249 - val_mae: 0.2528\n",
      "Epoch 523/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0877 - mae: 0.2170 - val_loss: 0.1269 - val_mae: 0.2578\n",
      "Epoch 524/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0879 - mae: 0.2176 - val_loss: 0.1260 - val_mae: 0.2554\n",
      "Epoch 525/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0865 - mae: 0.2158 - val_loss: 0.1281 - val_mae: 0.2564\n",
      "Epoch 526/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0870 - mae: 0.2177 - val_loss: 0.1281 - val_mae: 0.2585\n",
      "Epoch 527/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0869 - mae: 0.2167 - val_loss: 0.1324 - val_mae: 0.2624\n",
      "Epoch 528/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0864 - mae: 0.2170 - val_loss: 0.1299 - val_mae: 0.2619\n",
      "Epoch 529/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0853 - mae: 0.2151 - val_loss: 0.1253 - val_mae: 0.2565\n",
      "Epoch 530/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0888 - mae: 0.2212 - val_loss: 0.1252 - val_mae: 0.2571\n",
      "Epoch 531/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0865 - mae: 0.2169 - val_loss: 0.1256 - val_mae: 0.2569\n",
      "Epoch 532/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0864 - mae: 0.2175 - val_loss: 0.1284 - val_mae: 0.2590\n",
      "Epoch 533/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0849 - mae: 0.2141 - val_loss: 0.1289 - val_mae: 0.2613\n",
      "Epoch 534/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0861 - mae: 0.2161 - val_loss: 0.1247 - val_mae: 0.2553\n",
      "Epoch 535/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0886 - mae: 0.2192 - val_loss: 0.1267 - val_mae: 0.2553\n",
      "Epoch 536/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0880 - mae: 0.2181 - val_loss: 0.1298 - val_mae: 0.2622\n",
      "Epoch 537/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0838 - mae: 0.2135 - val_loss: 0.1236 - val_mae: 0.2514\n",
      "Epoch 538/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0840 - mae: 0.2137 - val_loss: 0.1271 - val_mae: 0.2589\n",
      "Epoch 539/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0870 - mae: 0.2184 - val_loss: 0.1242 - val_mae: 0.2558\n",
      "Epoch 540/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0878 - mae: 0.2190 - val_loss: 0.1280 - val_mae: 0.2560\n",
      "Epoch 541/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0844 - mae: 0.2128 - val_loss: 0.1251 - val_mae: 0.2547\n",
      "Epoch 542/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0855 - mae: 0.2157 - val_loss: 0.1267 - val_mae: 0.2554\n",
      "Epoch 543/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0853 - mae: 0.2158 - val_loss: 0.1260 - val_mae: 0.2547\n",
      "Epoch 544/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0854 - mae: 0.2148 - val_loss: 0.1246 - val_mae: 0.2514\n",
      "Epoch 545/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.0871 - mae: 0.2173 - val_loss: 0.1284 - val_mae: 0.2606\n",
      "Epoch 546/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0858 - mae: 0.2164 - val_loss: 0.1284 - val_mae: 0.2575\n",
      "Epoch 547/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0856 - mae: 0.2166 - val_loss: 0.1272 - val_mae: 0.2539\n",
      "Epoch 548/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0844 - mae: 0.2149 - val_loss: 0.1282 - val_mae: 0.2613\n",
      "Epoch 549/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0849 - mae: 0.2153 - val_loss: 0.1269 - val_mae: 0.2544\n",
      "Epoch 550/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0870 - mae: 0.2171 - val_loss: 0.1280 - val_mae: 0.2576\n",
      "Epoch 551/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0875 - mae: 0.2191 - val_loss: 0.1275 - val_mae: 0.2601\n",
      "Epoch 552/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0862 - mae: 0.2178 - val_loss: 0.1269 - val_mae: 0.2568\n",
      "Epoch 553/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0841 - mae: 0.2147 - val_loss: 0.1248 - val_mae: 0.2562\n",
      "Epoch 554/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0856 - mae: 0.2171 - val_loss: 0.1299 - val_mae: 0.2595\n",
      "Epoch 555/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0848 - mae: 0.2143 - val_loss: 0.1273 - val_mae: 0.2567\n",
      "Epoch 556/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0847 - mae: 0.2144 - val_loss: 0.1259 - val_mae: 0.2556\n",
      "Epoch 557/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0854 - mae: 0.2169 - val_loss: 0.1293 - val_mae: 0.2580\n",
      "Epoch 558/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0846 - mae: 0.2150 - val_loss: 0.1304 - val_mae: 0.2591\n",
      "Epoch 559/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0850 - mae: 0.2137 - val_loss: 0.1257 - val_mae: 0.2556\n",
      "Epoch 560/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0875 - mae: 0.2181 - val_loss: 0.1278 - val_mae: 0.2567\n",
      "Epoch 561/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0833 - mae: 0.2130 - val_loss: 0.1238 - val_mae: 0.2541\n",
      "Epoch 562/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0835 - mae: 0.2145 - val_loss: 0.1276 - val_mae: 0.2570\n",
      "Epoch 563/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.0856 - mae: 0.2146 - val_loss: 0.1301 - val_mae: 0.2622\n",
      "Epoch 564/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0866 - mae: 0.2173 - val_loss: 0.1250 - val_mae: 0.2528\n",
      "Epoch 565/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0834 - mae: 0.2127 - val_loss: 0.1251 - val_mae: 0.2561\n",
      "Epoch 566/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0839 - mae: 0.2139 - val_loss: 0.1301 - val_mae: 0.2593\n",
      "Epoch 567/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0803 - mae: 0.2076 - val_loss: 0.1303 - val_mae: 0.2637\n",
      "Epoch 568/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0833 - mae: 0.2121 - val_loss: 0.1261 - val_mae: 0.2571\n",
      "Epoch 569/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0829 - mae: 0.2116 - val_loss: 0.1330 - val_mae: 0.2653\n",
      "Epoch 570/2000\n",
      "566/566 [==============================] - 7s 13ms/step - loss: 0.0834 - mae: 0.2120 - val_loss: 0.1269 - val_mae: 0.2547\n",
      "Epoch 571/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0857 - mae: 0.2149 - val_loss: 0.1252 - val_mae: 0.2538\n",
      "Epoch 572/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0839 - mae: 0.2127 - val_loss: 0.1245 - val_mae: 0.2529\n",
      "Epoch 573/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0817 - mae: 0.2097 - val_loss: 0.1246 - val_mae: 0.2506\n",
      "Epoch 574/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0840 - mae: 0.2138 - val_loss: 0.1332 - val_mae: 0.2650\n",
      "Epoch 575/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0834 - mae: 0.2121 - val_loss: 0.1307 - val_mae: 0.2596\n",
      "Epoch 576/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0843 - mae: 0.2144 - val_loss: 0.1288 - val_mae: 0.2612\n",
      "Epoch 577/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0849 - mae: 0.2138 - val_loss: 0.1253 - val_mae: 0.2544\n",
      "Epoch 578/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0835 - mae: 0.2126 - val_loss: 0.1276 - val_mae: 0.2626\n",
      "Epoch 579/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0838 - mae: 0.2131 - val_loss: 0.1250 - val_mae: 0.2544\n",
      "Epoch 580/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0833 - mae: 0.2130 - val_loss: 0.1277 - val_mae: 0.2576\n",
      "Epoch 581/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0832 - mae: 0.2122 - val_loss: 0.1256 - val_mae: 0.2561\n",
      "Epoch 582/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0841 - mae: 0.2126 - val_loss: 0.1262 - val_mae: 0.2536\n",
      "Epoch 583/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0820 - mae: 0.2104 - val_loss: 0.1271 - val_mae: 0.2569\n",
      "Epoch 584/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0832 - mae: 0.2125 - val_loss: 0.1281 - val_mae: 0.2579\n",
      "Epoch 585/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0831 - mae: 0.2130 - val_loss: 0.1253 - val_mae: 0.2559\n",
      "Epoch 586/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0846 - mae: 0.2151 - val_loss: 0.1231 - val_mae: 0.2523\n",
      "Epoch 587/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0848 - mae: 0.2146 - val_loss: 0.1242 - val_mae: 0.2507\n",
      "Epoch 588/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0814 - mae: 0.2109 - val_loss: 0.1292 - val_mae: 0.2580\n",
      "Epoch 589/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0861 - mae: 0.2156 - val_loss: 0.1254 - val_mae: 0.2533\n",
      "Epoch 590/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0826 - mae: 0.2129 - val_loss: 0.1276 - val_mae: 0.2564\n",
      "Epoch 591/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0839 - mae: 0.2143 - val_loss: 0.1299 - val_mae: 0.2622\n",
      "Epoch 592/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0843 - mae: 0.2140 - val_loss: 0.1236 - val_mae: 0.2534\n",
      "Epoch 593/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0826 - mae: 0.2113 - val_loss: 0.1252 - val_mae: 0.2519\n",
      "Epoch 594/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0836 - mae: 0.2131 - val_loss: 0.1275 - val_mae: 0.2537\n",
      "Epoch 595/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0837 - mae: 0.2133 - val_loss: 0.1246 - val_mae: 0.2549\n",
      "Epoch 596/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0839 - mae: 0.2135 - val_loss: 0.1258 - val_mae: 0.2549\n",
      "Epoch 597/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0844 - mae: 0.2141 - val_loss: 0.1263 - val_mae: 0.2572\n",
      "Epoch 598/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0843 - mae: 0.2129 - val_loss: 0.1287 - val_mae: 0.2618\n",
      "Epoch 599/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0835 - mae: 0.2128 - val_loss: 0.1258 - val_mae: 0.2536\n",
      "Epoch 600/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0823 - mae: 0.2111 - val_loss: 0.1265 - val_mae: 0.2534\n",
      "Epoch 601/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0826 - mae: 0.2120 - val_loss: 0.1300 - val_mae: 0.2613\n",
      "Epoch 602/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0816 - mae: 0.2098 - val_loss: 0.1256 - val_mae: 0.2561\n",
      "Epoch 603/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0849 - mae: 0.2144 - val_loss: 0.1273 - val_mae: 0.2579\n",
      "Epoch 604/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0829 - mae: 0.2122 - val_loss: 0.1264 - val_mae: 0.2542\n",
      "Epoch 605/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0801 - mae: 0.2081 - val_loss: 0.1264 - val_mae: 0.2562\n",
      "Epoch 606/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0822 - mae: 0.2109 - val_loss: 0.1256 - val_mae: 0.2566\n",
      "Epoch 607/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0820 - mae: 0.2116 - val_loss: 0.1244 - val_mae: 0.2521\n",
      "Epoch 608/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0812 - mae: 0.2092 - val_loss: 0.1290 - val_mae: 0.2598\n",
      "Epoch 609/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0794 - mae: 0.2069 - val_loss: 0.1286 - val_mae: 0.2583\n",
      "Epoch 610/2000\n",
      "566/566 [==============================] - 8s 13ms/step - loss: 0.0834 - mae: 0.2126 - val_loss: 0.1278 - val_mae: 0.2569\n",
      "Epoch 611/2000\n",
      "566/566 [==============================] - 8s 14ms/step - loss: 0.0819 - mae: 0.2115 - val_loss: 0.1318 - val_mae: 0.2616\n",
      "Models saved at: model/Sun Jul 21 05:03:56 2024\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "def load_catalogs(folder: str):\n",
    "    _img_name = []\n",
    "    _angle = []\n",
    "    _throttle = []\n",
    "\n",
    "    for _file in sorted(glob.glob(f\"{folder}/*.catalog\"),\n",
    "                        key=lambda x: [\n",
    "                            int(c) if c.isdigit()\n",
    "                            else c for c in re.split(r'(\\d+)', x)]):\n",
    "        with open(_file) as f:\n",
    "            for _line in f:\n",
    "                _img_name.append(_line.split()[7][1:-2])\n",
    "                _angle.append(float(_line.split()[9][0:-1]))\n",
    "                _throttle.append(float(_line.split()[13][0:-1]))\n",
    "\n",
    "    print(f'Image count: {len(_img_name)}')\n",
    "    return _img_name, _angle, _throttle\n",
    "\n",
    "\n",
    "def load_images(_img_name: list, folder: str):\n",
    "    _image = []\n",
    "    for i in range(len(_img_name)):\n",
    "        _img = cv2.imread(os.path.join(f\"{folder}/images\", _img_name[i]))\n",
    "        assert _img.shape == (224, 224, 3),\\\n",
    "            \"img %s has shape %r\" % (_img_name[i], _img.shape)\n",
    "        _image.append(_img)\n",
    "    return _image\n",
    "\n",
    "\n",
    "def data_preprocessing(_throttle, _angle, _image):\n",
    "    _throttle = np.array(_throttle)\n",
    "    _steering = np.array(_angle)\n",
    "    _train_img = np.array(_image)\n",
    "    _label = _steering\n",
    "    _cut_height = 80\n",
    "    _train_img_cut_orig = _train_img[:, _cut_height:224, :]\n",
    "    _train_img_cut_gray = _train_img_cut_orig\n",
    "    return _train_img_cut_orig, _train_img_cut_gray, _label\n",
    "\n",
    "\n",
    "def train_split(_train_img_cut_orig, _train_img_cut_gray, _label):\n",
    "    _X_train, _X_val, _y_train, _y_val = train_test_split(\n",
    "        _train_img_cut_gray, _label,\n",
    "        test_size=0.15, random_state=42)\n",
    "    return _X_train, _X_val, _y_train, _y_val\n",
    "\n",
    "\n",
    "def build_fine_tuned_efficientnet_model(input_shape):\n",
    "    base_model = tf.keras.applications.DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    _x = base_model.output\n",
    "    _x = GlobalAveragePooling2D()(_x)\n",
    "    _x = Dense(1024, activation='relu')(_x)\n",
    "    _x = Dropout(0.5)(_x)\n",
    "    _outputs = Dense(1, activation='linear')(_x)\n",
    "\n",
    "    _model = Model(inputs=base_model.input, outputs=_outputs)\n",
    "    return _model\n",
    "\n",
    "\n",
    "def train_start(_model, _X_train, _X_val, _y_train, _y_val, \n",
    "                epochs: int=100, batch_size: int=16, patience: int=100, save_folder: str=''):\n",
    "    _optimizer = tf.optimizers.Adam(learning_rate=0.0001,\n",
    "                                    beta_1=0.9, beta_2=0.999)\n",
    "    _model.compile(optimizer=_optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
    "    _model.summary()\n",
    "    \n",
    "    # Add EarlyStopping callback\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                  patience=patience, \n",
    "                                                  restore_best_weights=True)\n",
    "    \n",
    "    # Add ModelCheckpoint callback to save the best model\n",
    "    best_model_path = os.path.join(save_folder, \"best_model.h5\")\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_path, \n",
    "                                                          monitor='val_loss', \n",
    "                                                          save_best_only=True)\n",
    "    \n",
    "    _trained_model = _model.fit(_X_train, _y_train,\n",
    "                                epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(_X_val, _y_val),\n",
    "                                callbacks=[early_stop, model_checkpoint])\n",
    "    return _trained_model\n",
    "\n",
    "\n",
    "def plot_trained_model(_trained_model, \n",
    "                       show: bool=False,\n",
    "                       save: bool=True,\n",
    "                       save_folder: str=''):\n",
    "    \n",
    "    history = _trained_model.history\n",
    "\n",
    "    plt.plot(history['loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(save_folder, f'Loss.png'),\n",
    "                    bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    plt.plot(history['mae'], label='Train MAE')\n",
    "    plt.plot(history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(save_folder, f'MAE.png'),\n",
    "                    bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"../data/data_0202\"\n",
    "    save_folder = f\"model/{time.ctime(time.time())}\"\n",
    "    # create save path\n",
    "    if not os.path.isdir(save_folder):\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    img_name, angle, throttle = load_catalogs(data_folder)\n",
    "    image = load_images(img_name, data_folder)\n",
    "    image = np.array(image)\n",
    "    train_img_cut_orig, train_img_cut_gray, label = data_preprocessing(\n",
    "        throttle, angle, image)\n",
    "    X_train, X_val, y_train, y_val = train_split(\n",
    "        train_img_cut_orig, train_img_cut_gray, label)\n",
    "\n",
    "    # Update input shape for EfficientNet\n",
    "    model = build_fine_tuned_efficientnet_model(input_shape=(144, 224, 3))\n",
    "    trained_model = train_start(model, X_train, X_val, y_train, y_val, \n",
    "                               epochs=2000, save_folder=save_folder)\n",
    "    plot_trained_model(trained_model, show=False, save=True, save_folder=save_folder)\n",
    "    \n",
    "    # Save the last model\n",
    "    model.save(os.path.join(save_folder, \"last_model.h5\"))\n",
    "    \n",
    "    print(f\"Models saved at: {save_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336eb36e-73ca-4e1d-b028-afe3e30817d0",
   "metadata": {},
   "source": [
    "### Models saved at: model/Sun Jul 21 05:03:56 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb8fbd-038f-4420-9d3d-a327725c6706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.7.0",
   "language": "python",
   "name": "tensorflow-2.7.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
